---
layout: post
comments: true
categories: 机器学习
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

* content
{:toc}
#### XGBoost概述

XGBoost是GBDT算法的一种变种，是一种常用的有监督集成学习算法；是一种伸缩性强，便捷的可并行构建模型的Gradient Boosting算法。

XGBoost官网:<https://xgboost.readthedocs.io/en/latest/>

XGBoost Github源码位置：<https://github.com/dmlc/xgboost>

XGBoost支持开发语言：Python、R、Java、Scala、C++等。

XGBoost安装

安装方式一：编译Github上的源码，参考<http://xgboost.readthedocs.io/en/latest/build.html>

安装方式二：直接使用python的whl文件进行安装，要求python版本3.5或者3.6；

下载链接：<https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost>

安装参考命令：pip install f:///xgboost-0.7-cp36-cp36m-win_amd64.whl

#### XGBoost原理讲解

##### 模型

###### 目标函数

$$
Obj(\theta) = L(\theta)+\Omega(\theta)
$$

误差函数：体现的是模型有多拟合数据。

正则化项：惩罚复杂模型的参数，用于解决过拟合。

![](..\..\imgs/模型.png)

##### GBDT的目标函数

$$
\hat{a}_i^{(0)}=0\\
\hat{y_i}^{(1)}=\hat{a}_i^{(0)}+f_1(x_i)\\
\hat{y_i}^{(2)}=\hat{a}_i^{(0)}+f_2(x_i)\\
...\\
\hat{y_i}^{(t)}=\hat{a}_i^{(t-1)}+f_t(x_i)\\
obj=\sum_{i=1}^n(y_i,\hat{y}_i^{(t)})
$$

##### XGBoost目标函数

$$
\hat{a}_i^{(0)}=0\\
\hat{y_i}^{(1)}=\hat{a}_i^{(0)}+f_1(x_i)\\
\hat{y_i}^{(2)}=\hat{a}_i^{(0)}+f_2(x_i)\\
...\\
\hat{y_i}^{(t)}=\hat{a}_i^{(t-1)}+f_t(x_i)\\
obj=\sum_{i=1}^n(y_i,\hat{y}_i^{(t)})+\sum_{i=1}^t\Omega(f_i)\\
f_t(x)=w_q(x)\\
\Omega(f)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2\;\;\;{\color{red} 正则化项系数防止过拟合}
$$

##### XGBoost公式推导

第t次迭代后，模型的预测等于前t-1次的模型加上第t棵树的预测：
$$
\hat{y}_i^{(t)}=\hat{y}_i^{(t-1))}+f_t(x_i)
$$
目标函数可以写成：
$$
loss=\sum_{i=1}^nl(y_i,\hat{y_i}^{(t-1)}+f_t(x_i))+\sum_{i=1}^{t-1}\Omega(f_i)+\Omega(f_t)
$$
将误差函数中的$\hat{y}_i^{(t-1)}+f_t(x_i)$看成一个整体，求解这个整体取值在$\hat{y}_i^{(t-1)}$处进行二次泰勒展开：
$$
loss\approx\sum_{i=1}^n\left[l(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)\right]+\sum_{t=1}^{t-1}\Omega(f_i)+\Omega(f_t)\\
g_i=\part_{\hat{y}_i^{(t-1)}}l(y_i,\hat{y}_i^{(t-1)})\\
h_i=\part_{\hat{y}_i^{(t-1)}}^2l(y_i,\hat{y}_i^{(i)})\\
$$
将函数中的所有常数项全部去掉，可以得到以下的公式：
$$
loss\approx\sum_{i=1}^n\left[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)\right]+\Omega(f_t)
$$
将函数f和正则项带入公式中得到以下公式：
$$
loss\approx\sum_{i=1}^n\left[g_iw_{q(x_i)}+\frac{1}{2}h_iw_{q(x_i)}^2\right]+\gamma T+\lambda\frac{1}{2}\sum_{j=1}^Tw_j^2
$$
定义每个叶子节点j上的样本集合为：$I_j$
$$
I_j=\left\{i|q(x_i)==j\right\}
$$
将样本累加操作转换为叶节点的操作：
$$
loss\approx\sum_{j=1}^T\left[\left(\sum_{i\in I_j}g_i\right)w_j+\frac{1}{2}\left(\sum_{i\in I_j}h_i\right)w_j^2\right]+\gamma T+\lambda\frac{1}{2}\sum_{j=1}^Tw_j^2\\
=\sum_{j=1}^T\left[\left(\sum_{i\in I_j}g_i\right)w_j+\frac{1}{2}\left(\sum_{i\in I_j}h_i\right)w_j^2\right]+\gamma T=\sum_{j=1}^T\left[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2\right]+\gamma T
$$
最终的目标函数：
$$
loss=\sum_{j=1}^T\left[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2\right]+\gamma T
$$
如果树的结构可以确定（q函数确定），为了使目标函数最小，可以令导数为0，可以求得最优的w，将w带入目标函数，可以得到最终的损失为：
$$
w_j^*=-\frac{G_j}{H_j+\lambda};\;\;\ loss^*=-\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T
$$
![](imgs/xgboost公式推导.png)

当树的结构确定的时候，我们可以得到最优的叶子节点分数以及对应的最小损失值，问题在于如何确定树结构？

​	暴力穷举所有可能的结构，选择损失值最小值；（很难求解）

​	贪心法，每次尝试选择一个分裂点进行分割，计算操作前后的增益，选择增益最大的方式进行分裂。

##### 决策树相关算法指标

ID3算法：信息增益。

C4.5算法：信息增益率。

CART:Gini系数。

##### XGBoost的学习策略

XGBoost目标函数：
$$
loss^*=-\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T
$$
从目标函数中，我们希望损失函数越小越好，那就是$\frac{G^2}{H+\lambda}$越大越好；从而，对于一个叶子节点的分裂的分裂，分裂前后的信息增益定义为：
$$
Gain=\frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{(H_L+H_R)+\lambda}\right]-\gamma
$$
Gain值越大，分裂后减少的损失值越大。所以对于一个叶子节点分割时，计算所有候选的（feature，value）对应的Gain，选择Gain最大的特征进行分割。

#### XGBoost项目案例