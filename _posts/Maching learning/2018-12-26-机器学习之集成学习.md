---
layout: post
comments: true
categories: 机器学习
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

* content
{:toc}

#### 集成学习

~~~
集成学习的思想是将若干个弱分类器（分类器和回归器）组合之后产生一个新的学习器。
弱分类器就是哪些学习能力相对较差的分类器。
~~~

##### 集成学习的需要

~~~
集成一些弱分类器，可以是导致分类边界不同的存在一些错误的弱分类器，在集成后得到更加合理的边界，减少了整体的粗无虑，实现更好的结果。
对于过大或者过小的数据集，可以通过进行划分和又放回的操作善生不同的数据子集，使用数据子集可以训练不同的分类器，最终合并成为一个大的分类器。
数据的划分如边界过于复杂，使用线性模型很难描述情况，那么可以训练多个模型，在进行模型融合。
对于比较难融合的多个异构的特征集。可以考虑每个数据集构建一个分类器模型，然后将多个模型融合。
~~~

##### Bagging

~~~
Bagging即自举枚举法，在元数据上通过有放回的抽样的方式，重新选择s个新数据集分别训练s个分类器的集成技术。
Bagging方法训练出来的模型在预测新样本分类的时候，会使用多数投票或者求均值的方式来统计最终的分类结果。
Bagging方法的弱分类器可以使基本的算法模型。eg：Linear、Ridge、Lasso、Logistic、Softmax、ID3、C4.5、SVM、KNN等。
~~~



#### 随机森林

~~~
鉴于决策树容易欠拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树，假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，如果全样本去训练m棵决策树显然不可取，全样本训练忽视了
局部样本的规律，对于模型泛化能力是有害的。
~~~

bagging+决策树=随机森林

>1、从原始样本集（n个样本）找那个用Bootstrap采样（有放回）选出n个样本，去重。（用户模型训练模型的样本数目实际上要小于n）
>2、从所有属性中随机选取K个属性。从K个属性找那个选择出最佳分割属性作为节点来迭代创建决策树。
>3、重复以上两步m次，创立m棵决策树。
>4、这m个决策树形成随机森林，通过投票结果表决数据属于哪一类。

RF算法在实际应用中具有比较好的特性，应用也比较广泛，主要应用在：分类、回归、特征转换、异常点检测。常见的RF变种算法：

##### Extra Tree

>和RF的区别：
>​	RF会随机采样来作为子决策树的训练集，而Extra Tree每个子决策树采用原始数据集训练。
>​	RF在选择划分特征点的时候回和传统决策树一样，会基于信息增益、信息增益率、基尼系数、均方差等原则来选择最优特征值；而Extra Tree会随机的选择一个特征值来划分决策树。
>​	Extra Tree因为是随机选择特征值的划分点，会导致决策树的规模一般大于RF所生成的决策树。Extra Tree模型的方差相对于RF进一步减少。某些情况下，Extra Tree的泛化能力比RF强。
>
><font color="red">**question**：Extra Tree每个子决策树采用原始数据集训练（这里是没有去重么？）</font>

##### TRTE

>TRTE是一种非监督的数据转化方式。将低维的数据映射到高维，高维数据更好的应用于分类回归模型。
>
>TRTE算法的转化过程类似RF算法的方法。建立T个局册数来拟合数据（是类似KD-TREE一样基于特征属性的方差选择划分特征）。当决策树构建完成后，数据集利的每个数据在T个决策树中叶子节点的位置就定下来了，将位置信息转换为向量就完成了特征转换操作 。
>
>案例：有三棵决策树，各个决策树的叶子节点数目分别为：5,5,4，某个数据x划分到第一个决策树的第三个叶子节点，第二个决策树的第一个叶子节点，第三个决策树的第四个叶子节点，那么最终的x映射特征码为：(0,0,1,0,0 1,0,0,0,0 0,0,0,1)

##### Isolation Forest(IForest)

>IForest是一种异常点检测算法，使用类似RF的方式来检测异常点；IForest算法和RF区别在于：
>
>1、在随机采样过程中，一般只需要少量数据即可。
>
>2、在进行决策树构建过程中，IForest算法会随机选择一个划分特征，并对划分特征随机选择一个划分阈值。
>
>3、IForest算法构建的决策树一般深度max_depth是比较小的。
>
>区别原因：目的是异常点检测，所以只要能够区分异常的即可，不需要大量数据；另外在异常点检测的过程中，不需要太大规模的决策树。

异常点检测的算法过程：

对于异常点的判断。则是将测试样本x拟合到T棵决策树上。计算在每课树上该样本的叶子节点的深度$h_t(x)$。从而计算出平均深度$h(x)$;然后就可以使用下列公式计算样本点x的异常概率值，$p(s,m)$的取值范围为[0,1],接近于1，则是异常点的概率越大。如果落在叶子节点为正常样本点，那么当前决策树不考虑，如果所有决策树上都是正常样本点，那么直接认为异常点概率为0.
$$
p(x,m)=2^{-\frac{h(x)}{c(m)}}\\
c(m) = 2ln(m-1)+\xi-2\frac{m-1}{m}
$$

##### RF总结

RF的主要特点：

>1、训练棵进行合并化，对于大规模样本的训练具有速度的优势。
>
>2、由于进行随机选择决策划分特征集表，这样在样本维度比较高的时候，仍然具有比较高的训练性能。
>
>3、给以给出各个特征的重要性列表。
>
>4、由于存在随机抽样，训练出来的模型方差小，泛化能力强。
>
>5、RF实现简单。
>
>6、对于部分特征的缺失感不敏感。

RF的主要特点

>1、在某些噪音比较大的特征上，RF模型容易陷入过拟合。
>
>2、取值比较多的划分 特征对RF的决策会产生更大的影响，从而可能影响模型的效果。

#### 提升算法

##### 随机森林的思考

在随机森林的构建过程中，由于各科树之间是没有关系的，相对独立的；在构建的过程中，构建第m棵子树的时候，不会考虑前面的m-1棵树。

##### Boosting

提升学习是一种机器学习的技术，可以用于回归和分了IDE问题，它每一步产生弱分类预测模型（如决策树），并加权累加到总模型中，如果每一步的弱预测模型的生成都是依据损失函数的梯度方式，那么就称为梯度提升。

提升技术的意义：如果一个问题存在若预测模型，那么可以通过提升技术的办法得到一个强预测模型。

常见的模型有：

​	Adaboost

​	Gradient Boosting(GBT/GBDT/GBRT)

##### Bagging、Boosting区别

1.样本选择：Bagging算法是又放回的随机采样；Boosting算法是每一轮训练集不变，只是训练集中的每个样例在分类器中的权重发生变化或者目标属性y发生变化，而权重&y值都是根据上一轮的预测结果进行调整；

2.样例权重：Bagging使用随机抽样，样例是等权重；Boosting根据错误率不断的调整样例的权重值，错误率越大则权重越大（Asaboost）；

3.预测函数：Bagging所有预测模型的权重相等；Boosting算法对于误差晓得分类器具有更大的权重

4.并行计算：Bagging算法可以并行声场各个基模型；Boosting理论上只能顺序生产，因为后一个模型需要前一个模型的结果；

5.Bagging是减少模型的variance（方差）；Boosting是减少模型的Bias（偏度）。

6.Bagging利每个分类模型都是强分类器，因为降低的是方差，方差过高需要降低是过拟合；Boosting利每个分类模型都是弱分类器，因为降低的是偏度，偏度过高是欠拟合。

7。Bagging对样本重采样，对每一轮的采样数据集都训练一个模型，最后去平均值。由于样本集的相似性和使用的同种模型，因此各个模型的具有相似的bias和
$$
E\left\{\frac{\sum_{i=1}^n}{n}\right\}=E(X_i)\\
var\left\{\frac{\sum_nX_i}{n}\right\}=\frac{Var(X_i)}{n}:模型完全独立\\
Var\left\{\frac{\sum_nX_i}{n}\right\}=Var(X_i):模型完全相同
$$

##### stacking

![](https://raw.githubusercontent.com/MaoChengEr/maochenger.github.io/master/imgs/stacking流程图.jpg)

上半部分是五折交叉验证，用XGBoost作为基础模型modle1，五折交叉验证拿出四折作为training data，另外一折作为testing data。<font color="red">注意：在stacking中此部分数据会用到整个training set。假设training set包含了10000个数据，testing set包含了2500个数据。那么每一次交叉验证其实就是对training  set进行划分没在每一次的交叉验证中training data将会是8000行，testing data是2000行</font>

每一次的交叉验证包含两个过程，1、基于training data训练模型 2、基于triaining data训练生成的模型对testing data进行预测。在整个第一次交叉验证完成以后我们将会得到关于当前testing data的预测值，这将是一个一维2000行的数据，记为a1。<font color=red>注意</font>这部分操作完成之后，我们还要对数据集原来的整个testing set进行预测，这个过程会产生2500个预测值，这部分预测值将会做为下一层模型testing data的一部分，记为b1,。因为进行的是五折交叉验证，所以以上提及的过程将会进行五次，最终生成对testing set数据预测的5列2000行数据a1,a2,a3,a4,a5,testing set的预测回事5列2500行数据b1,b2,b3,b4,b5。

在完成对Model1的整个过程之后，我们发现a1,a2,a3,a4,a5其实就是对原来整个training set的预测值，将他们拼凑起来，会形成一个10000行一列的矩阵，记为A1。而对于b1,b2.b3.b4.b5这部分数据，我们将各部分相加去平均值，得到一个2500行一列的矩阵，记为B1。

##### AdaBoost算法原理

>Adaptive Boosting是一种迭代算法。每轮迭代中会在训练集上产生一个新的学习器，然后使用该学习器对所有样本进行预测，以评估每个样本的重要性（informative）。算法为每个样本赋予一个权重，每次用训练好的学习器标注/预测各个样本，如果某个样本点呗预测的越正确，则将其权重降低，否则提高样本的权重。权重越高的样本在下一个迭代训练中所占的比重就越大，也就是说越难区分的样本在训练过程中会变得越重要。整个迭代过程直到错误率足够小或者达到一定的迭代系数为止。

Adaboost算法将基分类器的线性组合作为强分类器，同时给误差率较小的基分类器以大的权值，给分类器误差率大的分类器以小的权值，构建的线性组合为：
$$
f(x)=\sum_{m=1}^M\alpha_mG_m(x)
$$
最终分类器是在线性组合的基础上进行Sign函数转换：
$$
G(x)=sign(f(x))=sign\left[\sum_{m=1}^M\alpha_mG_m(x)\right]
$$
最终的强学习器：$$G(x)=sign(f(x))=sign\left[\sum_{m=1}^M\alpha_mG_m(x)\right]$$

损失函数（以错误率作为损失函数）：$$loss=\frac{1}{n}\sum_{i=1}^nI(G(x_i)\neq y_i)$$

损失函数：$$loss=\frac{1}{n}\sum_{i=1}^nI(G(x_i)\neq y_i)\leq\frac{1}{n}\sum_{i=1}^ne^{(-y_if(x))}$$

第k-1轮的强学习器：
$$
f_{k-1}(x)=\sum_{j=1}^{k-1}\alpha_jG_j(x)
$$
第k轮的强学习器：
$$
f_k(x)=\sum_{j=1}^k\alpha_jG_j(x)\\
f_k(x)=f_{k-1}(x)+\alpha_kG_k(x)
$$
损失函数：
$$
loss(\alpha_m,G_m(x))=\frac{1}{n}\sum_{i=1}^ne^{(-y_i(f_{m-1}(x)+\alpha_mG_m(x)))}
$$

$$
loss(\alpha_m,G(m))=\frac{1}{n}\sum_{i=1}^ne^{-y_i(f_{m-1}(x)+\alpha_mG_m(x))}\\
=\frac{1}{n}\sum_{i=1}^ne^{-y_if_{m-1}(x)}e^{(-y_i\alpha_mG_m(c))}\\
\overset{令\bar{w_{mi}}=e^{-y_if_{m-1}(x)}}{\rightarrow}\\
=\frac{1}{n}\sum_{i=1}^n\bar{w_{mi}}e^{-y_i\alpha_mG_m(x)}
$$
是下列公式达到最小值的$\alpha_m$和$G_m$就是Adaboost算法的最终求解值
$$
loss(\alpha_m,G_m(x))=\frac{1}{n}\sum_{i=1}^n\bar{w}_{mi}e^{-y_i\alpha_mG_m(x)}
$$
G分类器在训练的过程中，是为了让误差率最小，所以可以认为G越小就是误差率越小
$$
G_m^*(x)=\frac{1}{n}\sum_{i=1}^n\bar{w}_{mi}I(y_i\neq G_m(x_i))\\
\varepsilon_m=P(G_m(x)\neq y)=\frac{1}{n}\sum_{i=1}^n\bar{w}_{mi}I(y_i\neq G_m(x))
$$
对于$\alpha$而言，通过求导后令导数为零，可以得到公式（log对象可以以e为底也可以以2为底）:
$$
\alpha_m^*=\frac{1}{2}\ln(\frac{1-\varepsilon_m}{\varepsilon_m})
$$

##### Adaboost算法构建过程

>1、假设训练数据集$T={(X_1,Y_1),(X_2,Y_2),...,(X_n,Y_n)}$
>
>2、初始化训练数据权重分布
>$$
>D_1=(W_{11},W_{12},...W_{1i},...,w_{1n}),w_{1i}=\frac{1}{n},i=1,2,3,...,n
>$$
>3、使用具有权值分布$D_m$的训练数据集学习，得到基本分类器
>$$
>G_m(x):x\rightarrow \{-1,+1\}
>$$
>4、计算$G_m(x)$在训练集上的分类误差
>$$
>\varepsilon_m =P(G_M(X_I)\neq y_i)=\sum_{i=1}^nw_{mi}I(G_m(x_i)\neq y_i)
>$$
>5、计算$G_m(x)$的模型的权重系数$\alpha_m$
>$$
>\alpha_m = \frac{1}{2}*log_2{\left(\frac{1-\varepsilon_m}{\varepsilon_m}\right)}
>$$
>

>6、权重训练数据集的权值分布
>$$
>D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m+1,i},...,w_{m+1,n})\\
>w_{m+1,i}=\frac{w_{m,i}}{Z_m}e^{-\alpha_m y_iG_m(x)}
>$$
>7、这里$Z_m$是规范化因子（归一化）
>$$
>Z_m=\sum_{i=1}^nw_{m.i}e^{-\alpha_my_iG_m(x_i)}
>$$
>8、构建基本分类器的线性组合
>$$
>f(x)=\sum_{m=1}^M\alpha_mG_m(x)
>$$
>9、得到最终分类器
>$$
>G(x)=sign(f(x))=sign\left(\sum_{m=1}^M\alpha_mG_m(x)\right)
>$$
>

AdaBoost直观理解

~~~python
#X  0    1   2   3   4   5   6   7   8   9
#Y  1    1   1  -1  -1  -1   1   1   1  -1
#w1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1
import numpy as np
import math

#计算信息熵公式
def entropy(t):
    return np.sum(-x*np.log2(x) for x in t)

#原始的信息熵
a = entropy([0.6,0.4])

def errorate(ε):
    return 0.5*np.log2((1-ε)/ε)

X = [0,1,2,3,4,5,6,7,8,9]
Y = [1,1,1,-1,-1,-1,1,1,1,-1]
W = [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]


def G1(x,t):
        if x < t:
            return 1
        else:
            return -1

D1 = []
weight = []

def updateWeight(alpha,X,Y,W,t):
    S = 0
    for i in range(len(X)):
        d= W[i]*pow(np.e,(-1)*alpha*Y[i]*G1(X[i],t))
        D1.append(d)
    for j in D1:
        S += j
    return D1/S

#当x以2.5作为划分的特征属性，来计算其信息增益
b1 = entropy([1.0])
b2 = entropy([3/7,4/7])
#合并信息熵，得到条件熵
b = 0.3*b1+0.7*b2
#以x=2.5为划分特征属性得到的信息增益
print('以x=2.5为信息熵得到的信息增益%.3f'%(a-b))

#当以x=5.5作为划分特征属性，来计算其信息增益
c1 = entropy([0.5,0.5])
c2 = entropy([0.75,0.25])
#合并信息熵，得到条件熵
c = 0.6*c1+0.4*c2
#以x=5.5作为划分特征属性的信息增益
print('以x=5.5作为划分特征属性的信息增益%.3f'%(a-c))


#当以x=8.5为划分特征属性的信息增益
d1 = entropy([6/9,3/9])
d2 = entropy([1.0])
#合并信息熵，获得条件熵
d = 0.9*d1+0.1*d2
#以x=8.5为划分特征属性的信息增益
print('以x=8.5为划分特征属性的信息增益%.3f'%(a-d))

#通过计算可得出信息增益最大的是以x=2.5作为划分特征属性的时候
#G1(x)在训练集上的误差率为 ε1
ε1 = 0.3
alpha1 = errorate(ε1)
print('以2.5作为划分特征属性的误差率:{},G1的系数为alpha1:{}'.format(ε1,alpha1))
print('f(x)=%.4fG1(x)'%(alpha1))
newWeight = updateWeight(alpha1,X,Y,W,2.5)
print(newWeight)
print("="*50)
#得到的新的权重值
#X  0    1   2   3   4   5   6   7   8   9
#Y  1    1   1  -1  -1  -1   1   1   1  -1
W1= [0.05818722,0.05818722,0.05818722,0.05818722,0.05818722,0.05818722,0.19756314,0.19756314,0.19756314,0.05818722]

#通过计算可得出信息增益最大的是以x=5.5作为划分特征属性的时候
#G2(x)在训练集上的误差率为 ε2
ε2 = 0.05818722*4
alpha2 = errorate(ε2)
print('以5.5作为划分特征属性的误差率:{},G2的系数为alpha2:{}'.format(ε2,alpha2))
print('f(x)=%.4fG1(x)+%.4G2(x)'%(alpha1,alpha2))
def G2(x, t):
    if x > t:
        return 1
    else:
        return -1


D2 = []
def updateWeight(alpha, X, Y, W,t):
    S = 0
    for i in range(len(X)):
        d = W[i] * pow(np.e, (-1) * alpha * Y[i] * G2(X[i],t))
        D2.append(d)
    for j in D2:
        S += j
    return D2 / S
newWeight2 = updateWeight(alpha2,X,Y,W1,5.5)
print(newWeight2)
print("="*50)
#得到的新的权重值
#X  0    1   2   3   4   5   6   7   8   9
#Y  1    1   1  -1  -1  -1   1   1   1  -1
W2=[0.19710632,0.19710632,0.19710632,0.03526245,0.03526245,0.03526245,0.03526245,0.03526245,0.03526245,0.19710632]


#通过计算可得出信息增益最大的是以x=5.5作为划分特征属性的时候
#G3(x)在训练集上的误差率为 ε3
ε3 = 0.0281336*3
alpha3 = errorate(ε3)
print('以8.5作为划分特征属性的误差率:{},G3的系数为alpha3:{}'.format(ε3,alpha3))

def G3(x, t):
    if x < t:
        return 1
    else:
        return -1


D3 = []
def updateWeight(alpha, X, Y, W,t):
    S = 0
    for i in range(len(X)):
        d = W[i] * pow(np.e, (-1) * alpha * Y[i] * G3(X[i],t))
        D3.append(d)
    for j in D2:
        S += j
    return D3 / S
newWeight3 = updateWeight(alpha3,X,Y,W2,5.5)
print('f(x)=%.4fG1(x)+%.4fG2(x)+%.4fG3(x)'%(alpha1,alpha2,alpha3))
print(newWeight3)
print("="*50)
#得到的新的权重值
#X  0    1   2   3   4   5   6   7   8   9
#Y  1    1   1  -1  -1  -1   1   1   1  -1
W2=[0.04035885,0.04035885,0.04035885,0.22503901,0.22503901,0.22503901,0.22503901,0.22503901,0.22503901,0.04035885]

#计算结果显示
'''
以x=2.5为信息熵得到的信息增益0.281
以x=5.5作为划分特征属性的信息增益0.046
以x=8.5为划分特征属性的信息增益0.144
以2.5作为划分特征属性的误差率:0.3,G1的系数为alpha1:0.611196210668224
f(x)=0.6112G1(x)
[0.05818722 0.05818722 0.05818722 0.05818722 0.05818722 0.05818722
 0.19756314 0.19756314 0.19756314 0.05818722]
==================================================
以5.5作为划分特征属性的误差率:0.23274888,G2的系数为alpha2:0.8604623106008397
f(x)=0.6112G1(x)+0.86052(x)
[0.15725825 0.15725825 0.15725825 0.0281336  0.0281336  0.0281336
 0.09552206 0.09552206 0.09552206 0.15725825]
==================================================
以8.5作为划分特征属性的误差率:0.0844008,G3的系数为alpha3:1.7196938117660618
f(x)=0.6112G1(x)+0.8605G2(x)+1.7197G3(x)
[0.04035885 0.04035885 0.04035885 0.22503901 0.22503901 0.22503901
 0.22503901 0.22503901 0.22503901 0.04035885]
'''
~~~

##### Adaboost总结

Adaboost的优点如下：

>可以处理连续值和离散值
>
>模型的鲁棒性比较强
>
>解释强，结构简单

缺点：

>对异常样本敏感，异常样本可能会在迭代过程中获得较高的权重值，最终影响模型效果。

#### GBDT(迭代决策树)

##### GBDT

>GBDT也是Boosting算法的一种，但是和Adaboost算法不同：
>
>Adaboost算法是利用前一轮学习器的误差来更新样本权重值，然后一轮一轮的迭代，GBD也是迭代，但是GBDT要求弱学习器必须是CART模型，而且GBDT在模型训练的时候，是要求模型预测的样本损失尽可能的小。
>
>GBDT中，底层都是回归树。

##### 梯度提升迭代决策树GBDT

>GBDT由但部分构成：DT(Regression Decision Tree)，GB(Gradient Boosting)，和Shrinkage（衰减）
>
>DT(回归树)：
>
>​	回归树和分类树的区别（前者用于预测实数值，如明天的温度，用户的年龄，网页的相关程度；后者用于分类标签值，如晴天/阴天/雾/雨，用户性别、网页是否为垃圾页面。）GBDT中用得是CART，这是一种回归树。GBDT的核心在于累加所有树的结果作为最终的结果，而分类树累加没有意义。所以GBDT是由回归树构成的。（尽管GBDT调整后也可以用于分类但不代表GBDT的树是分类树）。
>
>GB(梯度迭代)：
>
>​	GBDT的核心在于，每一刻树学的是之前所有树结论和残差，这个残差就是一个假预测值后能得到的真实值的累加量。ep：比如A的真实年龄是18岁，但第一棵树的预测年龄是14岁，差了4岁，即残差为4岁。那么在第二棵树利我们把A的年龄设置为4岁去学习，如果第二棵树真的能把A分到4岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的年龄是3岁，则A任然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。这就是Gradient Boosting在GBDT中的意义。
>
>Shrinkage(缩减)：
>
>​	Shrinkage的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合，即它不完全相信每一棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。
>
>由多棵决策树组成，所有树的结果累加起来就是最终结果
>
>迭代决策树和随机森林的区别：
>
>​	随机森林使用抽取不同的样本构建不同的子树，也就是说第m棵树的构建和前m-1棵树的结果是没有关系的
>
>​	迭代决策树在构建子树的时候，使用之前子树构建结果后形成残差作为输入数据构建下一个子树，然后最终预测的时候按照子树构建的顺序进行预测，并将预测结果相加 

##### GBDT算法原理

给定输入向量X和输出向量Y组成若干训练样本$(X_1,Y_1),(X_2,Y_2),...,(X_n,Y_n)$,目标是找到近似函数F(X),使损失函数L（Y，F(X))的损失之最小。

L损失函数一般采用最小二乘法函数或者绝对值损失函数。
$$
L(y,F(X))=\frac{1}{2}\left(y-F(X)\right)^2\\
L(y,F(X))=|Y-f(X)|
$$
最优解为：
$$
F^*(X)=\underset{F}{argmin}L(y,F(X))
$$
假定F(X)是一组最优基函数$f_i(X)$的加权和：
$$
F(X)=\sum_{i=1}^Mf_i(X)\overset{防止每个学习器能力过强导致过拟合,给定一个缩放系数v}\Longrightarrow F(X)=v\sum_{i=0}^Mf_i(X)
$$
以贪心算法（局部最优解）的思想扩展得到$F_m(X)$,求解最优f
$$
F_m(x)=F_{m-1}(x)+\underset{f} {argmin}\sum_{i=1}^nL(y_i,F_{m-1}(X_i)+f_m(X_i))
$$
给定常函数F_0(X)
$$
F_0(X)=\underset{c}{argmin}\sum_{i=1}^nL(y_i,c)
$$
根据梯度值下降计算y的变化值
$$
\alpha_{im}=\left[\frac{\part L(y_i,F(x_i))}{\part F(x_i)}\right]F(x)=F_{m-1}(x)
$$
使用数据$(x_i,\alpha_m)(i=1,2,....,n)$计算拟合残差找到一个CART回归树，得到第m棵树
$$
f_m(x)=f_{m-1}(x)+\sum_{j=1}^{|leaf|_m}c_{mj}I(x)(\in leaf_{mj})
$$

##### GBDT回归算法和分类算法的区别

两者唯一的区别就是选择不同的损失函数

回归算法中选择的损失函数一般是均方差（最小二乘）或者绝对值误差值；而在分类算法重一般的损失函数选择对函数来表示。
$$
L(y,f(x))=\frac{1}{2}(y-f(x))^2\\
L(y,f(x))=\ln(1+e^{(-y^*f(x))});y\in\{-1,+1\}\\
L(y,f(x))=-\sum_{k=1}^Klog(p_k(x));K分类中
$$

##### GBDT总结

优点：

可以处理连续值和离散值，在相对较少的调参情况下，模型的预测效果也会不错；模型的鲁棒性比较强。

缺点：

由于弱学习器之间存在关联关系，难以并行训练模型。也就是模型训练的速度慢。
