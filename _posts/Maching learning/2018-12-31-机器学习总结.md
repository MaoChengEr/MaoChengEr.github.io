---
layout: post
comments: true
categories: 机器学习
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

* content
{:toc}

### 1.数学基础，python计算库回顾

#### 1.1常见函数

常函数、一次函数、二次函数、幂函数、指数函数、对数函数

#### 1.2导数、梯度（求导的方式、导数/梯度的含义/作用）

##### 1.2.1导数

​	一个函数在某一点的导数描述了这个函数在这一点附近的变化率，也可以认为是函数在某一点的导数就是该函数所代表的的曲线在这一点的切线斜率。导数值越大，表示函数在该点的变化越大。

定义：当函数$y=f(x)$在自变量$x=x_0$上产生一个增量$\Delta x$时，函数输出值得增量$\Delta y$和自变量增量$\Delta x$之间的壁纸在$\Delta x$趋近与0的时候存在极限值a，那么a即为函数在$x_0$的到数值。
$$
lim_{\Delta x\rightarrow0}\frac{f(x_0+\Delta x)}{\Delta x}=lim_{\Delta x\rightarrow 0}\frac{\Delta y}{\Delta x}
$$
​	导数就是曲线的斜率，是曲线变化快慢的一个反应。

​	二阶导数是斜率变化的反应，表现曲线的凹凸性。

##### 1.2.2偏导数

​	在一个多变量的函数中，偏导数就是关于其中一个变量的导数而保持其他变量恒定不变。假定二元函数$z=f(x,y)$,点$(x_0,y_0)$是其定义域内一个点，经y固定在$y_0$上，而x在$x_0$上增量$\Delta x$，相应的函数z有增量$\Delta z=f(x_0+\Delta x,y_0)-f(x_0,y_0)$;$\Delta z$和$\Delta x$的比值当$\Delta x$的值趋近于0的时候，如果极限存在，那么次极限称为函数$z=f(x,y)$在$x_0$处对x的偏导数（partial derivatice），记作：$f^{'}x(x_0,y_0)$

对x的偏导数：
$$
\left.\begin{matrix}
\frac{\partial f}{\partial x} 
\end{matrix}\right|_{{x=x_0}\\{y=y_0}}
$$
对y的偏导数：
$$
\left.\begin{matrix}
\frac{\partial f}{\partial y} 
\end{matrix}\right|_{{x=x_0}\\{y=y_0}}
$$
梯度：

​	梯度是一个向量，表示某一个函数在该点处的方向导数沿着该方向取最大值，即函数在该点处沿着该方向变化最快，变化率最大（即该梯度向量的模）；当函数为一维函数的时候，梯度其实就是导数。
$$
\bigtriangledown f(x_1,x_2)=\left\{\frac{\partial f(x_1,x_2)}{\partial x_1},\frac{\partial f(x_1,x_2)}{\partial x_2}\right\}
$$


#### 1.3Taylor公式

​	Taylor泰勒公式是用一个函数在某点的信息来描述其附近取值的公式。如果函数足够平滑，已知函数在某一点的各阶导数值得情况下，Taylor公式可以利用这些导数值来做一个多项式近似函数在这一点的领域中的值。

​	若函数f(x)在包含$x_0$的某个闭区间[a,b]上具有n阶函数，且在开区间(a,b)上具有n+1阶函数，则对闭区间[a,b]上任意一点x,有Taylor公式如下：$f^{(n)}(x)$表示f(x)的n阶导数，$R_n(x)$是Taylor公式的余项，是$(x-x_0)^n​$的高阶无穷小。

​	备注：Taylor公式是一种多项式近拟合的方式。用一个多项式的值去逼近某个函数。
$$
f(x)=\frac{f(x_0)}{0!}+\frac{f(x_1)}{1!}(x-x_0)+\frac{f(x_2)}{2!}(x-x_0)^2...+\frac{f(x_n)}{n!}(x-x_0)^n+R_n(x)
$$

#### 1.4联合概率、条件概率、全概率公式、贝叶斯公式

##### 1.4.1古典概率

​	概率是以假设为基础的，即假定随机现象所发生的时间是有限的，互补相容的，而且每个基本事件的发生是等可能性的。一般来讲，如果在全部可能出现的基本事件范围内构成事件A的基本事件有a个，不构成事件A的有b个，那么事件A出现的概率为：
$$
P(A)=\frac{a}{a+b}
$$
概率体现的是随机事件A发生的大小度量(数值)。

##### 1.4.2联合概率

​	表示两者事件发生的概率，事件A和事件B的共同的概率记作:P(AB),P(A,B)或者$P(A\cap B)​$,读作：“事件A和事件B同时发生的概率”

##### 1.4.3条件概率

​	事件A在另外一个时间B发生的条件下的发生的概率叫做条件概率，表示为P(A|B)，读作“在B条件下A发生的概率”，一般情况下P(A|B)$\neq$P(A)，而且条件概率具有三个特性：非负性、可列性、可加性。

​	将条件概率公式由两个时间推广到任意无穷多个事件时，，可以得到如下公式，假设$A_1,A_2,...,A_n$为n个任意事件(n$\geq$2),而且$P(A_1A_2A..A_n)$>0.则：
$$
P(A_1A_2...A_n)=P(A_1)P(A_2|A_1)...P(A_n|A_1A_2...A_{n-1})
$$

##### 1.4.4全概率公式

​	样本空间$\Omega$有一组事件$A_1、A_2、...、A_n$,如果事件满足下列两个条件，那么事件组称为样本空间的一个划分：
$$
\forall i\neq j\in\left\{1,2,...,n\right\},A_iA_J=\phi\\
A_1\cup A_2...\cup A_n=\Omega
$$
设事件$\{A_j\}$是样本空间$\Omega$的一个样本划分，且$P(A_i)>0$,那么对于任意事件B，全概率公式为：
$$
P(B)=\sum_{i=1}^nP(A_i)P(B|A_i)
$$

##### 1.4.5贝叶斯公式

设$A_1 、A_2 ...A_n ​$是样本空间Ω的一个划分，如果对任意事件B而言，有P(B)>0，那么：
$$
P(A_i|B)=\frac{P(B,A_i)}{P(B)}=\frac{P(A_i)*P(b|A_i)}{\sum_{j=1}^nP(A_j)*P(B|A_j)}
$$

#### 1.5期望、方差、协方差（了解这三样东西有什么特性）

##### 1.5.1期望

期望：也就是均值，是概率加权下的“平均值”。是每次可能的概率乘以其结果的综合，反应的是随机变量平均取值大小。常用符号$\mu$表示。

连续性数据：
$$
E(X)=\int_{-\infty}^{\infty}xf(x)dx
$$
离散型数据：
$$
E(X)=\sum_ix_ip_i
$$
假设C为一个常数，X和Y是两个随机变量，那么期望有以下性质：
$$
E(C)=C \quad E(CX)=CE(X)\\
E(X+Y)=E(X+E(Y))\\
$$
如果X和Y相互独立，那么$E(XY)=E(X)E(Y)$

如果$E(XY)=E(X)E(Y)$，那么X和Y无关

##### 1.5.2方差

​	方差(variance)是衡量随机变量或一组数据时离散程度的度量，是用来度量随机变量和其数学期望之间的偏离程度。即方差是衡量数据原数据和期望/均值相差的度量值。
$$
Var(X)=D(X)=\sigma^2=\frac{\sum(x-\mu)^2}{N}\\
D(X)=\sum_{i=1}^np_i(x-\mu)^2\quad D(X)=\int_a^b(x-\mu)^2f(x)dx\\
D(X)=E((X-E(X))^2)=E(X^2)-(E(X))^2
$$
假设C是一个常数，X和Y是两个随机变量，那么方差有以下性质：
$$
D(C)=0\quad D(CX)=C^2D(X)\quad D(C+X)=D(X)\\
D(X+Y)=D(X)+D(Y)\pm 2Cov(X,Y)\\
$$
协方差：$Cov(X,Y)=E\{(X-E(X))(Y-E(Y))\}$

如果X和Y不想管，那么$D(X\pm Y)=D(X)+D(Y)​$

##### 1.5.3标准差

标准差（Standard Deviation）是离均值平方的算数平均数的平方根，用符号$\sigma$表示，其实标准差就是方差的算数平方根。

标准差和方差都是测量离散趋势的最重要、最常见的指标。标注差和方差的不同点在于，标准差和变量的计算单位是相同的，比方差清楚，因此在很多分析的时候使用的是标准差。
$$
\sigma=\sqrt{D(X)}=\sqrt{\frac{\sum(x-\mu)^2)}{N}}
$$

##### 1.5.4协方差

协方差常用于衡量两个变量的总体误差；当两个变量相同的情况下，协方差其实就是方差。

如果X和Y是统计独立的，那么二者之间的协方差为零。但是如果协方差为零，那么X和Y是不相关。
$$
Cov(X,Y)=E[(X-E(X)(Y-E(Y))]\\=E[XY-XE(Y)-YE(X)+E(X)E(Y)]\\
=E(XY)-E(X)E(Y)
$$

假设C为一个常数，X和Y是两个随机变量，那么协方差有性质如下所示：
$$
Cov(X,Y)=Cov(Y,X)\\
Cov(aX,bY)=abCov(X,Y)\\
Cov(X_1+X_2,Y)=Cov(X_1,Y)+Cov(X_2,Y)
$$
协方差是两个随机变量具有相同方向趋势的度量：

若$Cov(X,Y)>0$,则X和Y变化趋势相同；	
若$Cov(X,Y)<0$,则X和Y变化趋势相反；	
若$Cov(X,Y)=0$,则X和Y不相关，也就是变化没什么相关性；

##### 1.5.4协方差矩阵

对于n个随机向量$(X_1,X_2,X_3,...,X_n)$,任意两个元素$X_i$和$X_j$都可以得到一个协方差，从而形成一个n*n的矩阵，该矩阵就叫做协方差矩阵，协方差矩阵为对称矩阵。
$$
c_{ij}=E\{[X_i-E(X_i)][X_j-E(X_j)]\}=Cov(X_i,X_j)\\
c=\begin{bmatrix}
c_{11}&c_{12}\quad...\quad c_{1n} \\ 
c_{21}&c_{22}\quad...\quad c_{2n} \\
...&...\quad...\quad ... \\ 
c_{n1}&c_{n2}\quad...\quad c_{nn} \\ 
\end{bmatrix}
$$


#### 1.6大数定理、中心极限定理

##### 1.6.1大数定理

大数定理：**随着样本容量n的增加，样本平均数将接近于总体平均数（期望 $\mu$）,所以在统计推断中，一般都会使用样本平均数估计总体平均数的值。**

也就是我们会使用一部分样本的平均值来代替整体样本的期望/均值，出现偏差的可能是存在的，但是当n足够大的时候，偏差的可能性是非常小的，当n无限大的时候没这种可能性的概率为0。

大数定理的主要作用就是为使用频率来估计概率提供了理论支持，为使用部分数据来近似的模拟构建全部数据特征提供了理论支持。

##### 1.6.2中心极限定理

中心极限定理（Centeral Limit Theorem）：假设$\{X_n\}$为独立同分布的随机变量序列，并具有相同的期望$\mu$和方差为$\sigma^2$,则$\{X_n\}$服从中心极限定理，且${Z_n}$为随机序列$\{X_n\}$的规范和：

$$
Y_n=X_1+X_2+...+X_n=\sum_{i=1}^nX_i\rightarrow N(n\mu,n\sigma^2)\\
Z_n=\frac{Y_n-E(y_N)}{\sqrt{D(Y_n)}}=\frac{Y_n-n\mu}{\sqrt{n}\sigma}\rightarrow N(0,1)
$$
中心极限定理就是一般在同分布的情况下，抽样样本值的规范和在总体数量趋于无穷时的极限分布近似于正态分布。

#### 1.7最大似然估计（MLE）（最大似然估计必须掌握）

最大似然法（Maximum Likelihood Estimation，MLE）也称为最大概似估计、极大似然估计，是一种具有理论性的参数估计方法。基本思想是：当从模型总体随机抽取n组样本观测值后，最合理的参数估计量应该是的从模型中抽取该n组样本观测值的概率最大化；一般步骤如下：

1.写出似然函数
2.对似然函数取对数，并整理
3.求导数
4.解似然函数

设总体分布为$f(x.\theta),\{X_n\}​$为该总体采样得到的样本，因为随机序列${X_n}​$独立同分布，则他们的联合密度函数为：
$$
L(x_1,x_2,...x_n;\theta_1,\theta_2,...,\theta_n)=\prod_{i=1}^nf(x_i;\theta_1,\theta_2,...,\theta_n)
$$
这里的$\theta$可以看做固定但是位置的参数，反过来，因为样本已经存在，可以看做$\{X_n\}$是固定的，$L(x,\theta)$是关于$\theta$的函数，即似然函数；

求参数$\theta$的值，使得似然函数去最大值，这种方法叫做最大似然估计法。

若给定一组样本$\{X_n\}$,已知随机样本符合高斯分布$N(\mu,\sigma^2)$,试估计$\sigma$和$\mu$的值
$$
f(x)=\frac{1}{\sqrt{2\pi}\sigma}*\exp^{-\frac{(x-\mu)^2}{2\sigma^2}}\quad
L(x)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}*\exp^{-\frac{(x-\mu)^2}{2\sigma^2}}\\
l(x)=\log(L(x))=\log\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}*\exp^{-\frac{(x-\mu)^2}{2\sigma^2}}=\sum_{i=1}^n\log\left\{\frac{1}{\sqrt{2\pi}\sigma}*\exp^{-\frac{(x-\mu)^2}{2\sigma^2}}\right\}\\
=\sum_{i=1}^n\log\left\{\frac{1}{\sqrt{2\pi}\sigma}\right\}-\sum_{i=1}^n\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}=-\frac{n}{2}\log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i}(x-\mu)^2\\
$$

$$
l(x)=-\frac{n}{2}\log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x-\mu)^2=\\-\frac{n}{2}\log(2\pi)-\left[n\log(\sigma)+\frac{1}{2\sigma^2}\sum_i(x-\mu)^2\right]
=-\frac{n}{2}\log(2\pi\sigma^2)-\frac{1}{2\pi\sigma^2}\sum_i(x-\mu)^2
$$

要求似然函数$l(x)$最大，即$l(x)$求极值即可，将似然函数对参数$\mu$和$\sigma$分别求偏导数
$$
\frac{l(x)}{d\mu}=-\frac{1}{\sigma^2}\sum_i(x_i-\mu)=0\Rightarrow\mu=\frac{1}{n}\sum_ix_i\\
\frac{l_2(x)}{d\sigma}=\frac{\pi}{\sigma}-\frac{\sum_i(x_i-\mu)^2}{\sigma^3}=0\Rightarrow\sigma^2=\frac{1}{n}\sum_i(x_i-\mu)^2
$$

#### 1.8向量、矩阵的运算

##### 1.8.1向量的运算

设量向量为：$\overset{\rightarrow}a=(x_1,y_1),\overset{\rightarrow}b=(x_2,y_2)​$

向量的加法/减法满足平行四边形和三角形法则
$$
\overset{\rightarrow}a+\overset{\rightarrow}b=(x_1+x_2,y_1+y_2)\\
\overset{\rightarrow}a-\overset{\rightarrow}b=(x_1-x_2,y_1-y_2)\\
$$
数乘：实数$\lambda$和向量a的叉乘乘积还是一个向量，记作$\lambda a$,且$|\lambda a|=\lambda|a|$;数乘的集合意义是将向量a进行伸长或者压缩操作
$$
\lambda \overset{\rightarrow}a=(\lambda x_1,\lambda y_1)
$$
设两向量为：$\overset{\rightarrow}a=(x_1,y_1),\overset{\rightarrow}b=(x_2,y_2)​$,并且a和b之间的夹角为：$\theta​$

数量级：两个向量的数量积（内积，点积）是一个数量/实数。记作:$\overset{\rightarrow}a*\overset{\rightarrow}b$
$$
\overset{\rightarrow}a*\overset{\rightarrow}b=|\overset{\rightarrow}a|*|\overset{\rightarrow}b|*cos\theta
$$

向量积：两个向量的向量积（外积，叉积）是一个向量，记作$\overset{\rightarrow}a \times \overset{\rightarrow}b​$,向量积即两个不共线非零向量所在平面的一组法向量。
$$
|\overset{\rightarrow}a \times \overset{\rightarrow}b|=|\overset{\rightarrow}a|*|\overset{\rightarrow}b|*sin\theta
$$

##### 1.8.2矩阵的直观表示

数域F中m\*n个数排成m行n列，并括以圆括弧(或方括弧)的数表示成为数域F上的矩阵，通常用大写字母记作A或者$A_{m*n}$，有时也记作$A=(a_ij )_{m*n}(i=1,2…,m;j=1,2,…n)$，其中$a_{ij}$ 表示矩阵A的第i行的第j列元素，当F为实数域R时，A叫做实矩阵，当F为复数域C时，A叫做复矩阵。
$$
A=\begin{Bmatrix}
a_{11}& a_{12}&...&a_{1n}\\ 
a_{21}& a_{22}&...&a_{2n}\\
...& ...&...&...\\
a_{m1}& a_{m2}&...&a_{mn}
\end{Bmatrix}
$$

###### 1.8.2.1矩阵的加减法

矩阵的加法与减法要求进行操作的两个矩阵A和B具有相同的阶，假设A为m\*n阶矩阵，B为m\*n阶矩阵，那么$C=A\pm B$也是m*n阶的矩阵，并且矩阵C的元素满足：$c_{ij}=a_{ij}\pm b_{ij}$
$$
A=\begin{Bmatrix}
a_{11}& a_{12}&...&a_{1n}\\ 
a_{21}& a_{22}&...&a_{2n}\\
...& ...&...&...\\
a_{m1}& a_{m2}&...&a_{mn}
\end{Bmatrix} \quad B=\begin{Bmatrix}
b_{11}& b_{12}&...&b_{1n}\\ 
b_{21}& b_{22}&...&b_{2n}\\
...& ...&...&...\\
b_{m1}&b_{m2}&...&b_{mn}
\end{Bmatrix}\\
C=A\pm B=\begin{Bmatrix}
a_{11}+b_{11}& a_{12}+b_{12}&...&a_{1n}+b_{1n}\\ 
a_{21}+b_{21}& a_{22}+b_{22}&...&a_{2n}+b_{2n}\\
...& ...&...&...\\
a_{m1}+b_{m1}& a_{m2}+b_{m2}&...&a_{mn}+b_{mn}
\end{Bmatrix}
$$
**矩阵加法符合交换律和结合律**

###### 1.8.2.2矩阵与数的乘法

数乘：将数$\lambda$与矩阵A相乘，就是将数$\lambda$与矩阵A中的每一个元素相乘，记作$\lambda A$；结果$C=\lambda A$，并且C中的元素满足：$c_{ij}=\lambda a_{ij}$
$$
A=\begin{Bmatrix}
a_{11}& a_{12}&...&a_{1n}\\ 
a_{21}& a_{22}&...&a_{2n}\\
...& ...&...&...\\
a_{m1}& a_{m2}&...&a_{mn}
\end{Bmatrix}\quad \lambda A=\begin{Bmatrix}
\lambda a_{11}& \lambda a_{12}&...&\lambda a_{1n}\\ 
\lambda a_{21}& \lambda a_{22}&...&\lambda a_{2n}\\
...& ...&...&...\\
\lambda a_{m1}& \lambda a_{m2}&...&\lambda a_{mn}
\end{Bmatrix}
$$


**矩阵运算律满足结合律和分配率**

交换律：
$$
A+B=B+A\\
(A+B)+C=A+(B+C)
$$


结合律：
$$
(\lambda \mu)=\lambda(\mu A)
$$
分配率：
$$
(\lambda+\mu)A=\lambda A+\mu A\\
\lambda(A+B)=\lambda A+\lambda B
$$
###### 1.8.2.3矩阵与向量的乘法

假设A为m\*n阶矩阵，x为n\*1的列向量，则Ax为m\*1的列向量，记作：$\overset{\rightarrow}y=A\overset{\rightarrow}x​$
$$
A=\begin{Bmatrix}
a_{11}& a_{12}&...&a_{1n}\\ 
a_{21}& a_{22}&...&a_{2n}\\
...& ...&...&...\\
a_{m1}& a_{m2}&...&a_{mn}
\end{Bmatrix}\quad \overset{\rightarrow}x=\begin{Bmatrix}
x_1\\x_2\\...\\x_n
\end{Bmatrix}\quad \overset{\rightarrow}y=A\overset{\rightarrow}x=\begin{Bmatrix}
y_1\\y_2\\...\\y_n
\end{Bmatrix}\
$$

$$
y_i=\sum_{j=1}^na_{ij}x_j
$$

###### 1.8.2.4矩阵与矩阵的计算

矩阵的乘法仅当第一个矩阵A的列数和第二个矩阵B的行数相等时才能够定义，假设A为m\*s阶矩阵，B为s\*n阶矩阵，那么C=A\*B是m*n阶矩阵，并且矩阵C中的元素满足:$c_{ij}=\sum_{k=1}^sa_{ik}b_{kj}​$
$$
A=\begin{Bmatrix}
a_{11}& a_{12}&...&a_{1n}\\ 
a_{21}& a_{22}&...&a_{2n}\\
...& ...&...&...\\
a_{m1}& a_{m2}&...&a_{mn}
\end{Bmatrix} \quad B=\begin{Bmatrix}
b_{11}& b_{12}&...&b_{1n}\\ 
b_{21}& b_{22}&...&b_{2n}\\
...& ...&...&...\\
b_{m1}&b_{m2}&...&b_{mn}
\end{Bmatrix}\quad \\
C=A*B=\begin{Bmatrix}
c_{11}&c_{12}&...&c_{1n}\\
c_{21}&c_{22}&...&c_{2n}\\
...&...&...&...\\
c_{m1}&c_{22}&...&c_{mn}\\
\end{Bmatrix}
$$

$$
(AB)C=A(BC)\quad(A+b)C=AC+BC\quad C(A+B)=CA+CB
$$

###### 1.8.2.5矩阵的转置

矩阵的转置：把矩阵A的行和列互相交换所产生的矩阵称为A的转置矩阵，这一过程叫做矩阵的转置。 使用$A^T $表示A的转置。
$$
A=\begin{Bmatrix}
a_{11}& a_{12}&...&a_{1n}\\ 
a_{21}& a_{22}&...&a_{2n}\\
...& ...&...&...\\
a_{m1}& a_{m2}&...&a_{mn}
\end{Bmatrix}\quad A^T=\begin{Bmatrix}
a_{11}& a_{21}&...&a_{m1}\\ 
a_{12}& a_{22}&...&a_{m2}\\
...& ...&...&...\\
a_{1n}& a_{2n}&...&a_{mn}
\end{Bmatrix}
$$
转置的运算性质：
$$
(A^T)^T=A\\
(\lambda A)^T=\lambda A^T\\
(AB)^T=B^TA_T\\
(A+B)^T=A^T+B^T
$$


#### 1.9向量、矩阵的求导

##### 1.9.1向量的导数

A为m\*n的矩阵，x为n\*1的列向量，则Ax为m\*1的列向量，记作:$\overset{\rightarrow}y=A*\overset{\rightarrow}x​$
$$
A=\begin{Bmatrix}
a_{11}& a_{12}&...&a_{1n}\\ 
a_{21}& a_{22}&...&a_{2n}\\
...& ...&...&...\\
a_{m1}& a_{m2}&...&a_{mn}
\end{Bmatrix}\quad \overset{\rightarrow}x=\begin{Bmatrix}
x_1\\x_2\\...\\x_n
\end{Bmatrix}\\ \overset{\rightarrow}y=\begin{Bmatrix}
a_{11}*x_1& a_{12}*x_2&...&a_{1n}*x_n\\ 
a_{21}*x_1& a_{22}*x_2&...&a_{2n}*x_n\\
...& ...&...&...\\
a_{m1*x_1}& a_{m2}*x_2&...&a_{mn}*x_n
\end{Bmatrix}\\
\frac{\partial\overset{\rightarrow}y}{\partial\overset{\rightarrow}x}=\frac{\partial A\overset{\rightarrow}x}{\partial\overset{\rightarrow}x}=\begin{Bmatrix}
a_{11}&a_{21}&..&a_{m1}\\
a_{21}&a_{22}&..&a_{m2}\\
...&...&..&...\\
a_{1n}&a_{2n}&..&a_{mn}\\
\end{Bmatrix}=A^T
$$

###### 1.9.1.1向量的偏导公式

$$
\frac{\partial A\overset{\rightarrow}x}{\partial\overset{\rightarrow}x}=A^T\\
\frac{\partial A\overset{\rightarrow}x}{\partial\overset{\rightarrow}x^T}=A\\
\frac{\partial(\overset{\rightarrow}xA)}{\partial\overset{\rightarrow}x}=A\\
$$

###### 1.9.1.2标量对向量的导数

A为n\*n的矩阵，x为n\*1的列向量，记：$y={\overset{\rightarrow}x}^T*A*\overset{\rightarrow}x​$

同理可得：
$$
\frac{\partial y}{\partial x}=\frac{{\partial(\overset{\rightarrow}x}^T*A*\overset{\rightarrow}x)}{\partial \overset{\rightarrow}x}=(A^T+A)*\overset{\rightarrow}x
$$
若A为对称矩阵，则有
$$
\frac{{\partial(\overset{\rightarrow}x}^T*A*\overset{\rightarrow}x)}{\partial \overset{\rightarrow}x}=2A\overset{\rightarrow}x
$$

$$
A=\begin{Bmatrix}
a_{11}& a_{12}&...&a_{1n}\\ 
a_{21}& a_{22}&...&a_{2n}\\
...& ...&...&...\\
a_{m1}& a_{m2}&...&a_{mn}
\end{Bmatrix}\quad \overset{\rightarrow}x=\begin{Bmatrix}
x_1\\x_2\\...\\x_n
\end{Bmatrix}\\
$$

$$
\overset{\rightarrow}x^T*A*\overset{\rightarrow}x=(x_1,x_2,...,x_n)\left\{\sum_{j=1}^na_{1j}x_j,\sum_{j=1}^na_{2j}x_j,...,\sum_{j=1}^na_{nj}x_j\right\}^T\\=\sum_{i=1}^n\sum_{j=1}^na_{ij}x_ix_j
$$

$$
\frac{\partial \overset{\rightarrow}x^T*A*x_n}{\partial \overset{\rightarrow}x}=\left\{\sum_{j=1}^na_{ij}x_j\right\}+\left\{\sum_{j=1}^na_{ij}x_j\right\}=\sum_{j=1}^n(a_{ij}+a_{ji})x_j
$$

##### 1.9.2标量对方阵的导数

A为n\*n的矩阵，|A|为A 的行列式，计算$\frac{\partial |A|}{\partial A}$
$$
\forall 1\leq i\leq n,|A|=\sum_{j=1}^na_{ij}*(-1)^{i+j}M_{ij}\\
\frac{\partial|A|}{\partial a_{ij}}=\frac{\partial(\sum_{j=1}^na_{ij}*(-1)^{i+j}M_{ij})}{\partial a_{ij}}=(-1)^{i+j}M_{ij}=A^*_{ji}\\
\frac{\partial|A|}{\partial A}=(A*)^T=|A|*(A^{-1})^T
$$

##### 1.9.3梯度下降法

梯度下降法（Gradient Descent,GD）常用于求解无约束情况下凸函数（Convex Function
）的极小值，是一种迭代类型的算法，因为凸函数只有一个极值点，故求解出来的极小值点就是函数的最小值点。
$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^n(h_{\theta}(x^{(i)})-y^{(i)})^2\\
\theta^*=argmin_{\theta}J(\theta)
$$
梯度下降法的优化思想是当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降的方向，所以梯度下降法也称为“最速下降法”。梯度下降法中越接近目标值，变量变化越小。计算公式如下：
$$
\theta^{k+1}=\theta^k-\alpha\bigtriangledown f(\theta^k)
$$
$\alpha$被称为步长或者学习率（learning rate），表示自变量x每次迭代变化的大小。

收敛条件：当目标函数的函数值变化非常小的时候或者达到最大迭代次数的时候，就结束循环。



#### 1.10QR分解、SVD分解

##### 1.10.1QR分解

QR分解是将矩阵分解为一个正交矩阵与上三角矩阵的乘积

![2019-01-30_155348](Scrshot/2019-01-30_155348.png)

这其中，Q为正交矩阵，$Q^TQ=1​$，R为上三角矩阵。

实际中，QR分解经常被用来解线性最小二乘问题。

##### 1.10.2SVD分解

奇异值分解（Singular Value Decomposition）是一种重要的矩阵分解方法，可以看做是对称方阵在任意矩阵上的推广。

假设A为一个m\*n阶实矩阵‘，则存在一个分解使得：
$$
A_{m*n}={U_{m*n}{\sum}_{m*n}V_{n*n}}^T
$$


通常将奇异值由大到小排列，这样$\sum$便能由A唯一确定了。

#### 1.11python计算科学库回顾

Python科学计算库主要是为机器学习提供了一些便捷、封装好的API，那么在实际工作中，主要是将其应用在机器学习的特征工程阶段，主要涉及到的库有以下几个：

Numpy-数学计算基础库：N维数组、线性代数计算、傅里叶变换、随机数等。

SciPy-数值计算库：香型代数、拟合与优化、插值、数值积分、稀疏矩阵、图像处理、统计等。

Pandas-数据分析库：数据导入、整理、处理、分析等。

Matplotlib-会图库：绘制二维图形和图表。