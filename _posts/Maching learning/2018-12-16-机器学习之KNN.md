---
layout: post
comments: true
categories: 机器学习之KNN
---

### <font color=salmon size=5 face="微软雅黑">机器学习之KNN</font>

#### KNN算法

##### KNN算法原理

K近邻（K-nearst neighbors,KNN）是一种基本的机器学习算法，所谓的k近邻，就是K个最近邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。

KNN做分类预测时，一般采用多数表决法；而在做回归预测时，一般采用平均值法。

从训练集合中获取k个样本数据来预测当前带预测样本的目标属性值。

##### 交叉验证

~~~
交叉验证：就是重复的数据，吧得到的样本数据进行切分，组合为不同的训练集和测试机，用训练集来训练模型，用测试机来评估模型预测的好坏。在此基础上可以得到不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试机中的样本，即所谓的“交叉”。
应用场景：一般都是数据不够充分的时候。一般的当数据小于一万条以下时，我们会用交叉验证来训练优化选择模型。
		对于大于一万条的数据，我们将数据分成三个不同的数据集，分别为训练集，验证集，最后一个为测试集。			用训练集来训练模型，用验证集来评估预测模型的好坏和选择模型以及其对应的参数。把最终得到的模型在			用于测试集，最终决定使用哪个模型以及对应参数。
根据数据的切分方法，交叉验证分为三种：
		简单交叉验证：将数据分为两个部分，一般的70%为训练集，30%为测试集。然后用训练集来训练模型，在测试集上验证模型以及参数。接着，吧样本打乱，重新选择训练集和测试机，继续训练数据和检验模型，最后我们选择损失函数评估最有的模型和参数。
		s折交叉验证：先把数据分成s份，每次选择s-1份作为训练集，剩下的一份为测试集。一轮结束后，重新随机选择s-1份来训练数据。若干轮（小于s）后，选择损失函数评估最有的模型和参数。
		留一交叉验证：s等于样本数n，这样对于n个样本，每次选择n-1个样本来训练数据，留一个样本来验证模型预测的好坏。相当于第二种的特例。此方法主要用于样本量非常少的情况下，比如对于普通始终问题，N小于20时，一般采用留一交叉验证的方法。
~~~

##### KNN三要素

k值的选择：对于K只得选择，一般根据样本分布选择一个较小的值，然后通过交叉验证选择一个比较适合的最终值；

~~~
当k值较小时，表示使用较小领域中的样本进行预测，训练误差会减小，但是会导致模型变得复杂，容易过拟合。
当k值较大时，表示使用较大领域中的样本进行预测，训练误差会增大，同时会使模型变得简单，容易导致欠拟合。
~~~

距离的度量：一般使用欧氏距离（欧几里得距离）。

决策规则：在分类模型中，主要是用多数表决法或者加权多数表决法；在回归模型中，主要是用平均值法或者加权平均值法。

~~~
多数表决法：每个近邻样本的权重是一样的，也就是说最终预测的结果为出现类别最多的那个类。
加权平均值法：加权平均值即将各数乘以相应的权数，然后加总求和得到的总体值除以总的单位数。
~~~

#### KNN算法实现

KNN算法实现的方法主要有两种：一种为蛮力实现，一种为KD-Tree

蛮力实现：计算样本到所有训练集样本的距离，然后选择最小的k个距离即可得到k个最邻近点。缺点在于特征数比较多的时候（样本数目比较多的时候），算法的执行效率比较低。

~~~python
KNN分类伪代码：
	def knn(datas,x,y):
	#datas训练数据集，x时一个带预测的样本，k表示预测的时候获取的k个近邻样本
	#1.从datas中获取距离x最近的k个样本，距离公式为欧几里得距离
	k_neighbors=[]
	size=0
	max_dist=-1
	max_index=-1
	for data in datas:
		dist = calc_dist(data,dist)
			if dist>max_dist:
				max_dist =dist
				max_index=size
			size+=1
			elif max_dist>dist:
				k_neighbors[max_index]=(data,dist)
					max_index=np.argmax(map(lambda t:t[1],k_neighbors))
					max_dist=k_neighbors[max_index][1]
	#2.从最近邻的K个样本中，产生预测值
	result_dict={}
	for neighbors in k_neighbors:
		label=neighbors[0].y
		if label not in result_dist:
			result_dict[label]=1
		else:
			result_dict+=1
	max_count=0		
        result_label=None
        for label in result_dict:
            label_count=result_dict[label]
            if label_count>max_count:
            	result_label=label
            		max_count=label_count
          return result_label
#简单的伪代码写法
def knn(datas,x,k):
	#datas训练数据集，x时一个待预测的样本，k表示预测的视乎获取k个邻居样本
	#1.从datas中获取距离x最近的k个样本，距离公式为欧几里得距离
	k_neighbors=fetch_k_neighbors(dats,x,k)
	#2.从最近的K个样本中，产生预测值
	result_dict=calc_label_count(k_neighbors)
	result_label=fetch_max_count_label(result_dict)
	return result_label
~~~

KD-Tree：KD树算法首先是对数据集进行建模，构建KD树，然后在根据剑豪的模型来获取近邻样本数据。

~~~
除此之外，还有一些从KD-Tree修改后的求解最近邻的算法，ep：Ball Tree、BBF Tree，MVP Tree等。
~~~

#### KD-Tree

##### KD-Tree的构建方式

KD树采用从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征$n_k$作为根节点。对于这个特征，选择取值的中位数$n_{kv}$作为样本的划分点，对于小于该值得样本划分到左子树，对于大于等于该值的样本划分到右子树，对于左右子树才有同样的方式找方差最大的特征作为根节点，地柜即可产生KD树。

##### KD_Tree查找最近邻步骤

~~~
落叶节点
遍历节点所有样本计算和x距离选最短
计算（2,4.5）和所有父节点距离选最短
以最短距离画圆
如果有交线，考虑另一个子树
~~~





