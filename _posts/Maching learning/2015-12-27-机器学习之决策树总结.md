---
layout: post
comments: true
categories: 机器学习
---

#### 比特化

当变量出现的概率一样：

$P(X=A)=1/4\quad P(X=B)=1/4\quad P(X=C)=1/4\quad P(X=D)=1/4$

| A    | B    | C    | D    |
| ---- | ---- | ---- | ---- |
| 00   | 01   | 10   | 11   |

当变量出现的概率不一样：

$P(X=A)=1/2\quad P(X=B)=1/4\quad P(X=C)=1/8\quad P(X=D)=1/8$

| A    | B    | C    | D    |
| ---- | ---- | ---- | ---- |
| 0    | 10   | 110  | 111  |

$$
E=1*\frac{1}{2}+2*\frac{1}{4}+3*\frac{1}{8}+3*\frac{1}{8}
$$

$$
E=-log_2{\frac{1}{2}}*\frac{1}{2}-log_2{\frac{1}{4}}*\frac{1}{4}-log_2{\frac{1}{8}}*\frac{1}{8}-log_2{\frac{1}{8}}*\frac{1}{8}=1.75
$$

一般的比特化：

假设：

$P(X=V_1)=p_1\quad P(X=V_2)=p_2\quad P(X=V_3)=p_3\quad ...\quad P(X=V_m)=p_m$

则这些变量的期望来表示每个变量需要多少个比特位来描述信息：
$$
E(X)=-P_1log_2{(P_1)}-P_2log_2{(P_2)}-...--P_Mlog_2{(P_M)}\\
=-\sum_{i=1}^mp_i*log_2{(p_i)}
$$

#### 信息熵

##### 信息熵

信息熵指的是一个样本/事件所蕴含的信息。一般的信息熵越大，系统越混乱。信息熵越小，系统越有序。

~~~
信息熵就是用来描述系统信息量的不确定度。
高信息熵：表示随机变量X是均匀分布的，各种取值是等概率出现的。
低信息熵：表示随机变量X各种取值不是等概率出现，可能出现有的事件概率很大，有的事件概率很小。
~~~


$$
H(X)=-\sum_{i=1}^mp_i*log_2{(p_i)}
$$

##### 条件熵

~~~
给定条件X的情况下，所有不同x的值情况下的Y的信息熵的平均值叫做条件熵。
~~~

$$
H(Y|X)=H(X.Y)-H(X)
$$

条件熵的推导公式：
$$
H(Y|X)=\sum_{j=1}(X=v_j)H(Y|X=v_j)=\sum_xP(x)H(Y|x)\\
=\sum_xp(x)\left\{-\sum_yp(y|x)log(p(y|x))\right\}=-\sum_x\sum_yp(x)p(y|x)log(p(y|x))\\
=-\sum_x\sum_yp(x,y)log\left(\frac{p(x,y)}{p(x)}\right)\\
=-\sum_x\sum_yp(x,y)log(p(x,y))-\left[-\sum_x\left(\sum_yp(x,y)log(p(x))\right)\right]\\
=H(X,Y)-\left[-\sum_xp(x)log(p(x))\right]=H(X,Y)-H(X)
$$


#### 决策树

##### 什么是决策树

~~~
决策树（Descision Tree）是在已知的各种情况发生概率的基础上，通过构建决策树来进行分析的一种方式，是一种直观应用概率分析的一种图解法；
决策树是一种预测模型，代表的是对象属性与对象值之间的映射关系；
决策树是一种树形结构，其中每个内部节点表示一个属性的测试，每个分支表示一个测试输出，每个叶节点代表一种类别；
决策树是一种非常常用的有监督的分类算法。
决策树决策过程从根节点开始，测试待分类项中对应的特征属性，并按照其值选择输出分支，知道叶子节点，将叶子节点的存放的类别作为决策结果。
决策树分为两大类：分类树和回归书，前者用于分类标签值，后者用于预测连续值，常用的算法有ID3,C4.5,CART等。
~~~

##### 决策树的构建过程

~~~
决策树的构建过程尽量保证各个分裂的子集尽可能在分类的过程中尽可能的纯。
构建步骤如下：
1.将所有的特征看成一个个节点。
2.遍历当前特征的每一种分割方式，找到最好的分割点；将数据划分成为不同的子节点，eg:N1,N2,N3,...,Nm。
3.计算划分之后所有子节点的‘纯度’信息。
4.对子节点N1,N2,...,Nm分别继续执行2-3步，知道每个最终的子节点都足够‘纯’。
~~~

##### 决策树特征属性类型

构建决策树根据其类型不同选择不同的划分方式：

属性为离散值，非二叉树，此时一个属性就是一个分支。

属性为离散值，二叉树，按照属性划分的子集进行测试，按照‘属于此子集’和‘不属于此子集’分成两个分支

属性为连续值，先确定一个分裂点s，按照>分裂点s和<分裂点s生成两个分支。

##### 决策树分割属性选择

~~~
贪心算法：在每一步求解的步骤中，它要求‘贪婪’的选择方式选择最佳操作，并通过一系列的最优选择，能够产生一个问题（全局）的解。
~~~

决策树算法是一种‘贪心算法’策略，只考虑当前数据特征情况下的最好分割方式，不能进行回溯操作。

对于整体数据集而言，按照所有的特征属性进行划分操作，对所有划分操作的结果的“纯度”进行比较，选择“纯度”越高的属性作为当前需要分割的数据集进行分割操作，持续迭代，直到的到最终结果。决策树是通过“纯度”来选择分割特征属性点的。

##### 决策树量化纯度

量化纯度的方式：

Gini系数：$Gini=1-\sum_{i=1}^nP(i)^2$

熵：$H=-\sum_{i=1}^nP(i)log_2(P(i))$

错误率：$Error=1-\max_{i=1}^n\{P(i)\}$

以上三种量化纯度的结果的值越大，表示数据越不纯；反之，数据越纯。一般使用熵公式和Gini系数。

##### 信息增益度

信息增益度：$Gini=\bigtriangleup =H(D)-H(D|A)$

信息增益度越大表示特征属性熵损失的纯度越大

##### 决策树算法的停止条件

决策树构建过程是一个递归过程，一般需要给定终止条件：

1.当每个节点只有一种类型的时候停止构建（一般会导致过拟合）

2.当前节点中记录数小某个阈值，通知迭代次数达到给定值时，停止构建过程，此时使用max(p(i))作为节点的对应类型。（推荐使用此种方式）

##### 决策树算法效果评估

决策树的效果评估和一般的分类算法一样，采用混淆矩阵来进行计算准确率、召回率、精确率等指标。

也可以采用叶子节点的纯度值综合来评估算法的效果，值越小，效果越好。
$$
loss=\sum_{t=1}^{leaf}\frac{|D_t|}{D}H(t)
$$
案例：决策树直观理解计算结果。

| ID   | 拥有房产（是/否） | 婚姻状态（单身\已婚\离婚） | 年收入（单位：千元） | 无法偿还债务（是/否） |
| ---- | :---------------- | -------------------------- | -------------------- | --------------------- |
| 1    | 是                | 单身                       | 125                  | 否                    |
| 2    | 否                | 已婚                       | 100                  | 否                    |
| 3    | 否                | 单身                       | 100                  | 否                    |
| 4    | 是                | 已婚                       | 110                  | 否                    |
| 5    | 是                | 离婚                       | 60                   | 否                    |
| 6    | 否·               | 离婚                       | 95                   | 是                    |
| 7    | 否                | 单身                       | 85                   | 是                    |
| 8    | 否                | 已婚                       | 75                   | 否                    |
| 9    | 否                | 单身                       | 90                   | 是                    |
| 10   | 是                | 离婚                       | 220                  | 否                    |

注意构建连特征属性为续性决策树的类型时，需要先对该特征属性进行由小到大的排序，然后取能够划分特征数据的分割点，一般这个分割点为能够划分该特征属性的平均值。然后计算这些分割点的最大信息增益。

代码如下：（分别以房产，婚姻，年收入作为特征的划分属性，其中前两个特征属性为离散型，收入为连续型）

~~~
import numpy as np

def entropy(t):
    return np.sum(-x*np.log2(x) for x in t)
h=entropy([0.7,0.3])
print('原始数据的信息熵：{}'.format(h))

#拥有房产的样本数目为4个，计算对应的信息熵。
p11 = 0.4
h11 = entropy([1])
#未拥有房产的样本数目为6个，计算对用的信息熵。
p12 = 0.6
h12 = entropy([0.5,0.5])
#合并信息熵，得到以房产作为划分特征属性的条件熵
h1 = p11*h11+p12*h12
print('以房产作为划分特征属性的条件熵:{}'.format(h1))
print('以房产作为划分特征属性的信息增益:%.3f'%(h-h1))


#以年收入80作为分割点的划分属性考虑
#年收入小于80，样本数目为：2（2:0）
p21 = 0.2
h21 = entropy([1])
#年收入大于80，样本数目：8（5:3）
p22 = 0.8
h22 = entropy([5/8,3/8])
#合并信息熵，得到以年收入为80作为划分特征属性的条件熵
h2 = p21*h21+p22*h22
print('以年收入80作为划分特征属性的条件熵:{}'.format(h2))
print('以年收入为80作为划分特征属性的信息增益%.3f'%(h-h2))

#以年收入为97.5作为分割点的划分属性考虑
#年收入小于97.5，样本数目:5（2，3）
p31 = 0.5
h31 = entropy([2/5,3/5])
#年收入大于97.5，样本数目：5（5,0）
p32 = 0.5
h32 = entropy([1])
#合并熵信息，得到以年收入为97.5作为划分特征属性的条件熵
h3 = p31*h31+p32*h32
print('以年收入97.5作为划分特征属性的条件熵:{}'.format((h3)))
print('以年收入97.5作为划分特征属性的信息增益%.3f'%(h-h3))

#以婚姻作为换分特征属性考虑
#单身4（2:2）
p41 = 0.4
h41 = entropy([0.5,0.5])
#已婚3（3,0）
p42 = 0.3
h42 = entropy([1])
#离婚3（2:1）
p43 = 0.3
h43 = entropy([2/3,1/3])
#合并信息熵，得到以婚姻作为划分特征属性的条件熵
h4 = p41*h41+p42*h42+p43*h43
print('以婚姻作为划分特征属性的条件熵:{}'.format(h4))
print('以婚姻作为划分特征属性的信息增益%.3f'%(h-h4))

#得到的结果
原始数据的信息熵：0.8812908992306927
以房产作为划分特征属性的条件熵:0.6
以房产作为划分特征属性的信息增益:0.281
以年收入80作为划分特征属性的条件熵:0.7635472023399721
以年收入为80作为划分特征属性的信息增益0.118
以年收入97.5作为划分特征属性的条件熵:0.4854752972273343
以年收入97.5作为划分特征属性的信息增益0.396
以婚姻作为划分特征属性的条件熵:0.6754887502163469
以婚姻作为划分特征属性的信息增益0.206
~~~

##### 决策树生成算法

最主要的几种决策树算法有：ID3,C4.5,CART

ID3

ID3算法是决策树的一个景点的构造算法，内部使用信息熵以及信息增益来进行构建；每次迭代选择信息增益增益最大的特征属性作为分割属性

1.ID3算法只持离线的特征属性，不支持连续的特征属性

2.ID3算法构建的是多叉树
$$
H(D)=-\sum_{i=1}^nP(i)log_2(P(i))\\
Gain = \bigtriangleup =H(D)-H(D|A)
$$

~~~
ID3算法的优缺点：

优点：

	决策速度快，实现简单。

缺点：

	计算依赖于特征数目较多的特征，而属性值最多的属性并不一定就是最优的

	ID3算法不是递增算法

	ID3算法是单变量决策树，对于特征属性之间的关系不会考虑

	抗噪性差

	只适合小规模数据集，需要将数据放到内存中
~~~

C4.5

在ID3算法基础上，进行算法优化提出来的一种算法（C4.5），是决策树一种特别经典的构造算法。使用信息增益率来取代ID3算法中的信息增益，在书的构造过程中会进行剪枝操作进行优化，能够自动完成对连续属性的离散化处理。C4.5算法在选中分割属性的时候选择信息增益率最大的属性。
$$
H(D)=-\sum_{i=1}^nP(i)log_2(P(i))\\
Gain(A)=\bigtriangleup =h(d)-H(D|A)\\
Gain_ratio(A)=\frac{Gain(A)}{H(A)}
$$

~~~
C4.5算法优点：
	产生的规则易于理解
	准确率较高
	实现简单
缺点：
	对数据集需要进行多次顺序扫描和排序，所以效率较低
	只适合小规模数据集，需要将数据放内存中
~~~

CART

使用基尼系数（分类树）作为数据纯度的量化指标来构建的决策树算法。CART算法使用GINI增益作为分割属性选择的标准。选择GINI增益最大的作为当前数据集的分割属性，可用于分类和回归两类的问题。CART构建的是二叉树。
$$
Gini = 1-\sum_{i=1}^nP(i)^2\\
Gain=\bigtriangleup =Gain(D)-Gini(D|A)
$$
ID3,C4.5,CART三种算法总结

~~~
ID3和C4.5只适合在小规模数据及上使用
ID3和C4.5算法都是单变量决策树
当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差
决策树分类一般情况只适合小数据量的情况(数据可以放内存)
CART算法是三种算法中最常用的一种决策树构建算法(sklearn中仅支持CART)。
三种算法的区别仅仅只是对于当前树的评价标准不同而已，ID3使用信息增益、
C4.5使用信息增益率、CART使用基尼系数。
CART算法构建的一定是二叉树，ID3和C4.5构建的不一定是二叉树
~~~

| 算法 | 支持模型  | 树构建 | 特征选择        | 连续值处理 | 缺失值处理 | 剪枝   | 特征属性多次使用 |
| ---- | --------- | ------ | --------------- | ---------- | ---------- | ------ | ---------------- |
| ID3  | 分类      | 多叉树 | 信息增益        | 不支持     | 不支持     | 不支持 | 不支持           |
| C4.5 | 分类      | 多叉树 | 信息增益率      | 支持       | 支持       | 支持   | 不支持           |
| CART | 分类/回归 | 二叉树 | 基尼系数/均方差 | 支持       | 支持       | 支持   | 支持             |

#### 分类树和决策树的区别

~~~
分类树采用信息增益、信息增益率、基尼系数来评价树的效果，都是基于概率值
进行判断的；而分类树的叶子节点的预测值一般为叶子节点中概率最大的类别作
为当前叶子的预测值。
在回归树中，叶子节点的预测值一般为叶子节点中所有值的均值来作为当前叶子
节点的预测值。所以在回归树中一般采用MSE作为树的评价指标，即均方差。
一般情况下，只会使用CART算法构建回归树。
~~~

$$
MSE=\frac{1}{n}\sum_{i=1}^n(y-\hat{y}_i)^2
$$



#### 决策树优化

##### 决策树的的优化策略

剪枝优化：

​	决策树过度拟合一般情况下由于节点太多导致的，剪枝优化对决策树的正确率影响是比较大的，也就是常用的一种优化方式。

Random Forest

​	利用训练数据集随机产生多个决策树，形成一个森林。然后使用这个森林对数据进行预测，选取最多结果作为预测结果。

#### 剪枝

前置剪枝：在构建决策树的过程中，提前停止。结果时决策树一般比较小，效果不是太好。

后置剪枝：在决策树构建好后，然后开始裁剪，一般使用两种方式：

​	1）用单一叶子节点代替整个子树，叶节点的分类采用子树中最主要的分类

​	2）将一个子树完全代替另外一棵子树；后置剪枝的主要问题是急速那效率的问题，存在一定的浪费情况。

后置剪枝（交叉验证）：

​	由完全树$T_0$开始，剪枝部分节点得到$T_1$，在此剪枝得到$T_2$......知道仅剩根的树$T_k$

​	在验证数据集上对这k+1树进行评价，选择最优树$T_a$(损失函数最小的树)

##### 后置剪枝过程

​	对于给定的决策树$T_0$

​		计算所有内部非叶子节点的剪枝系数

​		查找最小剪枝系数的节点，经期子节点进行删除操作，进行剪枝得到决策树$T_k$,如果存在多个最小剪枝系数节点，选择包含数据项最多的节点进行剪枝操作

​		重复上述操作，知道产生的剪枝决策树$T_k$只有一个节点

​		得到决策树$T_0T_1..T_k$

​		使用验证样本集选择最优子树$T_a$

使用验证集选择最有子树的标准，可以使用原始损失函数来考虑：
$$
loss = \sum_{t=1}^{leaf}\frac{|D_t|}{D}H(t)
$$
叶子节点越多，决策树越复杂，损失越大，修正添加剪枝系数:
$$
loss_{\alpha}=loss+\alpha*leaf
$$
考虑根节点为r的子树，剪枝后的损失函数分别为loss(R)和loss(r),当着两者相等的时候，可以求得剪枝系数
$$
loss_{\alpha}=loss(r)+\alpha\\
loss_{R}=loss(R)+\alpha*R_{leaf}\\
\alpha=\frac{loss(r)-loss(R)}{R_{leaf}-1}
$$

#### 决策树可视化

##### 决策树可视化准备工作

~~~
决策树可视化可以方便我们直观的观察所构建的树模型；决策树可视化依赖
graphviz服务，所以我们在进行可视化之前，安装对应的服务；操作如下：
安装graphviz服务
安装python的graphviz插件： pip install graphviz
安装python的pydotplus插件： pip install pydotplus
graphviz服务安装：
下载安装包(msi安装包): http://www.graphviz.org/；
执行下载好的安装包(双击msi安装包)；
将graphviz的根目录下的bin文件夹路径添加到PATH环境变量中；
~~~

##### 决策树可视化案例

~~~
方式一：将模型输出dot文件，然后使用graphviz的命令将dot文件转换为pdf
格式的文件
方式二：直接使用pydotplus插件直接生成pdf文件进行保存
方式三：使用Image对象直接显示pydotplus生成的图片
~~~

