---
layout: post
comments: true
categories: 第一章机器学习之线性回归算法总结
---
[TOC]



### <font color=salmon size=5 face="微软雅黑">第一章机器学习之线性回归算法总结</font>

#### 线性回归算法（有监督的算法）

```
有监督学习：
用一直某种或某些特性的样本作为训练集，建立一个数学模型，再用已经建立好的模型来预测未知样本。
无监督学习：
与监督学习比，无监督学习没有认为标注的结果，在飞监督学习过程中，书据并不被特别标识。
半监督学习：
利用少量的标注样本和大量的未标注样本进行训练和分类的问题，是有监督学习和无监督学习两者的结合。
```

$$
y^{i} = w^{i}*x^{i}+\varepsilon^{i}
$$

#### 最大似然估计以及二乘法

##### 似然函数：

$$
y^{(i)}=\theta^{T}*X^{(i)}+\varepsilon^{(i)}\\
p(\varepsilon^{(i)})=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{\varepsilon^{(i)^2}}{2\sigma^2}}\\
p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sigma\sqrt{2\pi}}exp\left\{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right\}\\
L(\theta)=\prod_{i=1}^m\frac{1}{\sigma\sqrt{2\pi}}exp^\left\{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right\}
$$

##### 最小二乘法最优解：

$$
l(\theta)=logL(\theta)=log\prod_{i=1}^m\frac{1}{\sigma\sqrt{2\pi}}exp^\left\{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right\}\\
=\sum_{i=1}^mlog\frac{1}{\sigma\sqrt{2\pi}}exp^\left\{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right\}\\
=mlog\frac{1}{\sigma\sqrt{2\pi}}-\frac{1}{\sigma^2}*\frac{1}{2}\sum_{i=1}^m\left(y^{(i)}-\theta^Tx^{(i)}\right)^2\\
loss(y_j,\hat{y}_j)=J(\theta)=\frac{1}{2}\sum_{i=1}^m\left(y^{(i)}-\theta^Tx^{(i)}\right)^2
$$

$\theta$的求解过程：
$$
J(\theta)=\frac{1}{2}\sum_{i=1}^m\left(y^{(i)}-\theta^Tx^{(i)}\right)^2=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)\\
J(\theta)=\bigtriangledown_{\theta}\left\{\frac{1}{2}(X\theta-Y)^T(X\theta-Y)\right\}=\bigtriangledown_{\theta}\left\{\frac{1}{2}(\theta^TX^T-Y^T)(X\theta-Y)\right\}\\
=\bigtriangledown_{\theta}\left\{\frac{1}{2}\left(\theta^TX^TX\theta-\theta^TX^TY-T^TX\theta+Y^TY\right)\right\}\\
=\frac{1}{2}\left(2X^TX\theta-X^TY-(Y^TX)^T\right)\\
=X^TX\theta-X^TY\Rightarrow \min_{\theta}J(\theta)
$$

$$
\theta = (X^TX)^{-1}X^TY
$$

注意：$X^TX$可逆

```
为了防止不可逆或者过拟合的问题存在，可以增加额外数据影响，导致最终的矩阵是可逆的。
```

$$
\theta=(X^TX+\lambda I)^{-1}X^Ty
$$



##### 欠拟合，过拟合

所谓的欠拟合就是模型在训练时候学习效果达不到预期要求

过拟合就是在模型训练时对包括某一个不太重要的特征学习效果太好，导致学习效果太好

#### 多项式回归算法

如果数据不符合线性规律而且更复杂，一种简单的解决办法就是讲每一维特征的幂次方添加为新的特征，在对所有的特征进行线性回归分析。这种方式就叫做多项式回归。

多项式扩展：属于增加维度的一种方式，通过这种方式可以将数据映射到高纬度空间变成线性可分的数据
功能：将低纬度空间上的数据通过都像是的组合，映射到高纬度空间中
效果：可以将低纬度的非线性的数据转换到高纬度空间中变成线性数据
方式：（sklearn中的多项式扩展的API的效果）

在以sklearn中的PolynomialFeatures类为例，当原始的特征为（a,b），次幂为3时，不仅仅会将$a^3,b^3$作为新的特征，还会添加$a^2b,ab^2,ab$，其中degree=d，将维度为n的原始特征转换为$\frac{(n+d)!}{d!n!}$，因此在使用多项式扩展特征的时候，必须注意特征维度爆炸的问题！

eg:

```
原始数据（2,3）；
   如果做最高次项为2的多项式转换：最终结果（1,2,3,4,6,9）
   如果做最高次项为3的多项式转换：最终结果（1,2,3,4,6,8,9,12,18,27）
   如果做最高次项为4的多项式转换：最终结果（1,2,3,4,6,8,9,12,16,18,24,27,36,54,81）
```

模型参数：

在模型训练数据集上通过某种给定的方式找出来的模型参数，也就是说这个模型参数的求解就是我们经常所说的模型学习。

超参：

在模型训练中需要使用到的参数值，但是给参数值需要开发人员给定的。eg：Ridge API中的alpha...

给定超参的方式：

1.可以根据算法的特性、业务背景以及经验，类给定一个比较合适的值

2.通过sklearn提供的交叉验证的方式来选择最优的参数

3.通过网格交叉验证的当时选择最优参数

#### 正则化(L1正则，L2正则)：

目的：为了防止过拟合。

L1正则又叫做Lasso回归，L2正则有叫做岭回归。
$$
J(\theta)=\frac{1}{2}\sum\left(h_{\theta}(x^{(i)}-y^{(i)})\right)+\lambda\sum_{i=1}^n\theta_j^2 (\lambda >0) -L2正则\\

J(\theta)=\frac{1}{2}\sum\left(h_{\theta}(x^{(i)}-y^{(i)})\right)+\lambda\sum_{i=1}^n\theta_j^2 (\lambda >0) -L1正则
$$
L1和L2正则比较:

L2-Norm中，对于各个维度的参数都是在一个圆内缩放的，不可能导致有维度参数为零的情况，就不会产生稀疏解；

```
稀疏就是训练出来得到的模型参数中有很多参数值都是0,；稀疏解的主要作用就是用于特征选择，因为参数为零所对应的特征相当于没有决策能力，步会影响y的取值，既然这样，我们可以将这些为0对应的特征删除。
```

Lasso算法可以让数据去掉噪音或者冗余特征。Ridge模型具有较高的准确性、鲁棒性以及稳定性（前提数据中不存在冗余特征的时候）；Lasso模型具有较高的求解速度。如果纪要考虑稳定性也考虑求解的速度，就使用Elastic Net。

##### Elastic Net(弹性网络)：

$$
J(\theta) = \frac{1}{2}\sum_{i=1}^m\left(h_{\theta}x^{(i)}-y^{(i)}\right)+\lambda\left\{p\sum_{j=1}^n|\theta_j|+(1-p)\sum_{j=1}^n\theta_j^2\right\}
$$

##### 模型效果判断：

$$
MSE = \frac{1}{m}\sum_{i=1}^m\left(y_i-\hat{y_j}\right)^2
$$

$$
RMSE=\sqrt{MSE}=\sqrt{\frac{1}{m}\sum_{i=1}^m\left(y_i-\hat{y_j}\right)^2}
$$

$$
R^2=1-\frac{RSS}{TSS}=1-\frac{\sum_{i=1}^m\left(y_i-\hat{y_j}\right)^2}{\sum_{i=1}^m\left(y_i-\bar{y_j}\right)^2}
$$

##### 模型效果判断:

MSE:误差平方和，月趋近于0表示模型月拟合训练数据。

RMSE:MSE的平方根，作用同MSE。

$R^2$:取值范围为$(负无穷，1]$，值越大表示模型月拟合训练数据；最优解是1；当模型预测为随机值得时候，有可能为负；若预测值恒为期望，$R^2$为0；

TSS:总平方和TSS，表示样本之间的差异情况，是伪方差的m倍。

RSS:残差平方和RSS，表示预测值和样本值之间的差异情况，是MSE的m倍。



#### 梯度下降：（批量梯度下降，随机梯度下降，小批量梯度下降）

##### 梯度下降算法:

目标函数$\theta$求解:$J(\theta)=\frac{1}{2}\sum_{i=1}^m\left(h_{\theta}(x^{(i)}-y^{(i)})\right)^2$

初始化$\theta$(随机初始化，可以初始为0)

沿着扶梯度方向迭代，更新后的$\theta$使$J(\theta)$更小
$$
\theta = \theta - \alpha*\frac{\partial J(\theta)}{\partial \theta}
$$
$\alpha$是学习率，步长

##### BGD和SGD比较：

随机梯度下降（SGD）迭代次数少，运行速度更快。

SGD在某一些情况下（全局存在多个相对最优解，或者说数据中存在异常数据的情况），SGD有可能会跳出某些小的局部最优解，所以不会比BGD坏（样本数据集中正常样本居多）；SGD在收敛的位置会存在$J(\theta)$函数波动的情况。

BGD一定能够得到局部最优解（在线性回归模型中一定是得到一个全局的最优解），SGD由于随机性的存在可能会导致最终的结果比SGD差（样本数据集中噪音样本居多）



#### Logistic回归：

$Logistic/sigmoid函数 :p=h_{\theta}(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$

令$g(z)=\frac{1}{1+e^{-z}}$
$$
g'(z)=\left\{\frac{1}{1+e^{-z}}\right\}'=\frac{e^{-z}}{(1+e^{-z})^2}\\
=\frac{1}{1+e^{-z}}*\frac{e^{-z}}{1+e^{-z}}=\frac{1}{1+e^{-z}}*\left\{1-\frac{1}{1+e^{-z}}\right\}\\
=g(z)*(1-g(z))
$$

##### Logistic回归及似然函数：

假设:
$$
P(y=1|x;\theta)=h_{\theta}(x)\\
P(y=0|x;\theta)=1-h_{\theta}(x)\\
p(y|x;\theta)=(h_{\theta}(x))^y(1-h_{\theta}(x))^{(1-y)}
$$

|          | y=1      | y=0        |
| -------- | -------- | ---------- |
| $p(y|x)$ | $\theta$ | $1-\theta$ |

似然函数：
$$
L(\theta)=p(\vec{y}|X;\theta)=\prod_{i=1}^mp\left(y^{(i)}|x^{(i)};\theta\right)\\
=\prod_{i=1}^m\left(h_{\theta}(x^{(i)})\right)^{y^{(i)}}\left(1-h_{\theta}(x^{(i)})\right)^{(1-y^{(i)})}
$$
对数似然函数：
$$
l(\theta)=logL(\theta)=\sum_{i=1}^m\left(y^{(i)}logh_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right)
$$

##### 最大似然/极大似然函数的随机梯度

$$
\frac{\part l(\theta)}{\part\theta}=\sum_{i=1}^m\left\{\frac{y^{(i)}}{h_{\theta}x^{(i)}}-\frac{1-y^{(i)}}{1-h_{\theta}(x^{(i)})}\right\}*\frac{\part h_{\theta}x^{(i)}}{\part\theta_j}\\
=\sum_{i=1}^m\left\{\frac{y^{(i)}}{a(\theta^Tx^{(i)})}-\frac{1-y^{(i)}}{1-h_{\theta}(x^{(i)})}\right\}\frac{\part h_{\theta}x^{(i)}}{\part\theta_j}\\
=\sum_{i=1}^m\left\{\frac{y^{(i)}}{a(\theta^Tx^{(i)})}-\frac{1-y^{(i)}}{1-h_{\theta}(x^{(i)})}\right\}*g\left(\theta^Tx^{(i)}\right)\left(1-g(\theta^Tx^{(i)})\right)*\frac{\part\theta x^{(i)}}{\part\theta_j}\\
=\sum_{i=1}^m\left(y^{(i)}\left(1-g\left(\theta^Tx^{(i)}\right)\right)-\left(1-y^{(i)}\right)g\left(\theta x^{(i)}\right)\right)*x_j^{(i)}=\sum_{i=1}^m\left(y^{(i)}-g\left(\theta^tX^{(i)}\right)\right)*x_j^{(i)}
$$

##### 极大似然估计与Logistic回归损失函数

$$
L(\theta)=\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^mp_i^{y^{(i)}}(1-p_i)^{1-y^{(i)}}\\
p_i = h_{\theta}(x^{(i)})=\frac{1}{1+e^{-\theta^Tx^{(i)}}}\\
loss = -l(\theta)=-\sum_{i=1}^m\left[y^{(i)}ln(p_i)+(1-y^{(i)})ln(1-p_i)\right]\\
=\sum_{i=1}^m\left[-y^{(i)}ln(h_{\theta}(x^{(i)}))-(1-y^{(i)})ln(1-h_{\theta}(x^{(i)}))\right]
$$

$\theta$的参数求解：
$$
\theta_j=\theta_j+\alpha\sum_{i=1}^m(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}\\
\theta_j=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}
$$

#### Sofatmax回归：

softmax回归是Logistic回归的一般化，适用于K分类的问题，第K类的参数为向量$\theta_k$,组成的二维矩阵为$\theta_{k*n}$;

sofemax函数的本质就是讲一个K维的文艺实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0,1）之间。

softmax回归概率函数：
$$
p(y=k|x;\theta)=\frac{e^{\theta_k^T*X}}{\sum_{l=1}^{K}*e^{\theta_l^Tx}},k=1,2,...,K
$$
概率：用于在已知一些参数情况下，预测接下来的观测所得到结果，而似然性正好相反则是用于已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

##### softmax算法原理：

$$
p(y=k|x;\theta)=\frac{e^{\theta_k^Tx}}{\sum_{l=1}^Ke^{\theta_l^T}x},k=1,2,...,K\\
h_{\theta}=\begin{bmatrix}
 p(y^{(i)}=1|x^{(i)};\theta)\\ 
 p(y^{(i)}=1|x^{(i)};\theta \\ 
 ...\\ 
 p(y^{(i)}=1|x^{(i)};\theta
\end{bmatrix}=\frac{1}{\sum_{j=1}^ke^{\theta^T_jx^{(i)}}}\begin{bmatrix}
e^{\theta_1^TX}\\
e^{\theta_2^TX}\\
...\\
e^{\theta_K^TX}
\end{bmatrix}\Rightarrow\theta=\begin{bmatrix}
\theta_{11}\quad  \theta_{12}\quad ...\quad\theta_{1n}\\
\theta_{21}\quad  \theta_{22}\quad ...\quad\theta_{2n}\\
...\quad ...\quad ...\quad ...\\
\theta_{k1}\quad\theta_{k2}\quad...\quad\theta_{kn}
\end{bmatrix}
$$



##### softmax算法损失函数

$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^m\sum_{j=1}^kI(y^{(i)}=j)ln\left(\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}}\right)\\
I(y^{(i)}=j)=\left\{\begin{matrix}
1,y^{(i)}=j\\ 
0,y^{(i)]}\neq j
\end{matrix}\right.
$$

##### softmax算法梯度下降求解

$$
\frac{\part}{\part\theta_j}J(\theta)=\frac{\part}{\part\theta}-I(y^{(i)}=j)ln\left\{\frac{e^{\theta_j^TX^{(i)}}}{\sum_{l=1}^ke^{\theta^Tx^{(i)}}}\right\}\\
=\frac{\part}{\part\theta_j}-I(y^{(i)}=j)(\theta^Tx^{(i)}-ln(\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}))\\
=-I(y^{(i)}=j)\left\{1-\frac{e^{\theta^Tx^{(i)}}}{\sum_{l=1}^k\theta_l^Tx^{(i)}}\right\}x^{(i)}
$$

$$
\frac{\part}{\part\theta_j}J(\theta)=-I(y^{(i)}=j)\left\{1-\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{i=1}^ke^{\theta_l^Tx^{(i)}}}\right\}x^{(i)}\\
\theta_j=\theta_j+\alpha\sum_{i=1}^mI(y^{(i)}=j)(1-p(y^{(i)}=j|x^{(i)};\theta))x^{(i)}\\
\theta_j=\theta_j+\alpha I(y^{(i)}=j)(1-P(Y^{(i)}=j|x^{(i)};\theta))x^{(i)}
$$

#### Logistic和softmax比较

两者都是属于分了问题。

求$\theta$的方式都是梯度下降的算法，梯度下降的算法是参数优化的重要手段，主要是SGD，适用于在线学习以及跳出局部极小值。

Logistic/Softmax回归是实践中解决问题的最重要的方法。

广义线性模型对样本要求不必要服从正态分布，只需要服从指数分布簇（二项式分布、泊松分布、伯努利分布、指数分布等）；广义线性模型的自变量可以是连续的也可以是离散的。

### <font color=salmon size=5 face="微软雅黑">第二章机器学习之KNN</font>

#### KNN算法

##### KNN算法原理

K近邻（K-nearst neighbors,KNN）是一种基本的机器学习算法，所谓的k近邻，就是K个最近邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。

KNN做分类预测时，一般采用多数表决法；而在做回归预测时，一般采用平均值法。

从训练集合中获取k个样本数据来预测当前带预测样本的目标属性值。

##### 交叉验证

```
交叉验证：就是重复的数据，吧得到的样本数据进行切分，组合为不同的训练集和测试机，用训练集来训练模型，用测试机来评估模型预测的好坏。在此基础上可以得到不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试机中的样本，即所谓的“交叉”。
应用场景：一般都是数据不够充分的时候。一般的当数据小于一万条以下时，我们会用交叉验证来训练优化选择模型。
		对于大于一万条的数据，我们将数据分成三个不同的数据集，分别为训练集，验证集，最后一个为测试集。			用训练集来训练模型，用验证集来评估预测模型的好坏和选择模型以及其对应的参数。把最终得到的模型在			用于测试集，最终决定使用哪个模型以及对应参数。
根据数据的切分方法，交叉验证分为三种：
		简单交叉验证：将数据分为两个部分，一般的70%为训练集，30%为测试集。然后用训练集来训练模型，在测试集上验证模型以及参数。接着，吧样本打乱，重新选择训练集和测试机，继续训练数据和检验模型，最后我们选择损失函数评估最有的模型和参数。
		s折交叉验证：先把数据分成s份，每次选择s-1份作为训练集，剩下的一份为测试集。一轮结束后，重新随机选择s-1份来训练数据。若干轮（小于s）后，选择损失函数评估最有的模型和参数。
		留一交叉验证：s等于样本数n，这样对于n个样本，每次选择n-1个样本来训练数据，留一个样本来验证模型预测的好坏。相当于第二种的特例。此方法主要用于样本量非常少的情况下，比如对于普通始终问题，N小于20时，一般采用留一交叉验证的方法。
```

##### KNN三要素

k值的选择：对于K只得选择，一般根据样本分布选择一个较小的值，然后通过交叉验证选择一个比较适合的最终值；

```
当k值较小时，表示使用较小领域中的样本进行预测，训练误差会减小，但是会导致模型变得复杂，容易过拟合。
当k值较大时，表示使用较大领域中的样本进行预测，训练误差会增大，同时会使模型变得简单，容易导致欠拟合。
```

距离的度量：一般使用欧氏距离（欧几里得距离）。

决策规则：在分类模型中，主要是用多数表决法或者加权多数表决法；在回归模型中，主要是用平均值法或者加权平均值法。

```
多数表决法：每个近邻样本的权重是一样的，也就是说最终预测的结果为出现类别最多的那个类。
加权平均值法：加权平均值即将各数乘以相应的权数，然后加总求和得到的总体值除以总的单位数。
```

#### KNN算法实现

KNN算法实现的方法主要有两种：一种为蛮力实现，一种为KD-Tree

蛮力实现：计算样本到所有训练集样本的距离，然后选择最小的k个距离即可得到k个最邻近点。缺点在于特征数比较多的时候（样本数目比较多的时候），算法的执行效率比较低。

```python
KNN分类伪代码：
	def knn(datas,x,y):
	#datas训练数据集，x时一个带预测的样本，k表示预测的时候获取的k个近邻样本
	#1.从datas中获取距离x最近的k个样本，距离公式为欧几里得距离
	k_neighbors=[]
	size=0
	max_dist=-1
	max_index=-1
	for data in datas:
		dist = calc_dist(data,dist)
			if dist>max_dist:
				max_dist =dist
				max_index=size
			size+=1
			elif max_dist>dist:
				k_neighbors[max_index]=(data,dist)
					max_index=np.argmax(map(lambda t:t[1],k_neighbors))
					max_dist=k_neighbors[max_index][1]
	#2.从最近邻的K个样本中，产生预测值
	result_dict={}
	for neighbors in k_neighbors:
		label=neighbors[0].y
		if label not in result_dist:
			result_dict[label]=1
		else:
			result_dict+=1
	max_count=0		
        result_label=None
        for label in result_dict:
            label_count=result_dict[label]
            if label_count>max_count:
            	result_label=label
            		max_count=label_count
          return result_label
#简单的伪代码写法
def knn(datas,x,k):
	#datas训练数据集，x时一个待预测的样本，k表示预测的视乎获取k个邻居样本
	#1.从datas中获取距离x最近的k个样本，距离公式为欧几里得距离
	k_neighbors=fetch_k_neighbors(dats,x,k)
	#2.从最近的K个样本中，产生预测值
	result_dict=calc_label_count(k_neighbors)
	result_label=fetch_max_count_label(result_dict)
	return result_label
```

KD-Tree：KD树算法首先是对数据集进行建模，构建KD树，然后在根据剑豪的模型来获取近邻样本数据。

```
除此之外，还有一些从KD-Tree修改后的求解最近邻的算法，ep：Ball Tree、BBF Tree，MVP Tree等。
```

#### KD-Tree

##### KD-Tree的构建方式

KD树采用从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征$n_k$作为根节点。对于这个特征，选择取值的中位数$n_{kv}$作为样本的划分点，对于小于该值得样本划分到左子树，对于大于等于该值的样本划分到右子树，对于左右子树才有同样的方式找方差最大的特征作为根节点，地柜即可产生KD树。

##### KD_Tree查找最近邻步骤

```
落叶节点
遍历节点所有样本计算和x距离选最短
计算（2,4.5）和所有父节点距离选最短
以最短距离画圆
如果有交线，考虑另一个子树
```

### <font color=salmon size=5 face="微软雅黑">第三章机器学习之决策树总结</font>

#### 比特化

当变量出现的概率一样：

$P(X=A)=1/4\quad P(X=B)=1/4\quad P(X=C)=1/4\quad P(X=D)=1/4$

| A    | B    | C    | D    |
| ---- | ---- | ---- | ---- |
| 00   | 01   | 10   | 11   |

当变量出现的概率不一样：

$P(X=A)=1/2\quad P(X=B)=1/4\quad P(X=C)=1/8\quad P(X=D)=1/8$

| A    | B    | C    | D    |
| ---- | ---- | ---- | ---- |
| 0    | 10   | 110  | 111  |

$$
E=1*\frac{1}{2}+2*\frac{1}{4}+3*\frac{1}{8}+3*\frac{1}{8}
$$

$$
E=-log_2{\frac{1}{2}}*\frac{1}{2}-log_2{\frac{1}{4}}*\frac{1}{4}-log_2{\frac{1}{8}}*\frac{1}{8}-log_2{\frac{1}{8}}*\frac{1}{8}=1.75
$$

一般的比特化：

假设：

$P(X=V_1)=p_1\quad P(X=V_2)=p_2\quad P(X=V_3)=p_3\quad ...\quad P(X=V_m)=p_m$

则这些变量的期望来表示每个变量需要多少个比特位来描述信息：
$$
E(X)=-P_1log_2{(P_1)}-P_2log_2{(P_2)}-...--P_Mlog_2{(P_M)}\\
=-\sum_{i=1}^mp_i*log_2{(p_i)}
$$

#### 信息熵

##### 信息熵

信息熵指的是一个样本/事件所蕴含的信息。一般的信息熵越大，系统越混乱。信息熵越小，系统越有序。

```
信息熵就是用来描述系统信息量的不确定度。
高信息熵：表示随机变量X是均匀分布的，各种取值是等概率出现的。
低信息熵：表示随机变量X各种取值不是等概率出现，可能出现有的事件概率很大，有的事件概率很小。
```

$$
H(X)=-\sum_{i=1}^mp_i*log_2{(p_i)}
$$

##### 条件熵

```
给定条件X的情况下，所有不同x的值情况下的Y的信息熵的平均值叫做条件熵。
```

$$
H(Y|X)=H(X.Y)-H(X)
$$

条件熵的推导公式：
$$
H(Y|X)=\sum_{j=1}(X=v_j)H(Y|X=v_j)=\sum_xP(x)H(Y|x)\\
=\sum_xp(x)\left\{-\sum_yp(y|x)log(p(y|x))\right\}=-sum_x\sum_yp(x)p(y|x)log(p(y|x))\\
=-\sum_x\sum_yp(x,y)log\left(\frac{p(x,y)}{p(x)}\right)\\
=-\sum_x\sum_yp(x,y)log(p(x,y))-\left[-\sum_x\left(\sum_yp(x,y)log(p(x))\right)\right]\\
=H(X,Y)-\left[-\sum_xp(x)log(p(x))\right]=H(X,Y)-H(X)
$$

#### 决策树

##### 什么是决策树

```
决策树（Descision Tree）是在已知的各种情况发生概率的基础上，通过构建决策树来进行分析的一种方式，是一种直观应用概率分析的一种图解法；
决策树是一种预测模型，代表的是对象属性与对象值之间的映射关系；
决策树是一种树形结构，其中每个内部节点表示一个属性的测试，每个分支表示一个测试输出，每个叶节点代表一种类别；
决策树是一种非常常用的有监督的分类算法。
决策树决策过程从根节点开始，测试待分类项中对应的特征属性，并按照其值选择输出分支，知道叶子节点，将叶子节点的存放的类别作为决策结果。
决策树分为两大类：分类树和回归书，前者用于分类标签值，后者用于预测连续值，常用的算法有ID3,C4.5,CART等。
```

##### 决策树的构建过程

```
决策树的构建过程尽量保证各个分裂的子集尽可能在分类的过程中尽可能的纯。
构建步骤如下：
1.将所有的特征看成一个个节点。
2.遍历当前特征的每一种分割方式，找到最好的分割点；将数据划分成为不同的子节点，eg:N1,N2,N3,...,Nm。
3.计算划分之后所有子节点的‘纯度’信息。
4.对子节点N1,N2,...,Nm分别继续执行2-3步，知道每个最终的子节点都足够‘纯’。
```

##### 决策树特征属性类型

构建决策树根据其类型不同选择不同的划分方式：

属性为离散值，非二叉树，此时一个属性就是一个分支。

属性为离散值，二叉树，按照属性划分的子集进行测试，按照‘属于此子集’和‘不属于此子集’分成两个分支

属性为连续值，先确定一个分裂点s，按照>分裂点s和<分裂点s生成两个分支。

##### 决策树分割属性选择

```
贪心算法：在每一步求解的步骤中，它要求‘贪婪’的选择方式选择最佳操作，并通过一系列的最优选择，能够产生一个问题（全局）的解。
```

决策树算法是一种‘贪心算法’策略，只考虑当前数据特征情况下的最好分割方式，不能进行回溯操作。

对于整体数据集而言，按照所有的特征属性进行划分操作，对所有划分操作的结果的“纯度”进行比较，选择“纯度”越高的属性作为当前需要分割的数据集进行分割操作，持续迭代，直到的到最终结果。决策树是通过“纯度”来选择分割特征属性点的。

##### 决策树量化纯度

量化纯度的方式：

Gini系数：$Gini=1-\sum_{i=1}^nP(i)^2$

熵：$H=-\sum_{i=1}^nP(i)log_2(P(i))$

错误率：$Error=1-\max_{i=1}^n\{P(i)\}$

以上三种量化纯度的结果的值越大，表示数据越不纯；反之，数据越纯。一般使用熵公式和Gini系数。

##### 信息增益度

信息增益度：$Gini=\bigtriangleup =H(D)-H(D|A)$

信息增益度越大表示特征属性熵损失的纯度越大

##### 决策树算法的停止条件

决策树构建过程是一个递归过程，一般需要给定终止条件：

1.当每个节点只有一种类型的时候停止构建（一般会导致过拟合）

2.当前节点中记录数小某个阈值，通知迭代次数达到给定值时，停止构建过程，此时使用max(p(i))作为节点的对应类型。（推荐使用此种方式）

##### 决策树算法效果评估

决策树的效果评估和一般的分类算法一样，采用混淆矩阵来进行计算准确率、召回率、精确率等指标。

也可以采用叶子节点的纯度值综合来评估算法的效果，值越小，效果越好。
$$
loss=\sum_{t=1}^{leaf}\frac{|D_t|}{D}H(t)
$$
案例：决策树直观理解计算结果。

| ID   | 拥有房产（是/否） | 婚姻状态（单身\已婚\离婚） | 年收入（单位：千元） | 无法偿还债务（是/否） |
| ---- | :---------------- | -------------------------- | -------------------- | --------------------- |
| 1    | 是                | 单身                       | 125                  | 否                    |
| 2    | 否                | 已婚                       | 100                  | 否                    |
| 3    | 否                | 单身                       | 100                  | 否                    |
| 4    | 是                | 已婚                       | 110                  | 否                    |
| 5    | 是                | 离婚                       | 60                   | 否                    |
| 6    | 否·               | 离婚                       | 95                   | 是                    |
| 7    | 否                | 单身                       | 85                   | 是                    |
| 8    | 否                | 已婚                       | 75                   | 否                    |
| 9    | 否                | 单身                       | 90                   | 是                    |
| 10   | 是                | 离婚                       | 220                  | 否                    |

注意构建连特征属性为续性决策树的类型时，需要先对该特征属性进行由小到大的排序，然后取能够划分特征数据的分割点，一般这个分割点为能够划分该特征属性的平均值。然后计算这些分割点的最大信息增益。

代码如下：（分别以房产，婚姻，年收入作为特征的划分属性，其中前两个特征属性为离散型，收入为连续型）

```
import numpy as np

def entropy(t):
    return np.sum(-x*np.log2(x) for x in t)
h=entropy([0.7,0.3])
print('原始数据的信息熵：{}'.format(h))

#拥有房产的样本数目为4个，计算对应的信息熵。
p11 = 0.4
h11 = entropy([1])
#未拥有房产的样本数目为6个，计算对用的信息熵。
p12 = 0.6
h12 = entropy([0.5,0.5])
#合并信息熵，得到以房产作为划分特征属性的条件熵
h1 = p11*h11+p12*h12
print('以房产作为划分特征属性的条件熵:{}'.format(h1))
print('以房产作为划分特征属性的信息增益:%.3f'%(h-h1))


#以年收入80作为分割点的划分属性考虑
#年收入小于80，样本数目为：2（2:0）
p21 = 0.2
h21 = entropy([1])
#年收入大于80，样本数目：8（5:3）
p22 = 0.8
h22 = entropy([5/8,3/8])
#合并信息熵，得到以年收入为80作为划分特征属性的条件熵
h2 = p21*h21+p22*h22
print('以年收入80作为划分特征属性的条件熵:{}'.format(h2))
print('以年收入为80作为划分特征属性的信息增益%.3f'%(h-h2))

#以年收入为97.5作为分割点的划分属性考虑
#年收入小于97.5，样本数目:5（2，3）
p31 = 0.5
h31 = entropy([2/5,3/5])
#年收入大于97.5，样本数目：5（5,0）
p32 = 0.5
h32 = entropy([1])
#合并熵信息，得到以年收入为97.5作为划分特征属性的条件熵
h3 = p31*h31+p32*h32
print('以年收入97.5作为划分特征属性的条件熵:{}'.format((h3)))
print('以年收入97.5作为划分特征属性的信息增益%.3f'%(h-h3))

#以婚姻作为换分特征属性考虑
#单身4（2:2）
p41 = 0.4
h41 = entropy([0.5,0.5])
#已婚3（3,0）
p42 = 0.3
h42 = entropy([1])
#离婚3（2:1）
p43 = 0.3
h43 = entropy([2/3,1/3])
#合并信息熵，得到以婚姻作为划分特征属性的条件熵
h4 = p41*h41+p42*h42+p43*h43
print('以婚姻作为划分特征属性的条件熵:{}'.format(h4))
print('以婚姻作为划分特征属性的信息增益%.3f'%(h-h4))

#得到的结果
原始数据的信息熵：0.8812908992306927
以房产作为划分特征属性的条件熵:0.6
以房产作为划分特征属性的信息增益:0.281
以年收入80作为划分特征属性的条件熵:0.7635472023399721
以年收入为80作为划分特征属性的信息增益0.118
以年收入97.5作为划分特征属性的条件熵:0.4854752972273343
以年收入97.5作为划分特征属性的信息增益0.396
以婚姻作为划分特征属性的条件熵:0.6754887502163469
以婚姻作为划分特征属性的信息增益0.206
```

##### 决策树生成算法

最主要的几种决策树算法有：ID3,C4.5,CART

ID3

ID3算法是决策树的一个景点的构造算法，内部使用信息熵以及信息增益来进行构建；每次迭代选择信息增益增益最大的特征属性作为分割属性

1.ID3算法只持离线的特征属性，不支持连续的特征属性

2.ID3算法构建的是多叉树
$$
H(D)=-\sum_{i=1}^nP(i)log_2(P(i))\\
Gain = \bigtriangleup =H(D)-H(D|A)
$$

```
ID3算法的优缺点：

优点：

	决策速度快，实现简单。

缺点：

	计算依赖于特征数目较多的特征，而属性值最多的属性并不一定就是最优的

	ID3算法不是递增算法

	ID3算法是单变量决策树，对于特征属性之间的关系不会考虑

	抗噪性差

	只适合小规模数据集，需要将数据放到内存中
```

C4.5

在ID3算法基础上，进行算法优化提出来的一种算法（C4.5），是决策树一种特别经典的构造算法。使用信息增益率来取代ID3算法中的信息增益，在书的构造过程中会进行剪枝操作进行优化，能够自动完成对连续属性的离散化处理。C4.5算法在选中分割属性的时候选择信息增益率最大的属性。
$$
H(D)=-\sum_{i=1}^nP(i)log_2(P(i))\\
Gain(A)=\bigtriangleup =h(d)-H(D|A)\\
Gain_ratio(A)=\frac{Gain(A)}{H(A)}
$$

```
C4.5算法优点：
	产生的规则易于理解
	准确率较高
	实现简单
缺点：
	对数据集需要进行多次顺序扫描和排序，所以效率较低
	只适合小规模数据集，需要将数据放内存中
```

CART

使用基尼系数（分类树）作为数据纯度的量化指标来构建的决策树算法。CART算法使用GINI增益作为分割属性选择的标准。选择GINI增益最大的作为当前数据集的分割属性，可用于分类和回归两类的问题。CART构建的是二叉树。
$$
Gini = 1-\sum_{i=1}^nP(i)^2\\
Gain=\bigtriangleup =Gain(D)-Gini(D|A)
$$
ID3,C4.5,CART三种算法总结

```
ID3和C4.5只适合在小规模数据及上使用
ID3和C4.5算法都是单变量决策树
当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差
决策树分类一般情况只适合小数据量的情况(数据可以放内存)
CART算法是三种算法中最常用的一种决策树构建算法(sklearn中仅支持CART)。
三种算法的区别仅仅只是对于当前树的评价标准不同而已，ID3使用信息增益、
C4.5使用信息增益率、CART使用基尼系数。
CART算法构建的一定是二叉树，ID3和C4.5构建的不一定是二叉树
```

| 算法 | 支持模型  | 树构建 | 特征选择        | 连续值处理 | 缺失值处理 | 剪枝   | 特征属性多次使用 |
| ---- | --------- | ------ | --------------- | ---------- | ---------- | ------ | ---------------- |
| ID3  | 分类      | 多叉树 | 信息增益        | 不支持     | 不支持     | 不支持 | 不支持           |
| C4.5 | 分类      | 多叉树 | 信息增益率      | 支持       | 支持       | 支持   | 不支持           |
| CART | 分类/回归 | 二叉树 | 基尼系数/均方差 | 支持       | 支持       | 支持   | 支持             |

#### 分类树和决策树的区别

```
分类树采用信息增益、信息增益率、基尼系数来评价树的效果，都是基于概率值
进行判断的；而分类树的叶子节点的预测值一般为叶子节点中概率最大的类别作
为当前叶子的预测值。
在回归树中，叶子节点的预测值一般为叶子节点中所有值的均值来作为当前叶子
节点的预测值。所以在回归树中一般采用MSE作为树的评价指标，即均方差。
一般情况下，只会使用CART算法构建回归树。
```

$$
MSE=\frac{1}{n}\sum_{i=1}^n(y-\hat{y}_i)^2
$$



#### 决策树优化

##### 决策树的的优化策略

剪枝优化：

​	决策树过度拟合一般情况下由于节点太多导致的，剪枝优化对决策树的正确率影响是比较大的，也就是常用的一种优化方式。

Random Forest

​	利用训练数据集随机产生多个决策树，形成一个森林。然后使用这个森林对数据进行预测，选取最多结果作为预测结果。

#### 剪枝

前置剪枝：在构建决策树的过程中，提前停止。结果时决策树一般比较小，效果不是太好。

后置剪枝：在决策树构建好后，然后开始裁剪，一般使用两种方式：

​	1）用单一叶子节点代替整个子树，叶节点的分类采用子树中最主要的分类

​	2）将一个子树完全代替另外一棵子树；后置剪枝的主要问题是急速那效率的问题，存在一定的浪费情况。

后置剪枝（交叉验证）：

​	由完全树$T_0$开始，剪枝部分节点得到$T_1$，在此剪枝得到$T_2$......知道仅剩根的树$T_k$

​	在验证数据集上对这k+1树进行评价，选择最优树$T_a$(损失函数最小的树)

##### 后置剪枝过程

​	对于给定的决策树$T_0$

​		计算所有内部非叶子节点的剪枝系数

​		查找最小剪枝系数的节点，经期子节点进行删除操作，进行剪枝得到决策树$T_k$,如果存在多个最小剪枝系数节点，选择包含数据项最多的节点进行剪枝操作

​		重复上述操作，知道产生的剪枝决策树$T_k$只有一个节点

​		得到决策树$T_0T_1..T_k$

​		使用验证样本集选择最优子树$T_a$

使用验证集选择最有子树的标准，可以使用原始损失函数来考虑：
$$
loss = \sum_{t=1}^{leaf}\frac{|D_t|}{D}H(t)
$$
叶子节点越多，决策树越复杂，损失越大，修正添加剪枝系数:
$$
loss_{\alpha}=loss+\alpha*leaf
$$
考虑根节点为r的子树，剪枝后的损失函数分别为loss(R)和loss(r),当着两者相等的时候，可以求得剪枝系数
$$
loss_{\alpha}=loss(r)+\alpha\\
loss_{R}=loss(R)+\alpha*R_{leaf}\\
\alpha=\frac{loss(r)-loss(R)}{R_{leaf}-1}
$$

#### 决策树可视化

##### 决策树可视化准备工作

```
决策树可视化可以方便我们直观的观察所构建的树模型；决策树可视化依赖
graphviz服务，所以我们在进行可视化之前，安装对应的服务；操作如下：
安装graphviz服务
安装python的graphviz插件： pip install graphviz
安装python的pydotplus插件： pip install pydotplus
graphviz服务安装：
下载安装包(msi安装包): http://www.graphviz.org/；
执行下载好的安装包(双击msi安装包)；
将graphviz的根目录下的bin文件夹路径添加到PATH环境变量中；
```

##### 决策树可视化案例

```
方式一：将模型输出dot文件，然后使用graphviz的命令将dot文件转换为pdf
格式的文件
方式二：直接使用pydotplus插件直接生成pdf文件进行保存
方式三：使用Image对象直接显示pydotplus生成的图片
```



### <font color=salmon size=5 face="微软雅黑"> 第四章机器学习之集成学习</font>

#### 集成学习

~~~
集成学习的思想是将若干个弱分类器（分类器和回归器）组合之后产生一个新的学习器。
弱分类器就是哪些学习能力相对较差的分类器。
~~~

##### 集成学习的需要

~~~
集成一些弱分类器，可以是导致分类边界不同的存在一些错误的弱分类器，在集成后得到更加合理的边界，减少了整体的粗无虑，实现更好的结果。
对于过大或者过小的数据集，可以通过进行划分和又放回的操作善生不同的数据子集，使用数据子集可以训练不同的分类器，最终合并成为一个大的分类器。
数据的划分如边界过于复杂，使用线性模型很难描述情况，那么可以训练多个模型，在进行模型融合。
对于比较难融合的多个异构的特征集。可以考虑每个数据集构建一个分类器模型，然后将多个模型融合。
~~~

##### Bagging

~~~
Bagging即自举枚举法，在元数据上通过有放回的抽样的方式，重新选择s个新数据集分别训练s个分类器的集成技术。
Bagging方法训练出来的模型在预测新样本分类的时候，会使用多数投票或者求均值的方式来统计最终的分类结果。
Bagging方法的弱分类器可以使基本的算法模型。eg：Linear、Ridge、Lasso、Logistic、Softmax、ID3、C4.5、SVM、KNN等
~~~



#### 随机森林

~~~
鉴于决策树容易欠拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树，假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，如果全样本去训练m棵决策树显然不可取，全样本训练忽视了
局部样本的规律，对于模型泛化能力是有害的。
~~~

bagging+决策树=随机森林

>1、从原始样本集（n个样本）找那个用Bootstrap采样（有放回）选出n个样本，去重。（用户模型训练模型的样本数目实际上要小于n）
>2、从所有属性中随机选取K个属性。从K个属性找那个选择出最佳分割属性作为节点来迭代创建决策树。
>3、重复以上两步m次，创立m棵决策树。
>4、这m个决策树形成随机森林，通过投票结果表决数据属于哪一类。

RF算法在实际应用中具有比较好的特性，应用也比较广泛，主要应用在：分类、回归、特征转换、异常点检测。常见的RF变种算法：

##### Extra Tree

>和RF的区别：
>​	RF会随机采样来作为子决策树的训练集，而Extra Tree每个子决策树采用原始数据集训练。
>​	RF在选择划分特征点的时候回和传统决策树一样，会基于信息增益、信息增益率、基尼系数、均方差等原则来选择最优特征值；而Extra Tree会随机的选择一个特征值来划分决策树。
>​	Extra Tree因为是随机选择特征值的划分点，会导致决策树的规模一般大于RF所生成的决策树。Extra Tree模型的方差相对于RF进一步减少。某些情况下，Extra Tree的泛化能力比RF强。
>
><font color="red">**question**：Extra Tree每个子决策树采用原始数据集训练（这里是没有去重么？）</font>

##### TRTE

>TRTE是一种非监督的数据转化方式。将低维的数据映射到高维，高维数据更好的应用于分类回归模型。
>
>TRTE算法的转化过程类似RF算法的方法。建立T个局册数来拟合数据（是类似KD-TREE一样基于特征属性的方差选择划分特征）。当决策树构建完成后，数据集利的每个数据在T个决策树中叶子节点的位置就定下来了，将位置信息转换为向量就完成了特征转换操作 。
>
>案例：有三棵决策树，各个决策树的叶子节点数目分别为：5,5,4，某个数据x划分到第一个决策树的第三个叶子节点，第二个决策树的第一个叶子节点，第三个决策树的第四个叶子节点，那么最终的x映射特征码为：(0,0,1,0,0 1,0,0,0,0 0,0,0,1)

##### Isolation Forest(IForest)

>IForest是一种异常点检测算法，使用类似RF的方式来检测异常点；IForest算法和RF区别在于：
>
>1、在随机采样过程中，一般只需要少量数据即可。
>
>2、在进行决策树构建过程中，IForest算法会随机选择一个划分特征，并对划分特征随机选择一个划分阈值。
>
>3、IForest算法构建的决策树一般深度max_depth是比较小的。
>
>区别原因：目的是异常点检测，所以只要能够区分异常的即可，不需要大量数据；另外在异常点检测的过程中，不需要太大规模的决策树。

异常点检测的算法过程：

对于异常点的判断。则是将测试样本x拟合到T棵决策树上。计算在每课树上该样本的叶子节点的深度$h_t(x)$。从而计算出平均深度$h(x)$;然后就可以使用下列公式计算样本点x的异常概率值，$p(s,m)$的取值范围为[0,1],接近于1，则是异常点的概率越大。如果落在叶子节点为正常样本点，那么当前决策树不考虑，如果所有决策树上都是正常样本点，那么直接认为异常点概率为0.
$$
p(x,m)=2^{-\frac{h(x)}{c(m)}}\\
c(m) = 2ln(m-1)+\xi-2\frac{m-1}{m}
$$

##### RF总结

RF的主要特点：

>1、训练棵进行合并化，对于大规模样本的训练具有速度的优势。
>
>2、由于进行随机选择决策划分特征集表，这样在样本维度比较高的时候，仍然具有比较高的训练性能。
>
>3、给以给出各个特征的重要性列表。
>
>4、由于存在随机抽样，训练出来的模型方差小，泛化能力强。
>
>5、RF实现简单。
>
>6、对于部分特征的缺失感不敏感。

RF的主要特点

>1、在某些噪音比较大的特征上，RF模型容易陷入过拟合。
>
>2、取值比较多的划分 特征对RF的决策会产生更大的影响，从而可能影响模型的效果。

#### 提升算法

##### 随机森林的思考

在随机森林的构建过程中，由于各科树之间是没有关系的，相对独立的；在构建的过程中，构建第m棵子树的时候，不会考虑前面的m-1棵树。

##### Boosting

提升学习是一种机器学习的技术，可以用于回归和分了IDE问题，它每一步产生弱分类预测模型（如决策树），并加权累加到总模型中，如果每一步的弱预测模型的生成都是依据损失函数的梯度方式，那么就称为梯度提升。

提升技术的意义：如果一个问题存在若预测模型，那么可以通过提升技术的办法得到一个强预测模型。

常见的模型有：

​	Adaboost

​	Gradient Boosting(GBT/GBDT/GBRT)

##### Bagging、Boosting区别

1.样本选择：Bagging算法是又放回的随机采样；Boosting算法是每一轮训练集不变，只是训练集中的每个样例在分类器中的权重发生变化或者目标属性y发生变化，而权重&y值都是根据上一轮的预测结果进行调整；

2.样例权重：Bagging使用随机抽样，样例是等权重；Boosting根据错误率不断的调整样例的权重值，错误率越大则权重越大（Asaboost）；

3.预测函数：Bagging所有预测模型的权重相等；Boosting算法对于误差晓得分类器具有更大的权重

4.并行计算：Bagging算法可以并行声场各个基模型；Boosting理论上只能顺序生产，因为后一个模型需要前一个模型的结果；

5.Bagging是减少模型的variance（方差）；Boosting是减少模型的Bias（偏度）。

6.Bagging利每个分类模型都是强分类器，因为降低的是方差，方差过高需要降低是过拟合；Boosting利每个分类模型都是弱分类器，因为降低的是偏度，偏度过高是欠拟合。

7。Bagging对样本重采样，对每一轮的采样数据集都训练一个模型，最后去平均值。由于样本集的相似性和使用的同种模型，因此各个模型的具有相似的bias和
$$
E\left\{\frac{\sum_{i=1}^n}{n}\right\}=E(X_i)\\
var\left\{\frac{\sum_nX_i}{n}\right\}=\frac{Var(X_i)}{n}:模型完全独立\\
Var\left\{\frac{\sum_nX_i}{n}\right\}=Var(X_i):模型完全相同
$$

##### stacking

![](imgs/stacking流程图.jpg)

上半部分是五折交叉验证，用XGBoost作为基础模型modle1，五折交叉验证拿出四折作为training data，另外一折作为testing data。<font color="red">注意：在stacking中此部分数据会用到整个training set。假设training set包含了10000个数据，testing set包含了2500个数据。那么每一次交叉验证其实就是对training  set进行划分没在每一次的交叉验证中training data将会是8000行，testing data是2000行</font>

每一次的交叉验证包含两个过程，1、基于training data训练模型 2、基于triaining data训练生成的模型对testing data进行预测。在整个第一次交叉验证完成以后我们将会得到关于当前testing data的预测值，这将是一个一维2000行的数据，记为a1。<font color=red>注意</font>这部分操作完成之后，我们还要对数据集原来的整个testing set进行预测，这个过程会产生2500个预测值，这部分预测值将会做为下一层模型testing data的一部分，记为b1,。因为进行的是五折交叉验证，所以以上提及的过程将会进行五次，最终生成对testing set数据预测的5列2000行数据a1,a2,a3,a4,a5,testing set的预测回事5列2500行数据b1,b2,b3,b4,b5。

在完成对Model1的整个过程之后，我们发现a1,a2,a3,a4,a5其实就是对原来整个training set的预测值，将他们拼凑起来，会形成一个10000行一列的矩阵，记为A1。而对于b1,b2.b3.b4.b5这部分数据，我们将各部分相加去平均值，得到一个2500行一列的矩阵，记为B1。

##### AdaBoost算法原理

>Adaptive Boosting是一种迭代算法。每轮迭代中会在训练集上产生一个新的学习器，然后使用该学习器对所有样本进行预测，以评估每个样本的重要性（informative）。算法为每个样本赋予一个权重，每次用训练好的学习器标注/预测各个样本，如果某个样本点呗预测的越正确，则将其权重降低，否则提高样本的权重。权重越高的样本在下一个迭代训练中所占的比重就越大，也就是说越难区分的样本在训练过程中会变得越重要。整个迭代过程直到错误率足够小或者达到一定的迭代系数为止。

Adaboost算法将基分类器的线性组合作为强分类器，同时给误差率较小的基分类器以大的权值，给分类器误差率大的分类器以小的权值，构建的线性组合为：
$$
f(x)=\sum_{m=1}^M\alpha_mG_m(x)
$$
最终分类器是在线性组合的基础上进行Sign函数转换：
$$
G(x)=sign(f(x))=sign\left[\sum_{m=1}^M\alpha_mG_m(x)\right]
$$
最终的强学习器：$$G(x)=sign(f(x))=sign\left[\sum_{m=1}^M\alpha_mG_m(x)\right]$$

损失函数（以错误率作为损失函数）：$$loss=\frac{1}{n}\sum_{i=1}^nI(G(x_i)\neq y_i)$$

损失函数：$$loss=\frac{1}{n}\sum_{i=1}^nI(G(x_i)\neq y_i)\leq\frac{1}{n}\sum_{i=1}^ne^{(-y_if(x))}$$

第k-1轮的强学习器：
$$
f_{k-1}(x)=\sum_{j=1}^{k-1}\alpha_jG_j(x)
$$
第k轮的强学习器：
$$
f_k(x)=\sum_{j=1}^k\alpha_jG_j(x)\\
f_k(x)=f_{k-1}(x)+\alpha_kG_k(x)
$$
损失函数：
$$
loss(\alpha_m,G_m(x))=\frac{1}{n}\sum_{i=1}^ne^{(-y_i(f_{m-1}(x)+\alpha_mG_m(x)))}
$$

$$
loss(\alpha_m,G(m))=\frac{1}{n}\sum_{i=1}^ne^{-y_i(f_{m-1}(x)+\alpha_mG_m(x))}\\
=\frac{1}{n}\sum_{i=1}^ne^{-y_if_{m-1}(x)}e^{(-y_i\alpha_mG_m(c))}\\
\overset{令\bar{w_{mi}}=e^{-y_if_{m-1}(x)}}{\rightarrow}\\
=\frac{1}{n}\sum_{i=1}^n\bar{w_{mi}}e^{-y_i\alpha_mG_m(x)}
$$
是下列公式达到最小值的$\alpha_m$和$G_m$就是Adaboost算法的最终求解值
$$
loss(\alpha_m,G_m(x))=\frac{1}{n}\sum_{i=1}^n\bar{w}_{mi}e^{-y_i\alpha_mG_m(x)}
$$
G分类器在训练的过程中，是为了让误差率最小，所以可以认为G越小就是误差率越小
$$
G_m^*(x)=\frac{1}{n}\sum_{i=1}^n\bar{w}_{mi}I(y_i\neq G_m(x_i))\\
\varepsilon_m=P(G_m(x)\neq y)=\frac{1}{n}\sum_{i=1}^n\bar{w}_{mi}I(y_i\neq G_m(x))
$$
对于$\alpha$而言，通过求导后令导数为零，可以得到公式（log对象可以以e为底也可以以2为底）:
$$
\alpha_m^*=\frac{1}{2}\ln(\frac{1-\varepsilon_m}{\varepsilon_m})
$$

##### Adaboost算法构建过程

>1、假设训练数据集$T={(X_1,Y_1),(X_2,Y_2),...,(X_n,Y_n)}$
>
>2、初始化训练数据权重分布
>$$
>D_1=(W_{11},W_{12},...W_{1i},...,w_{1n}),w_{1i}=\frac{1}{n},i=1,2,3,...,n
>$$
>3、使用具有权值分布$D_m$的训练数据集学习，得到基本分类器
>$$
>G_m(x):x\rightarrow \{-1,+1\}
>$$
>4、计算$G_m(x)$在训练集上的分类误差
>$$
>\varepsilon_m =P(G_M(X_I)\neq y_i)=\sum_{i=1}^nw_{mi}I(G_m(x_i)\neq y_i)
>$$
>5、计算$G_m(x)$的模型的权重系数$\alpha_m$
>$$
>\alpha_m = \frac{1}{2}*log_2{\left(\frac{1-\varepsilon_m}{\varepsilon_m}\right)}
>$$
>

>6、权重训练数据集的权值分布
>$$
>D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m+1,i},...,w_{m+1,n})\\
>w_{m+1,i}=\frac{w_{m,i}}{Z_m}e^{-\alpha_m y_iG_m(x)}
>$$
>7、这里$Z_m$是规范化因子（归一化）
>$$
>Z_m=\sum_{i=1}^nw_{m.i}e^{-\alpha_my_iG_m(x_i)}
>$$
>8、构建基本分类器的线性组合
>$$
>f(x)=\sum_{m=1}^M\alpha_mG_m(x)
>$$
>9、得到最终分类器
>$$
>G(x)=sign(f(x))=sign\left(\sum_{m=1}^M\alpha_mG_m(x)\right)
>$$
>

AdaBoost直观理解

~~~python
#X  0    1   2   3   4   5   6   7   8   9
#Y  1    1   1  -1  -1  -1   1   1   1  -1
#w1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1
import numpy as np
import math

#计算信息熵公式
def entropy(t):
    return np.sum(-x*np.log2(x) for x in t)

#原始的信息熵
a = entropy([0.6,0.4])

def errorate(ε):
    return 0.5*np.log2((1-ε)/ε)

X = [0,1,2,3,4,5,6,7,8,9]
Y = [1,1,1,-1,-1,-1,1,1,1,-1]
W = [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]


def G1(x,t):
        if x < t:
            return 1
        else:
            return -1

D1 = []
weight = []

def updateWeight(alpha,X,Y,W,t):
    S = 0
    for i in range(len(X)):
        d= W[i]*pow(np.e,(-1)*alpha*Y[i]*G1(X[i],t))
        D1.append(d)
    for j in D1:
        S += j
    return D1/S

#当x以2.5作为划分的特征属性，来计算其信息增益
b1 = entropy([1.0])
b2 = entropy([3/7,4/7])
#合并信息熵，得到条件熵
b = 0.3*b1+0.7*b2
#以x=2.5为划分特征属性得到的信息增益
print('以x=2.5为信息熵得到的信息增益%.3f'%(a-b))

#当以x=5.5作为划分特征属性，来计算其信息增益
c1 = entropy([0.5,0.5])
c2 = entropy([0.75,0.25])
#合并信息熵，得到条件熵
c = 0.6*c1+0.4*c2
#以x=5.5作为划分特征属性的信息增益
print('以x=5.5作为划分特征属性的信息增益%.3f'%(a-c))


#当以x=8.5为划分特征属性的信息增益
d1 = entropy([6/9,3/9])
d2 = entropy([1.0])
#合并信息熵，获得条件熵
d = 0.9*d1+0.1*d2
#以x=8.5为划分特征属性的信息增益
print('以x=8.5为划分特征属性的信息增益%.3f'%(a-d))

#通过计算可得出信息增益最大的是以x=2.5作为划分特征属性的时候
#G1(x)在训练集上的误差率为 ε1
ε1 = 0.3
alpha1 = errorate(ε1)
print('以2.5作为划分特征属性的误差率:{},G1的系数为alpha1:{}'.format(ε1,alpha1))
print('f(x)=%.4fG1(x)'%(alpha1))
newWeight = updateWeight(alpha1,X,Y,W,2.5)
print(newWeight)
print("="*50)
#得到的新的权重值
#X  0    1   2   3   4   5   6   7   8   9
#Y  1    1   1  -1  -1  -1   1   1   1  -1
W1= [0.05818722,0.05818722,0.05818722,0.05818722,0.05818722,0.05818722,0.19756314,0.19756314,0.19756314,0.05818722]

#通过计算可得出信息增益最大的是以x=5.5作为划分特征属性的时候
#G2(x)在训练集上的误差率为 ε2
ε2 = 0.05818722*4
alpha2 = errorate(ε2)
print('以5.5作为划分特征属性的误差率:{},G2的系数为alpha2:{}'.format(ε2,alpha2))
print('f(x)=%.4fG1(x)+%.4G2(x)'%(alpha1,alpha2))
def G2(x, t):
    if x > t:
        return 1
    else:
        return -1


D2 = []
def updateWeight(alpha, X, Y, W,t):
    S = 0
    for i in range(len(X)):
        d = W[i] * pow(np.e, (-1) * alpha * Y[i] * G2(X[i],t))
        D2.append(d)
    for j in D2:
        S += j
    return D2 / S
newWeight2 = updateWeight(alpha2,X,Y,W1,5.5)
print(newWeight2)
print("="*50)
#得到的新的权重值
#X  0    1   2   3   4   5   6   7   8   9
#Y  1    1   1  -1  -1  -1   1   1   1  -1
W2=[0.19710632,0.19710632,0.19710632,0.03526245,0.03526245,0.03526245,0.03526245,0.03526245,0.03526245,0.19710632]


#通过计算可得出信息增益最大的是以x=5.5作为划分特征属性的时候
#G3(x)在训练集上的误差率为 ε3
ε3 = 0.0281336*3
alpha3 = errorate(ε3)
print('以8.5作为划分特征属性的误差率:{},G3的系数为alpha3:{}'.format(ε3,alpha3))

def G3(x, t):
    if x < t:
        return 1
    else:
        return -1


D3 = []
def updateWeight(alpha, X, Y, W,t):
    S = 0
    for i in range(len(X)):
        d = W[i] * pow(np.e, (-1) * alpha * Y[i] * G3(X[i],t))
        D3.append(d)
    for j in D2:
        S += j
    return D3 / S
newWeight3 = updateWeight(alpha3,X,Y,W2,5.5)
print('f(x)=%.4fG1(x)+%.4fG2(x)+%.4fG3(x)'%(alpha1,alpha2,alpha3))
print(newWeight3)
print("="*50)
#得到的新的权重值
#X  0    1   2   3   4   5   6   7   8   9
#Y  1    1   1  -1  -1  -1   1   1   1  -1
W2=[0.04035885,0.04035885,0.04035885,0.22503901,0.22503901,0.22503901,0.22503901,0.22503901,0.22503901,0.04035885]

#计算结果显示
'''
以x=2.5为信息熵得到的信息增益0.281
以x=5.5作为划分特征属性的信息增益0.046
以x=8.5为划分特征属性的信息增益0.144
以2.5作为划分特征属性的误差率:0.3,G1的系数为alpha1:0.611196210668224
f(x)=0.6112G1(x)
[0.05818722 0.05818722 0.05818722 0.05818722 0.05818722 0.05818722
 0.19756314 0.19756314 0.19756314 0.05818722]
==================================================
以5.5作为划分特征属性的误差率:0.23274888,G2的系数为alpha2:0.8604623106008397
f(x)=0.6112G1(x)+0.86052(x)
[0.15725825 0.15725825 0.15725825 0.0281336  0.0281336  0.0281336
 0.09552206 0.09552206 0.09552206 0.15725825]
==================================================
以8.5作为划分特征属性的误差率:0.0844008,G3的系数为alpha3:1.7196938117660618
f(x)=0.6112G1(x)+0.8605G2(x)+1.7197G3(x)
[0.04035885 0.04035885 0.04035885 0.22503901 0.22503901 0.22503901
 0.22503901 0.22503901 0.22503901 0.04035885]
'''
~~~

##### Adaboost总结

Adaboost的优点如下：

>可以处理连续值和离散值
>
>模型的鲁棒性比较强
>
>解释强，结构简单

缺点：

>对异常样本敏感，异常样本可能会在迭代过程中获得较高的权重值，最终影响模型效果。

#### GBDT(迭代决策树)

##### GBDT

>GBDT也是Boosting算法的一种，但是和Adaboost算法不同：
>
>Adaboost算法是利用前一轮学习器的误差来更新样本权重值，然后一轮一轮的迭代，GBD也是迭代，但是GBDT要求弱学习器必须是CART模型，而且GBDT在模型训练的时候，是要求模型预测的样本损失尽可能的小。
>
>GBDT中，底层都是回归树。

##### 梯度提升迭代决策树GBDT

>GBDT由但部分构成：DT(Regression Decision Tree)，GB(Gradient Boosting)，和Shrinkage（衰减）
>
>DT(回归树)：
>
>​	回归树和分类树的区别（前者用于预测实数值，如明天的温度，用户的年龄，网页的相关程度；后者用于分类标签值，如晴天/阴天/雾/雨，用户性别、网页是否为垃圾页面。）GBDT中用得是CART，这是一种回归树。GBDT的核心在于累加所有树的结果作为最终的结果，而分类树累加没有意义。所以GBDT是由回归树构成的。（尽管GBDT调整后也可以用于分类但不代表GBDT的树是分类树）。
>
>GB(梯度迭代)：
>
>​	GBDT的核心在于，每一刻树学的是之前所有树结论和残差，这个残差就是一个假预测值后能得到的真实值的累加量。ep：比如A的真实年龄是18岁，但第一棵树的预测年龄是14岁，差了4岁，即残差为4岁。那么在第二棵树利我们把A的年龄设置为4岁去学习，如果第二棵树真的能把A分到4岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的年龄是3岁，则A任然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。这就是Gradient Boosting在GBDT中的意义。
>
>Shrinkage(缩减)：
>
>​	Shrinkage的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合，即它不完全相信每一棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。
>
>由多棵决策树组成，所有树的结果累加起来就是最终结果
>
>迭代决策树和随机森林的区别：
>
>​	随机森林使用抽取不同的样本构建不同的子树，也就是说第m棵树的构建和前m-1棵树的结果是没有关系的
>
>​	迭代决策树在构建子树的时候，使用之前子树构建结果后形成残差作为输入数据构建下一个子树，然后最终预测的时候按照子树构建的顺序进行预测，并将预测结果相加 

##### GBDT算法原理

给定输入向量X和输出向量Y组成若干训练样本$(X_1,Y_1),(X_2,Y_2),...,(X_n,Y_n)$,目标是找到近似函数F(X),使损失函数L（Y，F(X))的损失之最小。

L损失函数一般采用最小二乘法函数或者绝对值损失函数。
$$
L(y,F(X))=\frac{1}{2}\left(y-F(X)\right)^2\\
L(y,F(X))=|Y-f(X)|
$$
最优解为：
$$
F^*(X)=\underset{F}{argmin}L(y,F(X))
$$
假定F(X)是一组最优基函数$f_i(X)$的加权和：
$$
F(X)=\sum_{i=1}^Mf_i(X)\overset{防止每个学习器能力过强导致过拟合,给定一个缩放系数v}\Longrightarrow F(X)=v\sum_{i=0}^Mf_i(X)
$$
以贪心算法（局部最优解）的思想扩展得到$F_m(X)$,求解最优f
$$
F_m(x)=F_{m-1}(x)+\underset{f} {argmin}\sum_{i=1}^nL(y_i,F_{m-1}(X_i)+f_m(X_i))
$$
给定常函数F_0(X)
$$
F_0(X)=\underset{c}{argmin}\sum_{i=1}^nL(y_i,c)
$$
根据梯度值下降计算y的变化值
$$
\alpha_{im}=\left[\frac{\part L(y_i,F(x_i))}{\part F(x_i)}\right]F(x)=F_{m-1}(x)
$$
使用数据$(x_i,\alpha_m)(i=1,2,....,n)$计算拟合残差找到一个CART回归树，得到第m棵树
$$
f_m(x)=f_{m-1}(x)+\sum_{j=1}^{|leaf|_m}c_{mj}I(x)(\in leaf_{mj})
$$

##### GBDT回归算法和分类算法的区别

两者唯一的区别就是选择不同的损失函数

回归算法中选择的损失函数一般是均方差（最小二乘）或者绝对值误差值；而在分类算法重一般的损失函数选择对函数来表示。
$$
L(y,f(x))=\frac{1}{2}(y-f(x))^2\\
L(y,f(x))=\ln(1+e^{(-y^*f(x))});y\in\{-1,+1\}\\
L(y,f(x))=-\sum_{k=1}^Klog(p_k(x));K分类中
$$

##### GBDT总结

优点：

可以处理连续值和离散值，在相对较少的调参情况下，模型的预测效果也会不错；模型的鲁棒性比较强。

缺点：

由于弱学习器之间存在关联关系，难以并行训练模型。也就是模型训练的速度慢。

### <font color=salmon size=5 face="微软雅黑"> 第五章机器学习之XGBoost</font>

#### XGBoost概述

XGBoost是GBDT算法的一种变种，是一种常用的有监督集成学习算法；是一种伸缩性强，便捷的可并行构建模型的Gradient Boosting算法。

XGBoost官网:<https://xgboost.readthedocs.io/en/latest/>

XGBoost Github源码位置：<https://github.com/dmlc/xgboost>

XGBoost支持开发语言：Python、R、Java、Scala、C++等。

#### XGBoost安装

安装方式一：编译Github上的源码，参考<http://xgboost.readthedocs.io/en/latest/build.html>

安装方式二：直接使用python的whl文件进行安装，要求python版本3.5或者3.6；

下载链接：<https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost>

安装参考命令：pip install f:///xgboost-0.7-cp36-cp36m-win_amd64.whl

#### XGBoost原理讲解

##### 模型

###### 目标函数

$$
Obj(\theta) = L(\theta)+\Omega(\theta)
$$

误差函数：体现的是模型有多拟合数据。

正则化项：惩罚复杂模型的参数，用于解决过拟合。

![](../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/imgs/%E6%A8%A1%E5%9E%8B.png)

###### GBDT的目标函数

$$
\hat{a}_i^{(0)}=0\\
\hat{y_i}^{(1)}=\hat{a}_i^{(0)}+f_1(x_i)\\
\hat{y_i}^{(2)}=\hat{a}_i^{(0)}+f_2(x_i)\\
...\\
\hat{y_i}^{(t)}=\hat{a}_i^{(t-1)}+f_t(x_i)\\
obj=\sum_{i=1}^n(y_i,\hat{y}_i^{(t)})
$$

###### XGBoost目标函数

$$
\hat{a}_i^{(0)}=0\\
\hat{y_i}^{(1)}=\hat{a}_i^{(0)}+f_1(x_i)\\
\hat{y_i}^{(2)}=\hat{a}_i^{(0)}+f_2(x_i)\\
...\\
\hat{y_i}^{(t)}=\hat{a}_i^{(t-1)}+f_t(x_i)\\
obj=\sum_{i=1}^n(y_i,\hat{y}_i^{(t)})+\sum_{i=1}^t\Omega(f_i)\\
f_t(x)=w_q(x)\\
\Omega(f)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2\;\;\;{\color{red} 正则化项系数防止过拟合}
$$

##### XGBoost公式推导

第t次迭代后，模型的预测等于前t-1次的模型加上第t棵树的预测：
$$
\hat{y}_i^{(t)}=\hat{y}_i^{(t-1))}+f_t(x_i)
$$
目标函数可以写成：
$$
loss=\sum_{i=1}^nl(y_i,\hat{y_i}^{(t-1)}+f_t(x_i))+\sum_{i=1}^{t-1}\Omega(f_i)+\Omega(f_t)
$$
将误差函数中的$\hat{y}_i^{(t-1)}+f_t(x_i)$看成一个整体，求解这个整体取值在$\hat{y}_i^{(t-1)}$处进行二次泰勒展开：
$$
loss\approx\sum_{i=1}^n\left[l(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)\right]+\sum_{t=1}^{t-1}\Omega(f_i)+\Omega(f_t)\\
g_i=\part_{\hat{y}_i^{(t-1)}}l(y_i,\hat{y}_i^{(t-1)})\\
h_i=\part_{\hat{y}_i^{(t-1)}}^2l(y_i,\hat{y}_i^{(i)})\\
$$
将函数中的所有常数项全部去掉，可以得到以下的公式：
$$
loss\approx\sum_{i=1}^n\left[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)\right]+\Omega(f_t)
$$
将函数f和正则项带入公式中得到以下公式：
$$
loss\approx\sum_{i=1}^n\left[g_iw_{q(x_i)}+\frac{1}{2}h_iw_{q(x_i)}^2\right]+\gamma T+\lambda\frac{1}{2}\sum_{j=1}^Tw_j^2
$$
定义每个叶子节点j上的样本集合为：$I_j$
$$
I_j=\left\{i|q(x_i)==j\right\}
$$
将样本累加操作转换为叶节点的操作：
$$
loss\approx\sum_{j=1}^T\left[\left(\sum_{i\in I_j}g_i\right)w_j+\frac{1}{2}\left(\sum_{i\in I_j}h_i\right)w_j^2\right]+\gamma T+\lambda\frac{1}{2}\sum_{j=1}^Tw_j^2\\
=\sum_{j=1}^T\left[\left(\sum_{i\in I_j}g_i\right)w_j+\frac{1}{2}\left(\sum_{i\in I_j}h_i\right)w_j^2\right]+\gamma T=\sum_{j=1}^T\left[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2\right]+\gamma T
$$
最终的目标函数：
$$
loss=\sum_{j=1}^T\left[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2\right]+\gamma T
$$
如果树的结构可以确定（q函数确定），为了使目标函数最小，可以令导数为0，可以求得最优的w，将w带入目标函数，可以得到最终的损失为：
$$
w_j^*=-\frac{G_j}{H_j+\lambda};\;\;\ loss^*=-\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T
$$
![](../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/imgs/xgboost%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC.png)

当树的结构确定的时候，我们可以得到最优的叶子节点分数以及对应的最小损失值，问题在于如何确定树结构？

​	暴力穷举所有可能的结构，选择损失值最小值；（很难求解）

​	贪心法，每次尝试选择一个分裂点进行分割，计算操作前后的增益，选择增益最大的方式进行分裂。

##### 决策树相关算法指标

ID3算法：信息增益。

C4.5算法：信息增益率。

CART:Gini系数。

##### XGBoost的学习策略

XGBoost目标函数：
$$
loss^*=-\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T
$$
从目标函数中，我们希望损失函数越小越好，那就是$\frac{G^2}{H+\lambda}$越大越好；从而，对于一个叶子节点的分裂的分裂，分裂前后的信息增益定义为：
$$
Gain=\frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{(H_L+H_R)+\lambda}\right]-\gamma
$$
Gain值越大，分裂后减少的损失值越大。所以对于一个叶子节点分割时，计算所有候选的（feature，value）对应的Gain，选择Gain最大的特征进行分割。

##### 树节点分裂方法

近似算法：对于每个特征，之考虑分位点，减少计算复杂度。

![](C:\Users\Administrator\Desktop\机器学习总结\imgs\树节点分裂方法.png)

近似算法案例：三分位数

![](C:\Users\Administrator\Desktop\机器学习总结\imgs\树节点分裂方法二.png)
$$
Gain=\left\{max\left[\frac{1}{2}\frac{G_1^2}{H_1+\lambda}+\frac{G_{23}^2}{H_{23}+\lambda}-\frac{(G_{123})^2}{(H_{123}+\lambda)}\right]-\gamma,\\
\frac{1}{2}\left[\frac{G_3^2}{H_3+\lambda}+\frac{G_{12}^2}{H_{12}+\lambda}\right]-\gamma\right\}
$$

##### XGBoost树节点划分方法

XGBoost不是简单的按照样本个数进行分位的，而是按照上一轮的预测误差函数的二阶导数值作为权重来进行划分的：

![](C:\Users\Administrator\Desktop\机器学习总结\imgs\树节点划分方式.png)

##### XGBoost的其他特性

列采样（Column subsampling）：借鉴随机森林的做法，支持列抽样，支持咧抽样，不仅可以降低过拟合，还可以减少计算量。

支持对缺失值得自动处理。对于特征的值有缺失的样本，XGBoost可以自动学习分裂方向。

XGBoost支持并行，XGBoost的并行时特征粒度上的，在计算特征的Gain的时候，会并行执行，但是在树的构建过程中，还是穿行构建的。

XGBoost算法中加入正则项，用于控制模型的复杂度，最终模型更加不容易过拟合。

XGBoost基学习期支持CART、线性回归、逻辑回归。

XGBoost支持自定义损失函数（要求损失函数可导）。





#### XGBoost项目案例