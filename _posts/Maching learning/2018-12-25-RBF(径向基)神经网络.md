---
layout: post
comments: true
categories: 机器学习
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

* content
{:toc}
### RBF（径向基）神经网络

>只要模型是一层一层的，并使用AD/BP算法，就能称作BP神经网络，RBF神经网络使其中一个特例。其中包括以下内容：

#### 1.什么是径向基函数

​	径向基函数时一个取值仅仅依赖于原点距离的实值函数，也就是$\phi(x)=\phi (||x||)$,或者还可以是到任一点c的距离，c称为中心点，也就是$\phi (x,c)=\phi(||x-c||)$。
任意一个满足$\phi(x)=\phi(||x||)$特性的函数$\phi$都叫做径向基函数,标准的一般使用欧氏距离（也叫做欧氏径向基函数），尽管其他函数也是可以的。
​	最常用的径向基函数是高斯核函数，形式为$k(||x-xc||)=\exp{-||x-xc||^2/(2*\sigma)^2}$,其中$x_c$为核函数中心，$\sigma$为函数的宽度参数，控制了函数径向作用范围。

#### 2.RBF神经网络

​	RBF神经网络使一种三层神经网络，其包括输入层，隐层，输出层。从输入层到隐层空间的变换是非线性的，而从隐层空间到输出层变换是线性的。

![1548727955743](https://raw.githubusercontent.com/MaoChengEr/maochenger.github.io/master/imgs/1548727955743.png)

​	RBF的基本思想：用RBF作为隐单元的“基”构成隐含空间，这样就可以将输入矢量直接映射到隐空间，而不需要全连接。当RBF的中心点确定以后，这种映射关系也就确定了。

而隐含空间到输出空间的映射是线性的，即网络的输出是隐单元输出的线性加权和，此处的权即为网络可调参数。**其中，隐含层的作用是把此昂两从低纬度的p映射到高维度的h，这样低维度线性不可分的情到高维度就可以变得线性可分了，只要就是核函数的思想。**这样，网络由输出的映射是非线性的，而网络输出对可调参数而言又是线性的。网络的权就可由线性方程组直接解出，从而大大加快学习速度并避免局部极小问题。

径向基神经网络的激活函数可表示为：
$$
R(x_p-c_i)=\exp(-\frac{1}{2\sigma^2}||x_p-c_i||^2)
$$
其中$x_p$径向基神经网络的结构可得到网络的输出为：
$$
y_i=\sum_{i=1}^hw_{ij}\exp(-\frac{1}{2\sigma^2}||x_p-c_i||^2) j=1,2,...,n
$$
当然，采用最小二乘的损失函数表示：
$$
\sigma=\frac{1}{P}\sum_j^m||d_j-y_jc_i||^2
$$

#### 3.RBF神经网络与BP神经网络之间的区别

​	求解的参数有3个：基函数的中心、方差以及隐含层到输出层的权值。

##### 3.1自组织选取中心学习方法：

第一步：无监督学习过程，求解隐含层基函数的中心与方差

第二部：有监督学习过程，求解隐含层输出层之间的权值

首先，选取h个中心做k-means聚类，对于高斯核函数的径向基，当由由公式求解：
$$
\sigma_i=\frac{c_{max}}{\sqrt{2h}} i=1,2,...,h
$$
$c_{max}$为所选取中心点之间的最大距离。

​	隐含层直输出层之间的神经元的链接权值可以用最小二乘法直接计算得到，即对损失函数求解关于w的偏导数，使其等于0，可以简化得到计算公式为：
$$
w=\exp(\frac{h}{c_{max}^2}||x_p-c_i||^2) p=1,2,...,h
$$
##### 3.2直接计算法

​	隐含层神经元的中心是随机地在输入样本中选取，且中心固定。一旦固定下来，隐含层神经元的输出便是一直的，这样的神经网路的连接权就可以通过求解线性方程组来确定，适用于样本数据的分布有明显代表性。

##### 3.3有监督学习算法

​	通过训练样本集来获得满足监督要求的网络中心和其他权重参数，经历一个误差修正的过程，与BP网络的学习原理一样，同样采用梯度下降法。因此满足RBF童颜可以被当做BP神经网络的一种。

#### 4.RBF神经网络与BP神经网络的区别

##### 4.1局部逼近与全局逼近

​	BP神经网络的隐节点采用输入模式与权向量的内积作为激活函数的自变量，而激活函数采用sigmoid函数。各调参数对BP网络的输出具有同等地位的影响，因此BP神经网络使对非线性映射的全局逼近。

​	RBF神经网络的隐节点采用输去模式与中心向量的距离（如欧氏距离）作为函数的自变量，并使用径向基函数（如Gaussian）作为激活函数，神经元的输入离径向基函数中心点越远，神经元的激活函数程度就越低（高斯函数）。RBF网络的输出与部分调参数有关，譬如，一个$w_{ij}$值只影响一个$y_i$的输出，RBF神经网络因此具有“局部映射”的特性。

>全局逼近网络：当神经网络的一个或多个可调参数（权值和阈值）对任何一个输出都有影响，则称该神经网络为全局逼近网络。（学习速度慢，无法满足实时性要求的应用）
>
>局部逼近网络：对网路输入空间的某个举区域只有少数几个链接权影响网路的输出，则称改网络为局部逼近网络。（学习速度快，有可能满足有实时性要求的应用）

​	所谓**局部逼近**是指目标函数的逼近仅仅根据查询点附近的数据，而事实上，对于径向基网络，通常使用的是高斯径向基函数，函数图像是两边衰减且径向对称的，当选取的中心与查询点（即输入数据）很接近的时候才对输入有真正的映射作用，若中心与查询点很远的时候，欧氏距离太大的情况下，输出的结果趋近与0，所以真正起作用的点还是与查询点很近的点，所以是局部逼近；而BP网络对目标函数的逼近跟所有数据都相关，而不仅仅来自查询点附近的数据。

![1548730427917](https://raw.githubusercontent.com/MaoChengEr/maochenger.github.io/master/imgs/1548730427917.png)

##### 4.2中间层树的区别

​	BP神经网络可以有多个隐含层，但是RBF只有一个隐含层。

##### 4.3训练速度的区别

​	使用RBF的训练速度快，一方面是因为隐含层较少，另一方面，局部逼近可以简化计算量，对于一个输入x，只有部分神经元会无响应，其他的都近似为0，对应的w就不调参了。

##### 4.4Poggio和Girosi已经证明，RBF网络是连续函数的最佳逼近，而BP网络不是。

#### 5.RBF神经网络与SVM的区别

​	SVM等如果使用核函数的技巧的话，不太适应于大样本和大的特征的情况，因此提出了RBF。	
​	另外，SVM中的高斯核函数可以看做与每一个输入点的距离，而RBF神经网络对输入点做了一个聚类。RBF神经网络用高斯函数时，其数据中心C可以是训练样本中的抽样，此时SVM的高斯核函数是完全等价的，也可以是训练样本子集的多个聚类中心，所以他们都是需要选择数据中心的额，只不过SVM使用高斯核函数时，这里的数据中心都是训练样本本身而已。

RBF神经网络:
$$
G(X,X_p)=\exp(-\frac{1}{2\sigma^2}||X-X^p||^2)
$$
SVM:
$$
\kappa(x_1,x_2)=\exp(-\frac{||x_1-x_2||^2}{2\sigma^2})
$$

#### 6.为什么高斯核函数就是映射到高维区间

首先给出高斯核函数的定义公式：
$$
K(\overset{\rightarrow} x_i,\overset{\rightarrow} x_j)=e^{-\frac{||\overset{\rightarrow}x_i-\overset{\rightarrow}x_j||^2}{2\sigma^2}}
$$
实际上，可以简化为：
$$
K^{'}(\overset{\rightarrow}X_i,\overset{\rightarrow}X_j)=e^{-\frac{\overset{\rightarrow}X_i,\overset{\rightarrow}X_i}{\sigma^2}}
$$

$$
K^{'}(\overset{\rightarrow}X_i,\overset{\rightarrow}X_j)=\sum_{n=0}^{\infty}\frac{(\overset{\rightarrow}X_i,\overset{\rightarrow}X_j)}{\sigma^nn!}
$$

可以看到，其中X向量会生成类似多项式和展开的形式，譬如原来的参数有$x_1,x_2$,映射后，参数包含了$x_1*x_2,x_2*x_2$将原来2维映射到3维上了。

#### 7.前馈网络、递归网络和反馈网络

​	前馈网络一般只前馈神经网络或前馈型神经网络，它是一种最简单的神经网络，各种神经元分层排列，每个神经元值域前一层神经元相连，接受前一层的输出，并传输给下一层，各层间没有反馈。包括，BP神经网络，RBF神经网络等。

​	递归神经网络（RNN）是两种人工神经昂罗的总称。一种是时间递归神经网络，又名循环圣经网络，包括RNN,LSTM,GRU等；李毅中是结构神经递归神经网络。

​	反馈网络，又称自联想记忆网络，其目的是为了设计一个网络，存储一组平衡点，是的当给网络一组初始值是，网络通过自行运行而最终试炼得到这个设计的平衡点上，包括CHNN,DHNN等。

#### 8.完全内插法

​	之所以RBF能够拟合任意函数，可以从内插法的角度去理解没要㘝一个区县，我们可以通过内插法这个区县的表达函数，譬如：多项式插值、拉格朗日插值等。RBF插值是一系列精确插值方法的组合；即表面必须通过每一个测得的采样值。

​	RBF插值选择一个范数的近似函数或映射函数：
$$
\overset{\rightarrow}F(\overset{\rightarrow}x)=\sum_{i=1}^Nc_i\varphi(||\overset{\rightarrow} {x}-\overset{\rightarrow} {x_i}||)
$$
​	其中，近似函数$\overset{\rightarrow}{F}{(\overset{\rightarrow}x)}$代表 RBF的加权和，每一个RBF对应一个不同的中心$\overset{\rightarrow}{x_i}$,且权重由一个近似系数  $\overset{\rightarrow}{c_i} $可以利用最小二乘法线性方程组得到。
非线性函数$\varphi$就是RBF，它的参数依赖于n维空间的欧氏范数，如：
$$
||\overset{\rightarrow}{x}-\overset{\rightarrow}{x_i}||=\sqrt{\sum_{m=1}^n(x_m-x_{i,m})^2}
$$


​	对于RBF插值，其特点在于，在输入数据集中，与中心点距离近的点对映射函数的贡献最大。

​	完全内插法即要求所有插值点都经过曲面，由于RBF内插对于每个x都有用到，所以是一种完全没差的形式，存在的问题就是样本中包含噪音是，神经网络将拟合出一个错误的曲面，从而使泛化能力下降，另外，若样本x的数据远大于非线性函数$\varphi$，该求解变得不稳定，即为解超定方程。因此需要引入正则化方法，正则化方法及通常加上正则化项。
$$
E(F)=E_s(F)+\lambda E_c(F)=\frac{1}{2}\sum_{i=1}^N[d_i-F(X_i)]^2+\frac{1}{2}\lambda||DF||^2
$$


























