---
layout: post
comments: true
categories: 机器学习
---

#### 线性回归算法（有监督的算法）

~~~
有监督学习：
用一直某种或某些特性的样本作为训练集，建立一个数学模型，再用已经建立好的模型来预测未知样本。
无监督学习：
与监督学习比，无监督学习没有认为标注的结果，在飞监督学习过程中，书据并不被特别标识。
半监督学习：
利用少量的标注样本和大量的未标注样本进行训练和分类的问题，是有监督学习和无监督学习两者的结合。
~~~


$$
y^{i} = w^{i}*x^{i}+\varepsilon^{i}
$$

#### 最大似然估计以及二乘法

##### 似然函数：

$$
y^{(i)}=\theta^{T}*X^{(i)}+\varepsilon^{(i)}\\
p(\varepsilon^{(i)})=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{\varepsilon^{(i)^2}}{2\sigma^2}}\\
p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sigma\sqrt{2\pi}}exp\left\{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right\}\\
L(\theta)=\prod_{i=1}^m\frac{1}{\sigma\sqrt{2\pi}}exp^\left\{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right\}
$$

##### 最小二乘法最优解：

$$
l(\theta)=logL(\theta)=log\prod_{i=1}^m\frac{1}{\sigma\sqrt{2\pi}}exp^\left\{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right\}\\
=\sum_{i=1}^mlog\frac{1}{\sigma\sqrt{2\pi}}exp^\left\{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right\}\\
=mlog\frac{1}{\sigma\sqrt{2\pi}}-\frac{1}{\sigma^2}*\frac{1}{2}\sum_{i=1}^m\left(y^{(i)}-\theta^Tx^{(i)}\right)^2\\
loss(y_j,\hat{y}_j)=J(\theta)=\frac{1}{2}\sum_{i=1}^m\left(y^{(i)}-\theta^Tx^{(i)}\right)^2
$$

$\theta$的求解过程：
$$
J(\theta)=\frac{1}{2}\sum_{i=1}^m\left(y^{(i)}-\theta^Tx^{(i)}\right)^2=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)\\
J(\theta)=\bigtriangledown_{\theta}\left\{\frac{1}{2}(X\theta-Y)^T(X\theta-Y)\right\}=\bigtriangledown_{\theta}\left\{\frac{1}{2}(\theta^TX^T-Y^T)(X\theta-Y)\right\}\\
=\bigtriangledown_{\theta}\left\{\frac{1}{2}\left(\theta^TX^TX\theta-\theta^TX^TY-T^TX\theta+Y^TY\right)\right\}\\
=\frac{1}{2}\left(2X^TX\theta-X^TY-(Y^TX)^T\right)\\
=X^TX\theta-X^TY\Rightarrow \min_{\theta}J(\theta)
$$

$$
\theta = (X^TX)^{-1}X^TY
$$

注意：$X^TX$可逆

~~~
为了防止不可逆或者过拟合的问题存在，可以增加额外数据影响，导致最终的矩阵是可逆的。
~~~

$$
\theta=(X^TX+\lambda I)^{-1}X^Ty
$$



##### 欠拟合，过拟合

所谓的欠拟合就是模型在训练时候学习效果达不到预期要求

过拟合就是在模型训练时对包括某一个不太重要的特征学习效果太好，导致学习效果太好

#### 多项式回归算法

如果数据不符合线性规律而且更复杂，一种简单的解决办法就是讲每一维特征的幂次方添加为新的特征，在对所有的特征进行线性回归分析。这种方式就叫做多项式回归。

多项式扩展：属于增加维度的一种方式，通过这种方式可以将数据映射到高纬度空间变成线性可分的数据
功能：将低纬度空间上的数据通过都像是的组合，映射到高纬度空间中
效果：可以将低纬度的非线性的数据转换到高纬度空间中变成线性数据
方式：（sklearn中的多项式扩展的API的效果）

在以sklearn中的PolynomialFeatures类为例，当原始的特征为（a,b），次幂为3时，不仅仅会将$a^3,b^3$作为新的特征，还会添加$a^2b,ab^2,ab$，其中degree=d，将维度为n的原始特征转换为$\frac{(n+d)!}{d!n!}$，因此在使用多项式扩展特征的时候，必须注意特征维度爆炸的问题！

eg:

~~~
原始数据（2,3）；
   如果做最高次项为2的多项式转换：最终结果（1,2,3,4,6,9）
   如果做最高次项为3的多项式转换：最终结果（1,2,3,4,6,8,9,12,18,27）
   如果做最高次项为4的多项式转换：最终结果（1,2,3,4,6,8,9,12,16,18,24,27,36,54,81）
~~~

模型参数：

在模型训练数据集上通过某种给定的方式找出来的模型参数，也就是说这个模型参数的求解就是我们经常所说的模型学习。

超参：

在模型训练中需要使用到的参数值，但是给参数值需要开发人员给定的。eg：Ridge API中的alpha...

给定超参的方式：

1.可以根据算法的特性、业务背景以及经验，类给定一个比较合适的值

2.通过sklearn提供的交叉验证的方式来选择最优的参数

3.通过网格交叉验证的当时选择最优参数

#### 正则化(L1正则，L2正则)：

目的：为了防止过拟合。

L1正则又叫做Lasso回归，L2正则有叫做岭回归。
$$
J(\theta)=\frac{1}{2}\sum\left(h_{\theta}(x^{(i)}-y^{(i)})\right)+\lambda\sum_{i=1}^n\theta_j^2 (\lambda >0) -L2正则\\

J(\theta)=\frac{1}{2}\sum\left(h_{\theta}(x^{(i)}-y^{(i)})\right)+\lambda\sum_{i=1}^n\theta_j^2 (\lambda >0) -L1正则
$$
L1和L2正则比较:

L2-Norm中，对于各个维度的参数都是在一个圆内缩放的，不可能导致有维度参数为零的情况，就不会产生稀疏解；

~~~
稀疏就是训练出来得到的模型参数中有很多参数值都是0,；稀疏解的主要作用就是用于特征选择，因为参数为零所对应的特征相当于没有决策能力，步会影响y的取值，既然这样，我们可以将这些为0对应的特征删除。
~~~

Lasso算法可以让数据去掉噪音或者冗余特征。Ridge模型具有较高的准确性、鲁棒性以及稳定性（前提数据中不存在冗余特征的时候）；Lasso模型具有较高的求解速度。如果纪要考虑稳定性也考虑求解的速度，就使用Elastic Net。

##### Elastic Net(弹性网络)：

$$
J(\theta) = \frac{1}{2}\sum_{i=1}^m\left(h_{\theta}x^{(i)}-y^{(i)}\right)+\lambda\left\{p\sum_{j=1}^n|\theta_j|+(1-p)\sum_{j=1}^n\theta_j^2\right\}
$$

##### 模型效果判断：

$$
MSE = \frac{1}{m}\sum_{i=1}^m\left(y_i-\hat{y_j}\right)^2
$$

$$
RMSE=\sqrt{MSE}=\sqrt{\frac{1}{m}\sum_{i=1}^m\left(y_i-\hat{y_j}\right)^2}
$$

$$
R^2=1-\frac{RSS}{TSS}=1-\frac{\sum_{i=1}^m\left(y_i-\hat{y_j}\right)^2}{\sum_{i=1}^m\left(y_i-\bar{y_j}\right)^2}
$$

##### 模型效果判断:

MSE:误差平方和，月趋近于0表示模型月拟合训练数据。

RMSE:MSE的平方根，作用同MSE。

$R^2$:取值范围为$(负无穷，1]$，值越大表示模型月拟合训练数据；最优解是1；当模型预测为随机值得时候，有可能为负；若预测值恒为期望，$R^2$为0；

TSS:总平方和TSS，表示样本之间的差异情况，是伪方差的m倍。

RSS:残差平方和RSS，表示预测值和样本值之间的差异情况，是MSE的m倍。



#### 梯度下降：（批量梯度下降，随机梯度下降，小批量梯度下降）

##### 梯度下降算法:

目标函数$\theta$求解:$J(\theta)=\frac{1}{2}\sum_{i=1}^m\left(h_{\theta}(x^{(i)}-y^{(i)})\right)^2$

初始化$\theta$(随机初始化，可以初始为0)

沿着扶梯度方向迭代，更新后的$\theta$使$J(\theta)$更小
$$
\theta = \theta - \alpha*\frac{\partial J(\theta)}{\partial \theta}
$$
$\alpha$是学习率，步长

##### BGD和SGD比较：

随机梯度下降（SGD）迭代次数少，运行速度更快。

SGD在某一些情况下（全局存在多个相对最优解，或者说数据中存在异常数据的情况），SGD有可能会跳出某些小的局部最优解，所以不会比BGD坏（样本数据集中正常样本居多）；SGD在收敛的位置会存在$J(\theta)$函数波动的情况。

BGD一定能够得到局部最优解（在线性回归模型中一定是得到一个全局的最优解），SGD由于随机性的存在可能会导致最终的结果比SGD差（样本数据集中噪音样本居多）



#### Logistic回归：

$Logistic/sigmoid函数 :p=h_{\theta}(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$

令$g(z)=\frac{1}{1+e^{-z}}$
$$
g'(z)=\left\{\frac{1}{1+e^{-z}}\right\}'=\frac{e^{-z}}{(1+e^{-z})^2}\\
=\frac{1}{1+e^{-z}}*\frac{e^{-z}}{1+e^{-z}}=\frac{1}{1+e^{-z}}*\left\{1-\frac{1}{1+e^{-z}}\right\}\\
=g(z)*(1-g(z))
$$

##### Logistic回归及似然函数：

假设:
$$
P(y=1|x;\theta)=h_{\theta}(x)\\
P(y=0|x;\theta)=1-h_{\theta}(x)\\
p(y|x;\theta)=(h_{\theta}(x))^y(1-h_{\theta}(x))^{(1-y)}
$$

|          | y=1      | y=0        |
| -------- | -------- | ---------- |
| $p(y|x)$ | $\theta$ | $1-\theta$ |

似然函数：
$$
L(\theta)=p(\vec{y}|X;\theta)=\prod_{i=1}^mp\left(y^{(i)}|x^{(i)};\theta\right)\\
=\prod_{i=1}^m\left(h_{\theta}(x^{(i)})\right)^{y^{(i)}}\left(1-h_{\theta}(x^{(i)})\right)^{(1-y^{(i)})}
$$
对数似然函数：
$$
l(\theta)=logL(\theta)=\sum_{i=1}^m\left(y^{(i)}logh_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right)
$$

##### 最大似然/极大似然函数的随机梯度

$$
\frac{\part l(\theta)}{\part\theta}=\sum_{i=1}^m\left\{\frac{y^{(i)}}{h_{\theta}x^{(i)}}-\frac{1-y^{(i)}}{1-h_{\theta}(x^{(i)})}\right\}*\frac{\part h_{\theta}x^{(i)}}{\part\theta_j}\\
=\sum_{i=1}^m\left\{\frac{y^{(i)}}{a(\theta^Tx^{(i)})}-\frac{1-y^{(i)}}{1-h_{\theta}(x^{(i)})}\right\}\frac{\part h_{\theta}x^{(i)}}{\part\theta_j}\\
=\sum_{i=1}^m\left\{\frac{y^{(i)}}{a(\theta^Tx^{(i)})}-\frac{1-y^{(i)}}{1-h_{\theta}(x^{(i)})}\right\}*g\left(\theta^Tx^{(i)}\right)\left(1-g(\theta^Tx^{(i)})\right)*\frac{\part\theta x^{(i)}}{\part\theta_j}\\
=\sum_{i=1}^m\left(y^{(i)}\left(1-g\left(\theta^Tx^{(i)}\right)\right)-\left(1-y^{(i)}\right)g\left(\theta x^{(i)}\right)\right)*x_j^{(i)}=\sum_{i=1}^m\left(y^{(i)}-g\left(\theta^tX^{(i)}\right)\right)*x_j^{(i)}
$$

##### 极大似然估计与Logistic回归损失函数

$$
L(\theta)=\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^mp_i^{y^{(i)}}(1-p_i)^{1-y^{(i)}}\\
p_i = h_{\theta}(x^{(i)})=\frac{1}{1+e^{-\theta^Tx^{(i)}}}\\
loss = -l(\theta)=-\sum_{i=1}^m\left[y^{(i)}ln(p_i)+(1-y^{(i)})ln(1-p_i)\right]\\
=\sum_{i=1}^m\left[-y^{(i)}ln(h_{\theta}(x^{(i)}))-(1-y^{(i)})ln(1-h_{\theta}(x^{(i)}))\right]
$$

$\theta$的参数求解：
$$
\theta_j=\theta_j+\alpha\sum_{i=1}^m(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}\\
\theta_j=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}
$$

#### Sofatmax回归：

softmax回归是Logistic回归的一般化，适用于K分类的问题，第K类的参数为向量$\theta_k$,组成的二维矩阵为$\theta_{k*n}$;

sofemax函数的本质就是讲一个K维的文艺实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0,1）之间。

softmax回归概率函数：
$$
p(y=k|x;\theta)=\frac{e^{\theta_k^T*X}}{\sum_{l=1}^{K}*e^{\theta_l^Tx}},k=1,2,...,K
$$
概率：用于在已知一些参数情况下，预测接下来的观测所得到结果，而似然性正好相反则是用于已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

##### softmax算法原理：

$$
p(y=k|x;\theta)=\frac{e^{\theta_k^Tx}}{\sum_{l=1}^Ke^{\theta_l^T}x},k=1,2,...,K\\
h_{\theta}=\begin{bmatrix}
 p(y^{(i)}=1|x^{(i)};\theta)\\ 
 p(y^{(i)}=1|x^{(i)};\theta \\ 
 ...\\ 
 p(y^{(i)}=1|x^{(i)};\theta
\end{bmatrix}=\frac{1}{\sum_{j=1}^ke^{\theta^T_jx^{(i)}}}\begin{bmatrix}
e^{\theta_1^TX}\\
e^{\theta_2^TX}\\
...\\
e^{\theta_K^TX}
\end{bmatrix}\Rightarrow\theta=\begin{bmatrix}
\theta_{11}\quad  \theta_{12}\quad ...\quad\theta_{1n}\\
\theta_{21}\quad  \theta_{22}\quad ...\quad\theta_{2n}\\
...\quad ...\quad ...\quad ...\\
\theta_{k1}\quad\theta_{k2}\quad...\quad\theta_{kn}
\end{bmatrix}
$$



##### softmax算法损失函数

$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^m\sum_{j=1}^kI(y^{(i)}=j)ln\left(\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}}\right)\\
I(y^{(i)}=j)=\left\{\begin{matrix}
1,y^{(i)}=j\\ 
0,y^{(i)]}\neq j
\end{matrix}\right.
$$

##### softmax算法梯度下降求解

$$
\frac{\part}{\part\theta_j}J(\theta)=\frac{\part}{\part\theta}-I(y^{(i)}=j)ln\left\{\frac{e^{\theta_j^TX^{(i)}}}{\sum_{l=1}^ke^{\theta^Tx^{(i)}}}\right\}\\
=\frac{\part}{\part\theta_j}-I(y^{(i)}=j)(\theta^Tx^{(i)}-ln(\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}))\\
=-I(y^{(i)}=j)\left\{1-\frac{e^{\theta^Tx^{(i)}}}{\sum_{l=1}^k\theta_l^Tx^{(i)}}\right\}x^{(i)}
$$

$$
\frac{\part}{\part\theta_j}J(\theta)=-I(y^{(i)}=j)\left\{1-\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{i=1}^ke^{\theta_l^Tx^{(i)}}}\right\}x^{(i)}\\
\theta_j=\theta_j+\alpha\sum_{i=1}^mI(y^{(i)}=j)(1-p(y^{(i)}=j|x^{(i)};\theta))x^{(i)}\\
\theta_j=\theta_j+\alpha I(y^{(i)}=j)(1-P(Y^{(i)}=j|x^{(i)};\theta))x^{(i)}
$$

#### Logistic和softmax比较

两者都是属于分了问题。

求$\theta$的方式都是梯度下降的算法，梯度下降的算法是参数优化的重要手段，主要是SGD，适用于在线学习以及跳出局部极小值。

Logistic/Softmax回归是实践中解决问题的最重要的方法。

广义线性模型对样本要求不必要服从正态分布，只需要服从指数分布簇（二项式分布、泊松分布、伯努利分布、指数分布等）；广义线性模型的自变量可以是连续的也可以是离散的。

