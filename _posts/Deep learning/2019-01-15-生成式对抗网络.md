---
layout: post
comments: true
categories: 深度学习
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

* content
{:toc}
#### 1.什么是生成式对抗网络

生成式对抗网络(GAN)是一种深度学习的模型，是近年来复杂分布上无监督学习最具前景的方法之一。模型通过框架中(至少)两个模块；生成模型和判别模型的互相博弈学习产生相当好的输出。原始GAN理论中，并不要求G和N都是神经网络，值需要时拟合相应生成和判别的函数即可。但实用中一般均使用深度神经网络作为G和D。补货数据分布的生成模型G，和估计样本来自训练数据的概率的判别模型D.G的训练程序是将D错误的概率最大化。

机器学习可以分为两大类，生成模型和判别模型。判别模型需要输入变量，通过某种模型来预测。生成模型是给定某种隐含信息，来自随机产生观测数据。

生成式对抗网络将机器学习中两大类数据，生成模型和判别模型给紧密地联合在了一起。

假设我们有两个网络，G和D。它们的功能分别是：

G是一个生成图片的网络，它接受一个随机的噪声z，通过这个噪声生成图片，记作G(z)。

D是一个判别网络，判别一张图片是不是“real”。它的输入参数是x，x代表的是一张图。输出D(x)
代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。

在训练过程中，生成网络G的目标就是尽量生成正式的图片去欺骗判别网络D。而D的目标就是尽量把G生成的图片和真实的图片分别开来。这样，G和D构成了一个动态的“博弈过程”。

最后博弈的结果是什么?在最理想的状态下，G可以生成足以“以假乱真”的图片G(z)。对于D来说，它难以判定G生成的图片究竟是不是真实的，因此D(G(z))=0.5。

#### 2.生成式对抗网路的应用场景

图像生成、超分别率

语义分割

文字生成

数据增强

聊天机器人

信息检索、排序

#### 3.生成式对抗网络

##### 3.1GAN是什么，怎么完成对抗的？

生成式对抗网络包含一个生成米O型和一个判别模型。

主要解决的问题时shipping如何从训练样本中学习出新的样本。

生成模型就是负责训练出样本的分布，判别模型是一个二分类学器，用来判断输入样本时真实数据还是训练生成的数据。

##### 3.2优势

GAN是更好的生成模型，在某种意义上避免了马尔科夫链式的学习机制，这使得他能够区别于传统的概率生成模型。传统概率生成模型一般都需要进行马尔科夫链式的采样和推断，而GAN避免这个计算复杂度特别高的过程没直接进行采样和推断，从而提高了GAN的应用效率，多以其实际应用场景也就更为广泛。

其次GAN是一个非常灵活的设计框架，各种类型的损失函数都可以整合到GAN模型当中，这样使得针对不同的任务，我们可以设计不同类型的损失函数，都会在GAN的框架下进行学习和优化。

再次，最重要的一点，当概率密度不可计算的时候，传统依赖于数据自然性解释的一些生成模型就不可以在上面进行学习和应用。但是GAN在这种情况下依然可以使用，这是因为GAN引入了一个非常聪明的内部对抗的训练机制，可以逼近一些不是很容易计算的目标函数。

#### 4.基本框架

一个朴素的GAN模型，实际上是将一个随机变量（可以是高斯分布，或0到1之间的均匀分布），通过参数化的概率生成模型（通常是用一个神经网络模型来进行参数化），进行概率分布的逆变换采样，从而得到一个生成的概率分布。

GAN的或者一般概率生成模型的训练目的就是要是的生成的概率分布和真实数据的分布尽量接近，从而能够理解真实的数据。但是在实际应用中，我们完全没有办法知道真实数据的分布。我们所能够得到的是从这个真实的数据分布中所采样得到的一些真实数据。

##### 4.1怎么定义损失

传统的生成模型，一般采用数据的似然性来作为优化的目标，但GAN创新性的使用了另一种优化目标。

首先，它引入了一个判别模型（常用的有支持向量机和多层神经网络）。

其次，它的优化过程就是在寻找生成模型和判别模型之间的一个纳什均衡。

GAN所建立的一个学习框架，实际上就是生成模型和判别模型之间一个模仿游戏。生成模型的目的，就是尽量去模仿。建模和学习真实数据的分布规律；而判别模型则是需要判别子集所得到的一个输入数据，究竟是来自于真实的数据分布还是来自于一个生成模型。通过这两个内部模型之间不断的竞争，从而提高两个模型的生成能力和判别能力。

##### 4.2详细实现过程

假设我们现在的数据集是手写体数字的数据集minst。

舒适化生成模型G、判别模型D（假设生成模型是一个简单的RBF，判别模型是一个简单的全连接网络，后面连接一层softmax）这些都是假设，对抗网络的生成模型和判别模型没有任何限制。

![1548644973886](..\..\imgs\1548644973886.png)

##### 4.3例子与训练

假设一种概率分布M，它相对于我们是一个黑盒子。为了了解这个黑盒子中的东西是什么，我们构建了两个东西G和D，G是另一种我们完全知道的概率分布，D是用来区分一个时间是由黑盒子中那个不知道的东西产生的还是由我们自己产生的。

不断的调整G和D，直到D不能把事件区分出来为止。在调整过程中，需要：

优化G，使得他尽可能的让D混淆。

优化D，使得它尽可能的能区分出假冒的东西。

当D无法区分出事件的来源的时候，可以认为，G和M是一样的。从而，我们就能了解到黑盒子中的东西。

![1548645297838](..\..\imgs\1548645297838.png)

图a,b,c,d.黑色的点状线代表M所产生的一些数据，绿色的线代表我们自己模拟的分布G，蓝色代表的线代表着分类模型D。

a如表示初始状态，b图表示，保持G不动，优化D，直到分类的准确率最高。

c图表示保持，D不动，优化G，直到混淆度最高。d图表示，多次迭代后，终于使得G能够完全拟合M产生的数据,从而认为，G就是M。

#### 4.博弈

生成式对抗网络的优化是一个二元极小极大博弈问题，它的目的是使生成模型的输出在输入给判别模型时，判别模型很难判断是真实数据还是虚假数据。

训练好的生成模型，有能力把一个噪音向量转化成和训练类似的样本。

#### 5.极大极小值算法

MiniMax算法（极大极小算法）是一种找出失败的最大可能性中最小值的算法（即最小化对手的最大得益），该算法通常是通过递归的形式来实现的；MinMax算法常用于棋类等两个相仿较量的游戏或者程序中。

该算法是一个量总和算法，即一方要在可选的选项中将其优势最大化的选择，另一方则选择令对手优势最小化的一个，其输赢的综合为0（优点像能量守恒，就像本身两个玩家都有1点，最后输家将它的一点给赢家，但整体航还是总和2点）。

由于是递归的操作，所以层析深度会非常深，那么可能使用神经网络优化

#### 6.纳什均衡

纳什均衡是指博弈中这样的局面，对于每个参与者来说，只要其他人不改变策略，它就无法改善自己的状况。

纳什证明了在每个参与者都只有有限种策略选择并允许混合策略的前提下，纳什均衡定存在。

以两家公司的价格大战为例，价格大战存在着两败俱伤的可能，在对方不改变价格的条件下既不能提价，否则会进一步丧失市场；也不能降价，因为会出现赔本买卖。于是两家公司可以改变原先的利益格局，通过谈判寻求新的利益评估分摊方案。

相互作用的经济体假定其他主题所选择的战略为既定的，选择自己的最优战略的状态，也就是纳什均衡。

##### 6.1纳什均衡分类

纯战略纳什均衡是提供个玩家要如何进行赛局的一个完整的定义，特别的是，纯战略决定在任何一种情况下要做的移动。站虐集合是由玩家能够施行的纯战略所组成的集合。而混合战略是对每个站虐分配一个几率而形成的战略。

混合战略纳什均衡是允许玩家随机选择一个纯战略。混合战略博弈聚恒中要用到概率计算，因为每一种策略都是随机的，达到某一概率时，可以实现支付最优。因为几率是连续的，所以即使战略集合是有限的，也会有无线多个混合战略。

##### 6.2纳什均衡经典哲学案例--囚徒困境

假设有两个小偷A和B联合犯事、私入民宅被警察抓住。警方将两个人分别置于不同的两个房间内进行审讯，对每一个犯罪嫌疑人，警方给出的政策是：如果一个犯罪嫌疑人坦白了罪行，交出了赃物，于是证据确凿，两个人都被判有罪。如果另一个犯罪嫌疑人也做了坦白。则两个人各判刑8年；如果另一个犯罪嫌疑人没有坦白而是抵赖，则以妨碍公务罪（因已有证据表明其罪证）在加刑两年，而坦白这有功被减刑8年，立即释放。如果两人都抵赖，则警方因证据不足不能判断两人的偷窃罪，但可以私入民宅的罪名将两个人判入狱一年。

#### 7.前向传播阶段

可以有两种输入：

1、我们随机产生一个随机向量作为生成模型的数据，然后经过生成模型后产生一个新的向量，作为Fake Image，记作D(z)。

将1或2产出的输出，作为判别昂罗的输入，经过判别网络后输入值为一个0或1之间的树，用于表示输入图片为Real Image的概率，real为1，fake为0。

##### 7.1判别模型的损失函数

当输入的是从数据集中去除的real Image数据时，我们只需要考虑第二部分，D(x)为判别模型的输出，表示输入x为real数据的概率，我们的目的就是让判别模型的输出D(x)的输出尽量靠近1。

当输入的为fake数据时，我们只计算第一部分，G(z)是生成模型的输出，输出的是一张Fake Image。我们要做的是让D(G(z))的输出尽可能趋向于0.这样才能表示判别模型是有区分力的。

相对判别模型来说，这个损失函数其实就是交叉熵损失函数。计算loss，进行梯度反转。这里的梯度反转可以使用任何一张梯度修正的方法。当更新完判别模型的参数后，我们再去更新生成的模型的参数。
$$
-((1-y)\log(1-D(G(z)))+y\log D(x))
$$

##### 7.2生成模型的损失函数

对于生成的模型拉私活，我们要做的就是让G(z)产生的数据尽可能的和数据集中的数据一样。

就是所谓的同样的数据分布。那么我们要做的就是最小化生成模型的误差，即只讲由G(z)产生的误差传给生成模型。

但是针对判别式模型的预测结果，要对梯度变化的方向进行改变。当判别模型认为G(z)输出为真实数据集的时候和认为输出噪音数据的时候，梯度更新方向要进行改变。

其中$\bar D$表示判别模型的也测类型，对预测概率取整，为0或者为1.用于更改梯度方向，阈值可以自己设置，或者正常的话就是0.5。
$$
(1-y)\log (1-D(G(z)))(2*\bar D(g(z))-1)
$$

##### 7.3判别模型的目标函数

用数学语言描述整个博弈过程的话，就是：假设我们生成模型是g(z)，其中z是一个随机噪声，而g将这个随机噪声转化为数据类型x,仍拿图片问题举例，这里g的输出就是一张图片。D就是一个判别模型，对任何输入x，D(x)的输出是0-1范围内的一个实数，用来判断这个图片是一个是真实图片的概率有多大。令Pr和Pg分别代表真实图像的分布与生成图像的分布。
$$
\underset{D}max E_{x\sim P_r}[\log D(x)]+E_{x\sim P_g}[\log(1-D(x))]
$$

##### 7.4整体目标函数

将上述例子所描述的过程公式化买，得到如上的公式。公式D(x)表示属于分布M的概率，因而，优化D的时候就是让V(D,G)最大，优化G的时候就是让V(D,G)最小。其中，$x\sim p_{data}(x)$表示x取自真正的分布。z~pz(z)表示z取自我们模拟的分布。G表示生成模型,D表示分类模型。
$$
\underset{G}max\underset{D}v(D,G)=E_{x\sim P_{data}}[\log D(x)]+E_{z\sim p_x(z)}[\log(1-D(G(z)))]
$$
在我们的函数V(D,G)中，第一项是来自实际分布(pdata(x))的数据通过鉴别器（也称为最佳情况）的熵。鉴别器试图将其最大化为1.第二项是来自随机输入(p(z))的数据通过发生器的熵。生成器产生一个假的样本，通过家别气识别虚假（也称为最坏的情况）。在这一项中，鉴别器尝试将其最大化为0（即生成的数据是伪造的概率的对数为0）。所以总体而言，鉴别器正在尝试最大化函数V(D,G)。

另一方面，生成器的任务完全相反，它试图最小化函数V(D,G),使正式数据和假数据之间的区别最小化。这就是说，生成器和鉴别器项在玩猫和老鼠的游戏。
$$
\underset{G}max\underset{D}v(D,G)=E_{x\sim P_{data}}[\log D(x)]+E_{z\sim p_x(z)}[\log(1-D(G(z)))]
$$

#### 8.训练数据

##### 8.1训练细节

第一阶段：训练鉴别器，冻结生成器（冻结意思是不训练，神经网络只向前传播，不进行Backpropagation反向传播）

第二阶段：训练生成器，冻结鉴别器

##### 8.2训练步骤

第一步：定义问题。你想生成假的图像还是文字？你需要完全定义问题并收集数据。

第二步：定义GAN的架构。GAN看起来是怎么样的，生成器和鉴别器应该是多层感知器还是卷积神经网络？这一步取决于你要解决的问题。

第三步：用真实数据训练鉴别器N个epcho。训练鉴别器正确预测真实数据威震。这里N可以设置为1到无穷大之间的任意的自然数。

第四步：用生成器产生的输入数据，用来训练鉴别器。训练鉴别器正确预测假的数据为假。

第五步：用鉴别器的出入训练生成器。当鉴别器被训练后，将其预测值作为标记来训练生成器。训练生成器来迷惑鉴别器。

第六步：重复三到五不多个epcho

第七步：手动检查假数据是否合理。如果看起来合适就停止训练，否则回到第三步。这是一个手动任务，手动评估数据是检查其假冒程度的最佳方式，当这个步骤结束时，就可以评估GAN是否表现良好。

##### 8.3noise输入

假设我们现在的数据集是一个二维的高斯混合模型，那么这个noise就是x轴上我们随机陈输入的点，经过生成模型映射可以将x轴上的点映射到高斯混合模型上的点。当我们的数据集是图片的时候，那么我们输入的随机噪声其实就是相当于低维度的数据，经过生成模型D的映射就变成了一张生成的图片G(x)。

最终两个模型达到了稳态的时候判别模型D的输出接近1/2，也就是说判别器很难判断出图片是真是假，这也就说明了网络是会达到收敛的。

#### 9.GANS总结

![1548658162665](..\..\imgs\1548658162665.png)

$$
\underset{G}min\left\{\underset{D}maxV(D,G)=E_{x\sim p_{data}(x)}\left[\log D(x)\right]+E_{z\sim p_{z}}(z)[\log(1-D(G(z)))]\right\}
$$


图中上半部分是GAN模型的基本构架。我们先从一个简单的分布中采样一个噪音信号z（实际中可以采用【0,1】的均匀分布或者是标准正太分布），然后经过一个生成函数后映射为我们想要的数据分布$X_G​$（z和X都是向量）。生成的数据和真实数据都会输入一个识别网络的D。识别网络通过判别，输出一个标量，表示数据来自真实数据的概率。

在实现上，G和D都是可微分函数，都可以采用多层神经网络实现，因此上面的整个模型的参数就可以利用backpropagation来训练得到。

图中下半部分是模型训练中的目标函数，仔细看可以发现这个公式很想cross entropy，注意D是P(Xdata)的近似。对于D而言尽量是公式最大化（是别呢力强），而对于G又想使之最小（生成的数据接近实际数据）。

整个训练是一个迭代过程，但是在迭代中，对D的迭代又是内循环。所以每次迭代，D先训练K次，G训练一次。

##### 9.1优势和劣势

优势：

马尔科夫链不需要，只需要向后传播就可以了。

申城网络不需要直接采用样本来更新了，这是一个可能存在的优势。

对抗神经网络表达能力更强劲，而基于Markov链的模型需要分布比较模糊才能在不同的内模式之间呼和。

劣势：

对于生成模型，没有直接的表达，而是由一些参数控制。

D需要和G同步的很好才可以。



















































