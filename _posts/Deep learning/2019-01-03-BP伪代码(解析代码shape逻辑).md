---
layout: post
comments: true
categories: 深度学习
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

### BP伪代码(解析代码shape逻辑)

![1547780443168](..\..\imgs\1547780443168.png)

假设$input$输入层的数据的形状为[3,20]，$w1$权重为[8,3]，$output$输出层的数据的形状为[2,20],$w2​$的权重为[2,8],学习率为learning_rate=0.025

首先做数据的标准化：

intput_norm=max_min_norm(input)=[3,20]

output_norm=max_min_norm(output)=[2,20]

bp_net_forward(前传)：这里从数据的shape去考虑

>1.输入层到隐藏层：
>
>hidden_net=w1*intput_norm=[8,3]\*[3,20]=[8,20]
>
>hidden_out=sigmoid(hidden_net)=[8,20]
>
>2.隐藏层到输出层：
>
>output_net=w2\*hidden_out=[2,8]\*[8,20]=[2,20]
>
>output_out=sigmoid(output_net)=[2,20]

bp_net_backward(后传)：

>1.隐藏层到输出层的损失
>
>err=output_norm-output_out=[2,20]
>
>delta2=sum(err**2)=[2,20]
>
>2.输入层到隐层损失
>
>delta1=w2.T*delta2=[8,2]\*[2,20]=[8,20]
>
>dw2=delta2*hidden_out.T=[2,20]\*[20,8]=[2,8]
>
>db2=delta2*[20,1]=[8,20]\*[20,1]=[8,1]
>
>dw1=delta1*input_norm.T=[8,20]\*[20,3]=[8,3]
>
>db1=delta1\*[20,1]=[8,20]\*[20,1]=[8,1]

更新权重：

w2+=learning_rate*dw2

b1+=learning_rate*db2

w1+=learning_rate*dw1

db1=learning_rate*db1

























