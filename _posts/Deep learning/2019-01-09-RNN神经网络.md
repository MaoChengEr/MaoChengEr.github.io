---
layout: post
comments: true
categories: 深度学习
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

* content
{:toc}
#### 什么是神经递归网络

为什么有BP神经网络、CNN、还需要RNN?

​	BP神经网络和CNN输入输出都是相互独立的；但是实际应用中有些场景输出内容和之前的内容是有关联的。

​	RNN引入“记忆”的概念；循环指其每一个元素都执行相同的任务，但是依赖于输入和“记忆”。



BP神经网络和卷积神经网络，这两种结构有一个特点，就是假设输入的是一个独立的没有上下文联系的单位，比如输入的是一个图片，网络识别是狗还是猫。

但是对于一些有明显的上下文特征的序列化输入，比如预测视频中下一帧的播放内容，那么很明显这样的输出必须依赖以前的输入，也就是说网络必须有一定的“记忆能力”。

为了赋予网络这样的记忆力，一种特殊结构的神经网络--递归神经网络便应运而生。

#### 神经递归RNN结构

将序列按时间展开就可以得到RNN的结构

$X_t$是时间的输入
$S_t$是时间t出的“记忆”，$S_t=f(Ux_t+WS_{t-1}),f$可以是非线性转换的函数，比如$tanh​$等

$O_t$是时间t处的输出，比如是预测下一个词的话，可能是$softmax$输出的属于每个候选词的概率，

$O_t=softmax(VS_t)​$

![1547614635938](..\..\imgs\1547614635938.png)

按照一定的时间序列规定好计算顺序，于是实际上我们会将这样的代还的结构展开一个序列网络，也就是上图右侧被“unflod”之后的结构。

![1547614652237](..\..\imgs\1547614652237.png)

网络某一时刻的输入$x_t$,和之前接胡搜啊的bp神经网络的输入一样，$x_t$是一个n维向量，不同的是递归网络的输入将是一整个序列，也就是$x=[x_1,...,x_{t-1},x_t,x_{t+1},...,x_T]$,对于语言模型，每一个$x_t​$将嗲表一个词向量，一整个序列就代表一句话。

$h_t​$代表时刻t的隐藏状态

$o_t​$代表时刻t的输出

输入层到隐藏层直接的权重由U表示，它将我们的原始输入进行抽象作为隐藏层的输入

隐藏层到隐藏层的权重W，它是网络记忆控制者，负责调度记忆。

隐藏层到输出层的权重V，从隐藏层学习到的表示将通过它再一次抽象，并作为最终的输出。

#### 神经递归网络RNN正向传播阶段

在t-=0的时刻，U,V,W都被堆积初始化好,$h_0​$通常初始化为0，然后进行如下计算：
$$
s_1=Ux_1+Wh_0\\
h_1=f(s_1)\\
o_1=g(Vh_1)
$$
时间就向前推进，此时的状态$h_1​$作为时刻0的记忆状态将参与下一次的预测活动，也就是：
$$
s_2=Ux_2+Wh_1\\
h2=f(s_2)\\
o_2=g(Vh_2)
$$
以此类推，可得
$$
s_t=Ux_1+Wh_{t-1}\\
h_t=f(Ux_t+Wh_{t-1})\\
o_t=g(Vh_t)
$$
其中f可以是$tanh$，reluctant，logistic等激活函数，g通常是$softmax​$也可以是其他。

可以这么理解：h=f(现有的输入+过去的记忆总结)

#### 递归神经网络RNN反向传播阶段

bp神经网络用到的误差反向传播方法加工输出层的误差综合，对各个权重的梯度$\bigtriangledown U​$,$\bigtriangledown ​$V,$\bigtriangledown W​$，求偏导数，然后利用梯度下降法更新各个权重。

对于每一个时刻t的$RNN$网络，网路的输出$o_t$都会产生一个误差$e_t$,误差的损失函数，可以是cross entropy也可以是平方误差等等。那么总的误差为$E=\sum te_t​$,我们的目标就是要求取
$$
\bigtriangledown U=\frac{\partial E}{\partial U}=\frac{\partial e_t}{\partial U}\\
\bigtriangledown V=\frac{\partial E}{\partial V}=\frac{\partial e_t}{\partial V}\\
\bigtriangledown W=\frac{\partial E}{\partial W}=\frac{\partial e_t}{\partial W}\\
$$
对于输出$o_t=g(Vs_t)$,对于任意损失函数，求取$\bigtriangledown V$的将是简单的，我们可以直接求取每个时刻的$\partial e_t/\partial V​$,由于它不存在和之前的状态依赖，可以直接求导数取得，然后简单的求和即可。

对于$\bigtriangledown W,\bigtriangledown U​$的计算不能直接求导，因此需要链式求导法则。

为了使误差e能够对U和W求偏导数，定义一个$\delta=\partial e/\partial s$,首先计算出输出层的$\delta L​$,在向后传播到各个层。

关注当前层次发射出去的链接即可，也就是：
$$
\delta_t^h=(V^T\delta_t^o+W^T\delta_{t+1}^h)*f^{'}(s_t)
$$
![1547616768576](..\..\imgs\1547616768576.png)

只要计算出所有的$\delta o_t,\delta h_t​$,就可以通过以下计算出$\bigtriangledown W,\bigtriangledown U​$：

举个详细的例子计算W梯度的值：

![1547703643442](..\..\imgs\1547703643442.png)

​	举个例子时刻$t+1​$产生的误差$e_{t+1}​$，我们想计算它对于$W_1,W_2,...,W_t,W_{t+1}​$的梯度，棵以如下计算：
$$
\frac{\partial e_{t+1}}{\partial W^{t+1}}=\frac{\partial e_{t+1}}{\partial h_{t+1}}\frac{\partial h_{t+1}}{\partial W^{t+1}}\\
\frac{\partial e_{t+1}}{\partial W^{t}}=\frac{\partial e_{t+1}}{\partial h^{t+1}}\frac{\partial h_{t+1}}{\partial h^t}\frac{\partial h_t}{\partial W^t}\\
\frac{\partial e_{t+1}}{\partial W^{t-1}}=\frac{\partial e_{t+1}}{\partial h^{t+1}}\frac{\partial h_{t+1}}{}
$$





















