---
layout: post
comments: true
categories: 深度学习
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

* content
{:toc}
### 深度学习笔记

#### 1.深度学习概述 

CNN基础+案例

RNN基础+案例

GAN基础+案例

强化学习

人脸识别

#### 2.静态图和动态图：

(1)提前建立好计算的顺序和逻辑，然后写入到内存中，实际计算的时候，直接传入数据，相当于if~else形式处理。

动态图：计算顺序和逻辑也是写好的，但是实际计算的时候，他会和数据一起传入内存。数据长短不一样的时候，这个图的结构就会相应的进行动态修改。

(2)实际的区别

静态图的好处，计算速度快，且需要的内存空间不会很大。

缺点：如果数据一个批次batch中，数据长度不一致，就会出现问题。

动态图的好处，一个批次的数据，可以数据长度不一致。

缺点：每次加载这个图，都会占据一点的内存空间。

Tensorflow 1.x大部分的图，都是静态图，存在一部分动态图(RNN)。

#### 3.深度学习产品设计思路

训练出来一个模型->打包成.so（或者.dll）文件->spark（分布式计算框架）调用so文件

实现分布式计算->建立一个api供移动端或者web端或者哭护短调用(安全、稳定、兼容性)

tensorflow分布式计算

#### 4、机器学习/深度学习项目

数据采集+数据清洗->特征工程->构建模型->模型训练及调参->持久化

(1)如果你做的是一个别人已经完成的事情

项目的核心就不在于构建模型，而是考虑怎么做好特征工程和数据清洗，来提高准确率等等评估指标。



#### 5、定理：深度学习不可解释

(1)体现在哪里？

不是说神经网络结构设计不能解释，而是说每一个神经元计算的结果不可解释

PS:大部分的神经元计算出来的结果，都是可以知道到类似的数学结论(CNN中)。

第一波人工智能（跳棋比赛）

第二波人工智能（国际象棋、深蓝）

第三波人工智能（围棋、阿法狗->自我学习）

现在人工智能火起来原因

1、大数据环境、给训练提供了大量数据

2、芯片算力提高。缩短了计算时间

3、深度学习的理论不断完善，但是也让黑匣子越来越黑

人机耦合：电脑给人类决策提供参考指标



ImportError：DLL loadfailed：找不到指定的程序

原因：VC++库确实，缺少一些c文件

解决方案：VS2015...

tensorflow pip install protobuf == 3.6.0

#### TensorFlow基础

##### Tensorflow介绍

Tensorflow:是一个采用流程图(Data flow graphs),用于数值计算的开源软件库。Tensorflow最初由Google大脑小组的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用性使其也可广泛用于其他计算领域。它是谷歌基于DistBelief进行研发的第二代人工智能学习系统。

其命名来源于本身的原理，Tensor（张量）意味着N维数组，Flow（流）意味着基于数据流图的计算。Tensorflow运行过程就是张量从图的一段流动到另一端的计算过程。张量从图中流过的只管图像是取其名为‘Tensorflow的原因。

Tensorflow的关键点是：“Data Flow Graphs”，表示Tensorflow是一种基于图的计算框架，其中节点（Nodes）在图中表示数学操作，线（Edges）则表示在节点间相互联系的多维数据数组，即张量（Tensor）。这种基于流的架构让Tensorflow具有非常高的灵活性，该灵活性也让Tensorflow框架可以在平台上进行计算，例如：台式计算机、服务器、移动设备等。

备注：Tensorflow的开发过程中，重点在于构建执行流图。

##### 机器学习的定义

Machine Learning(ML)is a scientific discipline that deals with construction and study of algorithms that can learn from data.

机器学习是一门从数据中研究算法的科学学科。

机器学习直白来讲，是根据已有的数据，进行算法的选择，并基于算法和数据构建模型，最中对未来进行预测。

数据流图使用节点（Node）和线（Edges）的有向图描述数学计算；节点一般用来表示施加的数学操作，也可以表示数据输入（feed in）的起点和输出（push out）的终点，或者是读取/写入持久变量（persietent variable）的终点。线（Edges）表示的是节点之间的输入\输出关系，这些线可以输运“size可动态调整”的多维数组，即张量（Tensor）。

一旦输入端的所有张量准备好，节点将被分配到各种计算设备完成一部并行的执行计算。

![](https://raw.githubusercontent.com/MaoChengEr/maochenger.github.io/master/imgs/datagraph.gif)

##### Tensorlow的特性

高度的灵活性：只要能够计算表示称为一个数据流图，那么就可以使用Tensorflow。

可移植性：Tensorflow支持CPU和GPU的运算，并且可以运行在台式机、服务器、收集移动设备等等。

自动求微分：Tensorflow内部实现了自动对于各种给定目标函数求导方式。

多种语言支持：Pyrhon、C++

性能高度优化

![](https://raw.githubusercontent.com/MaoChengEr/maochenger.github.io/master/imgs/Tensorflow官方案例直观理解.png)

##### Tensorflow基本概念 

图（Graph）：图像描述了计算的过程，Tensorflow使用图来表示计算任务。

张量（Tensor）：Tensorflow使用tensor表示数据，每个tensor是一个类型化的多维数组。

操作（op）：图中的节点被称为op（operation的缩写），一个op获得/输入0个或多个Tensor，执行计算，产生0个或多个Tensor。

回话（Session）：图必须在称之为“会话”的上下文中执行。会话将图的op分发到诸如CPU或GPU之类的设备上执行。

变量（Variable）：运行过程中可以被改变，用于维护状态。

Tensorflow的边即有两种连接关系：

​	数据依赖

​	控制依赖

实线表示数据依赖。代表数据，即张量。任意维度的数据统称为张量。在机器学习算法中，张量在数据流图中从前往后流动一遍就完成一次向前传播，而残差从后向前流动一遍就完成一次反向传播。

虚线边表示控制依赖，可以用于控制操作的运行，这被用来确保happens-before关系。这类边上没有数据流过，但源节点必须在目的节点开始执行前完成。

节点：节点又被称为算子，它代表一个操作，一般用来表示施加的数字运算，也可以表示数据输入的起点以及输出的重点，或者是读取/写出持久化变量的终点。



##### Tensorflow基本用法

Tensorflow可以认为是一种编程工具，使用Tensorflow来实现具体的业务需求，所以我们可以认为Tensorflow就是一个“工具箱”，完后我们只用Tensorflow这个“工具箱”中的各种“工具”（方法/API）来实现各种功能，比如使用Tensorflow实现基本的数值计算、机器学习、深度学习等；使用Tensorflow必须理解下列概念：

​	使用图（graph）来表示计算任务。

​	在会话（session）的上下文中执行图。

​        使用Tensor表示数据。

​	通过变量（Variable）来维护状态。

​	使用feed和fectch可以为任意的操作（Operation/op）赋值或者从中获取数据。

Tensorflow一般分为两个阶段：构建阶段和执行阶段。

构建阶段：op的执行步骤被描述称为一个图，然后使用Tensorflow提供的API构建这个图。

执行阶段：将构建好的执行图（Operation Graph）在给定的会话中执行，并得到执行结果。



Tensorflow编程重点是根据业务需求，使用Tensorflow的API将业务转换为执行图（有向无环图）；图中的节点是Tensor，节点之间的连线是节点之间的操作，连线前的节点可以认为是操作的输入，连线后的节点可以认为操作的输出；根据节点的特性（是否有输入输出），可以将节点分为源节点、中间节点和最终的结果节点。

图构建的第一步就是创建源op（source op）；源op不需要任何的输入。op构造器的返回值代表被构造出的op的输出，这些返回值可以传递给其他op构造器作为输入或者直接获取结果。

Tensorflow库中有一个默认图（default graph），op构造器可以直接为其添加节点，一般情况下，使用默认的Graph即可完成程序代码的实现。不过Tensorflow也支持通过Graph类管理多个图。

默认图现在有三个节点，两个Constant op 和一个matmul op。

不使用默认图（Graph），使用多个图来进行编程；但是注意：操作必须属于同一个图，不同图中的节点不能相连。



##### Tensorflow会话

当执行图构建完成 后，才能给启动图，进入到执行阶段；启动图的第一步就是创建一个Session对象，如果无任何参数的情况下，会话构造器将启动默认图。

TensorFlow在构建会话的时候，如果不给定任何参数，那么构建出来的Session对应的内部的Graph其实就是默认Graph，不过我们可以通过参数给定具体对应的是那一个Graph以及当前Session对应的配何参数。Session的构造主要有三个参数，作用如下：

​	target：给定连接的url，只有当分布式运行的时候需给定（后面分布式运行讲）；

​	graph：给定当前的Session对应的图，默认为Tensorflow中的默认图；

​	config：给定当前Session的相关参数，参数详见：

<https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.pro>to中的[ConfigProto]

通过Session的config参数就可以对Tensorflow的应用执行一些优化调整，主要涉及的参数如下：

| 属性                 | 作用                                                         |
| -------------------- | ------------------------------------------------------------ |
| gpu_option           | GPU相关参数，主要参数：per_gpu_memory_fraction和allow_growth |
| allow_soft_placement | 是否允许使用CPU和GPU，默认为false，当我们的安装方式为GPU的时候，建议改参数设置为True，因为Tensorflow中的部分op在CPU上运行 |
| log_device_placement | 是否打印日志，默认为False，不打印日志                        |
| graph_option         | Graph优化相关参数，一般不需要给定，默认即可，主要参数:optimizer_options(do_common_subexpression_elimination、do_constant_folding和opt_level) |





























