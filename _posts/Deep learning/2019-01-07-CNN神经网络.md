---
layout: post
comments: true
categories: 深度学习
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

* content
{:toc}
## CNN神经网络

神经网络和卷积神经网络的区别

正则化与Dropout

典型的结构与训练方式

### 1.卷积神经网络

卷积神经网络，CNN可以有效的降低反馈神经网络（传统的神经网络）的复杂性，常见的CNN结果又LeNet-5,AlexNet,ZFNet,VGGNet,GooleNet,ResNet等等。

CNN主要应用主要是在图像分类和物品识别等应用场景中比较多



### 2.卷积神经网络主要层次

数据输入层，卷积层计算，RelU激励层，池化层，全链接层



### 3.卷积神经网络-Input Layer

和神经网络/机器学习一样，需要对输入的数据进行㔘操作，需要进行预处理的主要原因是：

​	输入数据单位不一样，可能会导致神经网络收敛速度慢，训练时间长

​	数据范围大的输入在模式分类中的作用可能偏大，而数据范围小的作用就可能偏小

​	由于神经网路中存在激活函数是有限的，因此需要将网络训练的目标数据映射到激活函数的值域

​	S形激活函数在(0,1)区间以外区域很平缓，区分度太小。例如s形函数f(x),f(100)与f(5)只相差0.0067



常见的三种预处理数据方式

去均值：

​	将输入数据的各个维度中心化到0

归一化：

​	将输入数据的各个维度的幅度归一化到同样的范围

PCA/白化

​	用PCA降维

​	白化是对数据的每个特征轴上的幅度归一化



### 4.卷积神经网络-CONV Layer

参数共享机制：假设每个神经元链接数据窗的权重是固定的

固定每个神经元的链接权重，可以将神经元看成一个模板；也就是每个神经元只关注一个特性

需要计算的权重个数会大大的减少

一组固定的权重和不同窗口内数据做内积：卷积



### 5.卷积神经网络-Relu Layer

![1550201358709](..\..\imgs\1550201358709.png)

#### 5.1常见非线性映射函数

Sigmoid(S形函数)

Tanh（双曲线正切，双S形函数）

ReLU

Leaky ReLU

ELU

Maxout

激励层建议

​	CNN尽量不要使用sigmoid。如果使用，建议只在全连接层使用

​	首先使用RELU,因为迭代速度快，但是可能效果不佳

​	如果使用RELU失效的情况下，考虑使用Leaky ReLU或者Maxout，此时一般情况都可以解决

​	tanh激活函数在某些情况下有比较好的效果，但是应用场景比较少



### 6.卷积神经网络-Pooling Layer

在连续的卷积层中间存在的就是池化层，主要功能是：通过逐步减小表征特征的空间尺寸来减小参数量和网络中的计算；池化层在每个特征图上独立操作。使用池化层可以压缩数据和参数的量，减小过拟合。

在池化层中，进行压缩减少特征数练的时候一般采用两种策略：

Max Pooling:最大池化每一班采用这种方式

Average Pooling：平均池化

### 7.卷积神经网络-FC

类似传统神经网络中的结构，FC层中的神经元连接着之前层次的所有激活输出；换一句来讲的话，就是车辆层之间所有的神经都有权重连接；通常情况下，在CNN中，FC层只会在尾部出现

一般的CNN结构依次为：

~~~
INPUT
[[CONV->RELU]*N->POOL?]*M
[FC->RELU]*K
FC
~~~



### 8.卷积神将网络可视化理解

保存了层级的网络结构

不同层次有不同的形式（运算）与功能



卷积神经网络的优缺点

优点：

​	共享卷积核（共享参数），对高维数据的处理没有压力

​	无需选择特征属性，只要训练好权重，即可得到特征值

​	深层西的网络抽取图像信息比较丰富，表达效果好

缺点：

​	需要调参，需要大量样本，训练迭代次数比较多，做好使用GPU训练

​	物理含义不明确，从每层输出中很难看书含义来



### 9.卷积神经网络正则化和Dropout

神经网络结构学习能力受神经元数目以及神经网络层析的影响面神经元数目越大，神经网络层次越高，那么神经网络额的学习能力越强，那么就可能出现过拟合的问题。

Regularization：正则化，通过降低模型的复杂度，通过在cost函数上添加一个正则项的方式来降低overfittin，主要有L1和L2两种方式

Dropout：通过随机删除神经网络中的神经元来解决overfitting问题，在每次迭代的时候，只使用部分神经元训练模型获取W和d的值。



一般情况下，对于同一组训练数据没利用不同的神经网络训练之后，求其输出的平均值可以减少overfitting。Dropout就是利用这个原理，每次丢掉一半左右的隐藏层神经元，相当于在不同的神经网络上进行训练，这样就减少了神经元之间的依赖性，即每个神经元不能依赖于某几个其他的神经元（指层于层之间相连接的神经元），使神将网络更加能学习到于其他神经元之间的更加健壮的特征。另外Dropout不仅减少overfitting，还能提高准确率。

正则化是通过cost拿书添加正则化项的方式来解决overfitting，Dropout是通过直接修改神经网络的结构来解决overfitting。



和一般的机器学习算法一样，需要先sing一loss Function，衡量预测值和实际值之间的误差，一般只用平方和误差方式。

找到最小损失函数的W和b的值，CNN中使用的是SGD

其实就是一般机器学习中的BP算法；SGD需要计算W和b的剪刀，BP废就是计算偏导用的，BP算法的核心就是求导链式法则。

使用B平算法主机酸醋$\bigtriangledown W和\bigtriangledown b$的值

格局SGD（随机梯度下降），迭代更新W和b



### 10.池化层反向传播

Maxpool池化层反向传播，出最大值处集成上层梯度外，其他位置为零



局部响应归一化

局部相应归一化层的建立是模仿生物神经系统的临近一直机制，对局部神经的活动创建竞争机制，是的局部相应比较大的值相对更大，这样更能凸显需要的特征，提高模型繁华能力，LRN一般是在激活、池化后进行的一种处理方法。用$a_{x,y}^i$表示经过激活函数后的输出值，对于第j和卷积核经过激活函数输出的神经元激活值$a_{x,y}^j$选取临近的n个卷积后的输出值，将它们在同一个位置上卷积后激活函数的输出值进行求和，其中(x,y)就代表特征层的坐标位置，i，j就代表不同的卷积核，N为总卷积核的数目，k，a和$\beta$都是超参数，有验证数据集决定，一般取k=2，a=10-4,$\beta$=0.75,具体的如下：
$$
b_{x,y}^i=a_{x,y}^i/(k+a\sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)}(a_{x,y}^j)^2)^{\beta}
$$
### 11.数据增强

增加训练数据，则能够提升算法的准确率，因为这样可以避免过拟合，而避免了过拟合就可以增大你的网络结构。当训练数据有限的时候，可以通过一些变换来换已有的训练数据集中生成一些新的数据，来扩大训练数据。数据增强的方法有：

1)水平翻转

2)随机裁剪

3)fancy PCA

在训练集像素值得RGB沿着空间进行PCA，得到RGB空间的3个脂肪险向量，3个特征值，$p_1,p_2,p_3.\lambda_1,\lambda_2.\lambda_3$,对于每幅图像的每个像素加上如下的变化：
$$
I_{x,y}=\left[I_{xy}^R,I_{xy}^G,I_{xy}^B\right]\\
\left[p_1,p_2,p_3\right]\left[\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3\right]^{T}
$$
其中：$\alpha_1$是满足均值为0，方差为0、1额的随机变量。

4)样本不均衡

样本不均衡即有些类别图像特别多，有些特别少。类别不平衡数据的处理：Label shuffle

![1548122250786](..\..\imgs\1548122250786.png)

5）其他

平移转换

旋转/仿射变换

高斯噪声、模糊处理

对颜色的数据增强：图像亮度、饱和度、对比度变化

6)训练和测试要协调

在训练的时候，我们通常都要做数据增强，在测试的时候，我们通常很少去做数据增强。这其中似乎有些比协调，因为你训练和测试之间有些不一致。实验发现，训练的最后几个的迭代，移除数据增强，和传统一样册数，可以提升一点性能。

如果训练的时候一直使用尺度个长宽比增强数据，在测试的时候也同样做这个变化，随机取32个裁剪图片来测试，也可以在最后的模型上提升一点性能。

就是多尺度的训练，多尺度的测试。

训练过程的中间结果，加入做测试，可以一定程度上降低过拟合。

### 12.梯度下降

梯度下降是常用的卷积神经网络模型参数求解方法，根据每次参更新使用样本数量的多少，可以分为以下三类：

批量梯度下降

小批量梯度下降

随机梯度下降

求参数过程即最小化损失函数过程。比如有一个含有D个训练数据的数据集，损失函数如下：
$$
L(W)=\frac{1}{|D|}\sum_i^{|D|}f_w(X^{(i)})+\lambda r(w)
$$
$f_w(X^{(i)})$是单个样本$X^{(i)}$的损失，$\gamma(w)$是正则项，$\gamma$权重。

### 13.卷积神将网络典型CNN

LeNet:最早用于数字识别的CNN

AlexLet：2012年ILSVRC比赛冠军，远超第二名的CNN，比LeNet更深，用多层晓娟及叠加来替换单个打卷机

ZF Net：2013ILSVRC冠军

GooleNet：2014ILSVRC冠军

VGGNet:2014ILSVRC比赛中算法模型，鲜果低于GooleNet

ResNet：2015ILSVRC冠军，结构修正以使用更深层次的CNN训练

#### 13.1CNN-LeNet解析

C1层是一个卷积层

​	6个特征图，每个特征图中的每一个神经元与输入中5*5的领域相连，特征图大小为28*28

​	每个卷积神经元的参数数目：5*5=25个weight参数和一个bias参数

​	链接数目：（5\*5+1）\*6\*（28\*28）=122304个链接

​	参数共享：每个特征图内共享参数，因此参数总数：共（5\*5\*1）\*6=156个参数

S2层是一个下采样层：

​	6个14\*14的特征图,每个图中的每个单元与C1特征图中的一个2\*2领域向链接，不重叠。因此，S2中每个特征图的大小是C1中的特征图大小的1\\4。

​	S2层中每个单元的4个输入相加，乘以一个可训练参数W，在加上一个棵训练偏置b，结果通过sigmoid函数计算。

连接数：（2\*2+1）\*1\*14\*14\*6=5880个

参数共享：每个特征图内共享参数，因此有2\*6=12个可训练参数。

C3层是一个卷积层

​	16个卷积核，得到16张特征图，特征图大小为10\*10

​	每个特征图中的每个神经元与S2中某几层的多个5\*5的领域相连接

​	例如：对于C3层第0张特征图，其每一个节点与S2层的低0~2张特征图，总共3个5\*5个节点相连接。

![1548123353312](..\..\imgs\1548123353312.png)



S4层是一个下采样层

​	由16个5\*5大小的特征图构成，特征图中的每个单元与C3中相应特征图的2\*2领域相连接

​	连接数：（2\*2+1）\*5\*5*16=2000个

​	参数共享：特征图内共享参数，每个特征图中的每个神经元需要1个因子和一个偏置，因此有2\*16个棵训练参数。

C5层是一个卷积层

​	120个神经元，可以看做120个特征图，买涨特征图的大小为1\*1

​	每个单元与4层的全部16个单元的5\*5领域相连（S4和C5之间的全连接）

​	连接数=可训练参数：（5\*5\*16+1）\*120=48120个

F6层是一个全连接层

​	有84个单元（之搜易选这个数字的原因来自于输出层的设计），与C5层全连接

​	F6层计算输入向量和去那种向量之间的点积，在加上一个偏置。

​	连接数=可训练参数：（120+1）\*84=10164

​	84：stylize image:7\*12

输出层采用欧氏径向基函数单元

​	给定一个输入模式，损失函数应能是的F6的配置与RBF参数向量（即模式的期望分类）足够接近。

​	每类一个单元，每个单元链接84个输入：每个输出RBF单元计算输入向量和参数向量之间的欧氏距离。

RBF输出可以被理解为F6层配置空间的高斯分布的对数似然【-log-likelihood】



#### 13.2 CNN-AlexNet

AlexNet结构优化

​	非线性激活函数：ReLU

​	防止过拟合的方法：Dropout，Data augmentation（数据增强）

​	大数据训练：百万级ImageNet图像数据

​	GPU实现：在每个GPU中防止一半核（或神经元），还有一个额外的技巧：GPU之间的通讯只在某些层进行。

​	LRN归一化：本质上，这个层也是为了防止激活函数的饱和的。

#### 13.3 CNN-ZF Net

ZF Net

 	基于AlexNet进行微调

​	top5错误率11.2%

​	使用ReLU激活函数和交叉熵损失函数

























